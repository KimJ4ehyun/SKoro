# ================================================================
# llm_utils.py - LLM ê´€ë ¨ ìœ í‹¸ë¦¬í‹°
# ================================================================

import re
import json
import hashlib
import os
from typing import Dict, List
from datetime import datetime
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage
from agents.evaluation.modules.module_06_4p_evaluation.db_utils import *
from config.settings import *

# ================================================================
# LLM í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
# ================================================================

llm_client = ChatOpenAI(model="gpt-4o-mini", temperature=0)
print(f"LLM Client initialized: {llm_client.model_name}")


def _extract_json_from_llm_response(text: str) -> str:
    """LLM ì‘ë‹µì—ì„œ JSON ë¸”ë¡ ì¶”ì¶œ"""
    match = re.search(r"```(?:json)?\s*(.*?)\s*```", text, re.DOTALL)
    if match:
        return match.group(1).strip()
    return text.strip()


# ================================================================
# íŒŒì¼ ê¸°ë°˜ ìºì‹œ ê´€ë¦¬
# ================================================================

def get_cache_file_path() -> str:
    """ìºì‹œ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.abspath(os.path.join(current_dir, '../../../../'))
    cache_dir = os.path.join(project_root, 'data', 'cache')
    
    # ìºì‹œ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs(cache_dir, exist_ok=True)
    
    return os.path.join(cache_dir, 'evaluation_criteria_cache.json')


def load_cache_from_file() -> Dict:
    """íŒŒì¼ì—ì„œ ìºì‹œ ë¡œë“œ"""
    cache_file = get_cache_file_path()
    
    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
                print(f"âœ… íŒŒì¼ ìºì‹œ ë¡œë“œë¨: {cache_file}")
                return cache_data
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # ê¸°ë³¸ ìºì‹œ êµ¬ì¡° ë°˜í™˜
    return {
        "raw_text": None,
        "raw_text_hash": None,
        "parsed_criteria": None,
        "last_updated": None
    }


def save_cache_to_file(cache_data: Dict) -> bool:
    """ìºì‹œë¥¼ íŒŒì¼ì— ì €ì¥"""
    cache_file = get_cache_file_path()
    
    try:
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… ìºì‹œ íŒŒì¼ ì €ì¥ë¨: {cache_file}")
        return True
    except Exception as e:
        print(f"âŒ ìºì‹œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False


def get_text_hash(text: str) -> str:
    """í…ìŠ¤íŠ¸ì˜ í•´ì‹œê°’ ê³„ì‚°"""
    return hashlib.md5(text.encode('utf-8')).hexdigest()


def parse_criteria_with_llm(raw_text: str) -> Dict[str, str]:
    """ë…ë¦½ì ì¸ LLM íŒŒì‹± í•¨ìˆ˜ - ë¬¸ìì—´ ì…ë ¥"""
    
    system_prompt = """
ë‹¹ì‹ ì€ ì„±ê³¼ í‰ê°€ ê¸°ì¤€ì„ ë¶„ì„í•˜ëŠ” í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìëŠ” Passionate, Proactive, Professional, People ë„¤ ê°€ì§€ í•­ëª©ì˜ í‰ê°€ ê¸°ì¤€ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ì— ëª¨ë‘ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.
ë‹¤ë§Œ í•­ëª© êµ¬ë¶„ì´ ëª…í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¬¸ë§¥ì„ í†µí•´ í•­ëª©ë³„ë¡œ ë‚´ìš©ì„ ë¶„ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ì‘ì—… ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì€ JSON í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤:

```json
{
  "passionate": "passionate í‰ê°€ ê¸°ì¤€ í…ìŠ¤íŠ¸...",
  "proactive": "proactive í‰ê°€ ê¸°ì¤€ í…ìŠ¤íŠ¸...",
  "professional": "professional í‰ê°€ ê¸°ì¤€ í…ìŠ¤íŠ¸...",
  "people": "people í‰ê°€ ê¸°ì¤€ í…ìŠ¤íŠ¸..."
}
```
"""

    human_prompt = f"""
ë‹¤ìŒì€ DBì—ì„œ ê°€ì ¸ì˜¨ ì „ì²´ í‰ê°€ ê¸°ì¤€ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤:

{raw_text}

ì´ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ 4ê°œì˜ í‰ê°€ í•­ëª©ìœ¼ë¡œ ë‚˜ëˆ ì£¼ì„¸ìš”.
ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
"""

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_prompt), 
        HumanMessage(content=human_prompt)
    ])

    chain = prompt | llm_client
    
    try:
        response = chain.invoke({})
        content = str(response.content)  # íƒ€ì… ì•ˆì „ì„± í™•ë³´
        match = re.search(r"```json\s*(.*?)```", content, re.DOTALL)
        extracted = match.group(1).strip() if match else content.strip()
        parsed = json.loads(extracted)

        expected_keys = {"passionate", "proactive", "professional", "people"}
        if not expected_keys.issubset(parsed.keys()):
            raise ValueError("4ê°œì˜ í‰ê°€ ê¸°ì¤€ í‚¤ ì¤‘ ì¼ë¶€ê°€ ëˆ„ë½ë¨")

        return parsed
        
    except Exception as e:
        print(f"âŒ LLM íŒŒì‹± ì‹¤íŒ¨: {e}")
        raise e


def load_and_cache_evaluation_criteria() -> Dict[str, str]:
    """íŒŒì¼ ê¸°ë°˜ ìºì‹œë¥¼ ì‚¬ìš©í•˜ëŠ” í‰ê°€ ê¸°ì¤€ ë¡œë”"""
    
    # 1. íŒŒì¼ì—ì„œ ìºì‹œ ë¡œë“œ
    cache_data = load_cache_from_file()
    
    # 2. DBì—ì„œ í˜„ì¬ í‰ê°€ ê¸°ì¤€ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    try:
        current_raw_text = fetch_evaluation_criteria_from_db()
        current_hash = get_text_hash(current_raw_text)
        
        print(f"ğŸ” DB í‰ê°€ ê¸°ì¤€ í•´ì‹œ: {current_hash[:8]}...")
        
    except Exception as e:
        print(f"âŒ DB ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise e
    
    # 3. ìºì‹œëœ ë°ì´í„°ì™€ ë¹„êµ
    cached_hash = cache_data.get("raw_text_hash")
    cached_parsed = cache_data.get("parsed_criteria")
    
    if cached_hash == current_hash and cached_parsed:
        print("âœ… íŒŒì¼ ìºì‹œëœ í‰ê°€ ê¸°ì¤€ ì‚¬ìš© (DB í…ìŠ¤íŠ¸ ë³€ê²½ ì—†ìŒ)")
        return cached_parsed
    
    # 4. ìºì‹œ ë¯¸ìŠ¤ ë˜ëŠ” í…ìŠ¤íŠ¸ ë³€ê²½ - ìƒˆë¡œ íŒŒì‹±
    print("ğŸ”„ í‰ê°€ ê¸°ì¤€ ìƒˆë¡œ íŒŒì‹± ì¤‘...")
    
    try:
        # âœ… ìˆ˜ì •: ë…ë¦½ í•¨ìˆ˜ í˜¸ì¶œ
        parsed_criteria = parse_criteria_with_llm(current_raw_text)
        
        # 5. ìºì‹œ ì—…ë°ì´íŠ¸ ë° íŒŒì¼ ì €ì¥
        updated_cache = {
            "raw_text": current_raw_text,
            "raw_text_hash": current_hash,
            "parsed_criteria": parsed_criteria,
            "last_updated": datetime.now().isoformat()
        }
        
        save_cache_to_file(updated_cache)
        
        print("âœ… í‰ê°€ ê¸°ì¤€ íŒŒì‹± ì™„ë£Œ ë° íŒŒì¼ ìºì‹œ ì—…ë°ì´íŠ¸")
        return parsed_criteria
        
    except Exception as e:
        print(f"âŒ í‰ê°€ ê¸°ì¤€ íŒŒì‹± ì‹¤íŒ¨: {e}")
        raise e


# ================================================================
# LLM í‰ê°€ í•¨ìˆ˜ë“¤
# ================================================================

def call_llm_for_passionate_evaluation(
    task_data: List[Dict], basic_info: Dict, evaluation_criteria: Dict[str, str]
) -> Dict:
    """Passionate (ì—´ì •ì  ëª°ì…) LLM í‰ê°€"""

    emp_name = basic_info.get("emp_name", "")
    task_details = ""

    for task in task_data:
        task_details += f"- ì—…ë¬´ ìš”ì•½: {task.get('task_summary', '')}\n"
        if task.get("task_performance"):
            task_details += f"  ì„±ê³¼: {task.get('task_performance')}\n"
        task_details += "\n"

    if not task_details.strip():
        task_details = "ë¶„ì„í•  ì—…ë¬´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

    # í‰ê°€ê¸°ì¤€ì„ ë™ì ìœ¼ë¡œ ì‚½ì…
    bars_text = evaluation_criteria.get("passionate", "").strip()
    if not bars_text:
        bars_text = "í‰ê°€ ê¸°ì¤€ ì—†ìŒ. ê¸°ë³¸ ì ìˆ˜ë¡œ í‰ê°€ ì§„í–‰"

    system_prompt = f"""
    ë‹¹ì‹ ì€ SK AX 4P í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    Passionate (ì—´ì •ì  ëª°ì…) ê¸°ì¤€ìœ¼ë¡œ ì§ì›ì„ í‰ê°€í•˜ì„¸ìš”.

    í‰ê°€ ê¸°ì¤€:
    {bars_text}

    Passionate ì •ì˜: "ì´ ê°€ì¹˜ëŠ” ê·œë²”ì„ ë„˜ì–´ì„œ í—Œì‹ ê³¼ ì—´ì •ì„ ê°€ì§€ê³  ì¼ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì„ ê°•ì¡°í•©ë‹ˆë‹¤. ì§ì›ë“¤ì€ ì—ë„ˆì§€ì™€ í—Œì‹ ìœ¼ë¡œ ì—…ë¬´ì— ì„í•˜ë©°, íƒì›”í•œ ê²°ê³¼ë¥¼ ì¶”êµ¬í•´ì•¼ í•©ë‹ˆë‹¤."
    """

    human_prompt = f"""
    <ì§ì› ì •ë³´>
    ì´ë¦„: {emp_name}
    </ì§ì› ì •ë³´>

    <ì—…ë¬´ ë°ì´í„°>
    {task_details}
    </ì—…ë¬´ ë°ì´í„°>

    ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ Passionate ê´€ì ì—ì„œ í‰ê°€í•˜ì„¸ìš”.


    ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:
    ```json
    {{
        "score": [1-5ì  ì‚¬ì´ì˜ ìˆ«ì],
        "evidence": ["êµ¬ì²´ì  ê·¼ê±°1", "êµ¬ì²´ì  ê·¼ê±°2", "êµ¬ì²´ì  ê·¼ê±°3"],
        "reasoning": "í‰ê°€ ê·¼ê±° ì„¤ëª…",
        "bars_level": "í•´ë‹¹ í™œë™ì´ ë¶€í•©í•œ í‰ê°€ ê¸°ì¤€ì˜ ë ˆì´ë¸” (ì˜ˆ: 'íƒì›”í•œ ì—´ì •', 'ì„±ì‹¤í•œ ìˆ˜í–‰' ë“±)",
        "improvement_points": ["ê°œì„ ì 1", "ê°œì„ ì 2"]
    }}
    ```
    """

    prompt = ChatPromptTemplate.from_messages(
        [SystemMessage(content=system_prompt), HumanMessage(content=human_prompt)]
    )

    chain = prompt | llm_client

    try:
        response = chain.invoke({})
        content = str(response.content)  # íƒ€ì… ì•ˆì „ì„± í™•ë³´
        json_output = _extract_json_from_llm_response(content)
        result = json.loads(json_output)

        # ìœ íš¨ì„± ê²€ì¦
        if not isinstance(result.get("score"), (int, float)) or not (
            1 <= result["score"] <= 5
        ):
            result["score"] = 3.0
        if not isinstance(result.get("evidence"), list):
            result["evidence"] = ["í‰ê°€ ê·¼ê±° ìƒì„± ì‹¤íŒ¨"]
        if not result.get("reasoning"):
            result["reasoning"] = "ê¸°ë³¸ í‰ê°€"
        if not result.get("bars_level"):
            result["bars_level"] = "ê¸°ë³¸ ì—´ì •"
        if not isinstance(result.get("improvement_points"), list):
            result["improvement_points"] = ["ì§€ì†ì  ê°œì„  í•„ìš”"]

        return result

    except Exception as e:
        print(f"Passionate í‰ê°€ LLM ì˜¤ë¥˜: {e}")
        return {
            "score": 3.0,
            "evidence": ["AI í‰ê°€ ì‹¤íŒ¨"],
            "reasoning": f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)[:100]}",
            "bars_level": "ê¸°ë³¸ ì—´ì •",
            "improvement_points": ["í‰ê°€ ì¬ì‹œë„ í•„ìš”"],
        }


def call_llm_for_proactive_evaluation(
    task_data: List[Dict], basic_info: Dict, evaluation_criteria: Dict[str, str]
) -> Dict:
    """Proactive (ëŠ¥ë™ì  ì£¼ë„) LLM í‰ê°€"""

    emp_name = basic_info.get("emp_name", "")
    task_details = ""

    for task in task_data:
        task_details += f"- ì—…ë¬´ ìš”ì•½: {task.get('task_summary', '')}\n"
        if task.get("task_performance"):
            task_details += f"  ì„±ê³¼: {task.get('task_performance')}\n"
        task_details += "\n"

    if not task_details.strip():
        task_details = "ë¶„ì„í•  ì—…ë¬´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

    # í‰ê°€ê¸°ì¤€ì„ ë™ì ìœ¼ë¡œ ì‚½ì…
    bars_text = evaluation_criteria.get("proactive", "").strip()
    if not bars_text:
        bars_text = "í‰ê°€ ê¸°ì¤€ ì—†ìŒ. ê¸°ë³¸ ì ìˆ˜ë¡œ í‰ê°€ ì§„í–‰"

    system_prompt = f"""
    ë‹¹ì‹ ì€ SK AX 4P í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    Proactive (ëŠ¥ë™ì  ì£¼ë„) ê¸°ì¤€ìœ¼ë¡œ ì§ì›ì„ í‰ê°€í•˜ì„¸ìš”.

    í‰ê°€ ê¸°ì¤€:
    {bars_text}

    Proactive ì •ì˜: "ì£¼ë„ì ì¸ íƒœë„ë¥¼ ì·¨í•˜ê³  ë¯¸ë˜ë¥¼ ëŒ€ë¹„í•˜ëŠ” ìì„¸ë¥¼ ì¥ë ¤í•©ë‹ˆë‹¤. ì§ì›ë“¤ì€ ë„ì „ ê³¼ì œë¥¼ ì˜ˆì¸¡í•˜ê³ , ê¸°íšŒë¥¼ ì°¾ìœ¼ë©°, ê¸ì •ì ì¸ ê²°ê³¼ë¥¼ ì´ëŒì–´ë‚´ê¸° ìœ„í•´ ëŠ¥ë™ì ìœ¼ë¡œ í–‰ë™í•´ì•¼ í•©ë‹ˆë‹¤."
    """

    human_prompt = f"""
    <ì§ì› ì •ë³´>
    ì´ë¦„: {emp_name}
    </ì§ì› ì •ë³´>

    <ì—…ë¬´ ë°ì´í„°>
    {task_details}
    </ì—…ë¬´ ë°ì´í„°>

    ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ Proactive ê´€ì ì—ì„œ í‰ê°€í•˜ì„¸ìš”.


    ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:
    ```json
    {{
        "score": [1-5ì  ì‚¬ì´ì˜ ìˆ«ì],
        "evidence": ["êµ¬ì²´ì  ê·¼ê±°1", "êµ¬ì²´ì  ê·¼ê±°2", "êµ¬ì²´ì  ê·¼ê±°3"],
        "reasoning": "í‰ê°€ ê·¼ê±° ì„¤ëª…",
        "bars_level": "í•´ë‹¹ í™œë™ì´ ë¶€í•©í•œ í‰ê°€ ê¸°ì¤€ì˜ ë ˆì´ë¸” (ì˜ˆ: 'íƒì›”í•œ ì—´ì •', 'ì„±ì‹¤í•œ ìˆ˜í–‰' ë“±)",
        "improvement_points": ["ê°œì„ ì 1", "ê°œì„ ì 2"]
    }}
    ```
    """

    prompt = ChatPromptTemplate.from_messages(
        [SystemMessage(content=system_prompt), HumanMessage(content=human_prompt)]
    )

    chain = prompt | llm_client

    try:
        response = chain.invoke({})
        content = str(response.content)  # íƒ€ì… ì•ˆì „ì„± í™•ë³´
        json_output = _extract_json_from_llm_response(content)
        result = json.loads(json_output)

        if not isinstance(result.get("score"), (int, float)) or not (
            1 <= result["score"] <= 5
        ):
            result["score"] = 3.0
        if not isinstance(result.get("evidence"), list):
            result["evidence"] = ["í‰ê°€ ê·¼ê±° ìƒì„± ì‹¤íŒ¨"]
        if not result.get("reasoning"):
            result["reasoning"] = "ê¸°ë³¸ í‰ê°€"
        if not result.get("bars_level"):
            result["bars_level"] = "ê¸°ë³¸ ì£¼ë„ì„±"
        if not isinstance(result.get("improvement_points"), list):
            result["improvement_points"] = ["ì§€ì†ì  ê°œì„  í•„ìš”"]

        return result

    except Exception as e:
        print(f"Proactive í‰ê°€ LLM ì˜¤ë¥˜: {e}")
        return {
            "score": 3.0,
            "evidence": ["AI í‰ê°€ ì‹¤íŒ¨"],
            "reasoning": f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)[:100]}",
            "bars_level": "ê¸°ë³¸ ì£¼ë„ì„±",
            "improvement_points": ["í‰ê°€ ì¬ì‹œë„ í•„ìš”"],
        }


def call_llm_for_professional_evaluation(
    task_data: List[Dict], basic_info: Dict, evaluation_criteria: Dict[str, str]
) -> Dict:
    """Professional (ì „ë¬¸ì„±) LLM í‰ê°€"""

    emp_name = basic_info.get("emp_name", "")
    position = basic_info.get("position", "")
    task_details = ""

    for task in task_data:
        task_details += f"- ì—…ë¬´ ìš”ì•½: {task.get('task_summary', '')}\n"
        if task.get("task_performance"):
            task_details += f"  ì„±ê³¼: {task.get('task_performance')}\n"
        task_details += "\n"

    if not task_details.strip():
        task_details = "ë¶„ì„í•  ì—…ë¬´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

    bars_text = evaluation_criteria.get("professional", "").strip()
    if not bars_text:
        bars_text = "í‰ê°€ ê¸°ì¤€ ì—†ìŒ. ê¸°ë³¸ ì ìˆ˜ë¡œ í‰ê°€ ì§„í–‰"

    system_prompt = f"""
    ë‹¹ì‹ ì€ SK AX 4P í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    Professional (ì „ë¬¸ì„±) ê¸°ì¤€ìœ¼ë¡œ ì§ì›ì„ í‰ê°€í•˜ì„¸ìš”.

    í‰ê°€ ê¸°ì¤€:
    {bars_text}

    Professional ì •ì˜: "ëª¨ë“  ì—…ë¬´ì—ì„œ ì „ë¬¸ì„±ì„ ìœ ì§€í•˜ëŠ” ì¤‘ìš”ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ì§ì›ë“¤ì€ ë†’ì€ ìœ¤ë¦¬ì  ê¸°ì¤€ê³¼ ì§ë¬´ ëŠ¥ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ì¼ì„ ìˆ˜í–‰í•˜ê³  íšŒì‚¬ì˜ ê°€ì¹˜ë¥¼ ëŒ€í‘œí•´ì•¼ í•©ë‹ˆë‹¤."
    """

    human_prompt = f"""
    <ì§ì› ì •ë³´>
    ì´ë¦„: {emp_name}
    ì§ì±…: {position}
    </ì§ì› ì •ë³´>

    <ì—…ë¬´ ë°ì´í„°>
    {task_details}
    </ì—…ë¬´ ë°ì´í„°>

    ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ Professional ê´€ì ì—ì„œ í‰ê°€í•˜ì„¸ìš”.


    ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:
    ```json
    {{
        "score": [1-5ì  ì‚¬ì´ì˜ ìˆ«ì],
        "evidence": ["êµ¬ì²´ì  ê·¼ê±°1", "êµ¬ì²´ì  ê·¼ê±°2", "êµ¬ì²´ì  ê·¼ê±°3"],
        "reasoning": "í‰ê°€ ê·¼ê±° ì„¤ëª…",
        "bars_level": "í•´ë‹¹ í™œë™ì´ ë¶€í•©í•œ í‰ê°€ ê¸°ì¤€ì˜ ë ˆì´ë¸” (ì˜ˆ: 'íƒì›”í•œ ì—´ì •', 'ì„±ì‹¤í•œ ìˆ˜í–‰' ë“±)",
        "improvement_points": ["ê°œì„ ì 1", "ê°œì„ ì 2"]
    }}
    ```
    """

    prompt = ChatPromptTemplate.from_messages(
        [SystemMessage(content=system_prompt), HumanMessage(content=human_prompt)]
    )

    chain = prompt | llm_client

    try:
        response = chain.invoke({})
        content = str(response.content)  # íƒ€ì… ì•ˆì „ì„± í™•ë³´
        json_output = _extract_json_from_llm_response(content)
        result = json.loads(json_output)

        if not isinstance(result.get("score"), (int, float)) or not (
            1 <= result["score"] <= 5
        ):
            result["score"] = 3.0
        if not isinstance(result.get("evidence"), list):
            result["evidence"] = ["í‰ê°€ ê·¼ê±° ìƒì„± ì‹¤íŒ¨"]
        if not result.get("reasoning"):
            result["reasoning"] = "ê¸°ë³¸ í‰ê°€"
        if not result.get("bars_level"):
            result["bars_level"] = "ê¸°ë³¸ ì „ë¬¸ì„±"
        if not isinstance(result.get("improvement_points"), list):
            result["improvement_points"] = ["ì§€ì†ì  ê°œì„  í•„ìš”"]

        return result

    except Exception as e:
        print(f"Professional í‰ê°€ LLM ì˜¤ë¥˜: {e}")
        return {
            "score": 3.0,
            "evidence": ["AI í‰ê°€ ì‹¤íŒ¨"],
            "reasoning": f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)[:100]}",
            "bars_level": "ê¸°ë³¸ ì „ë¬¸ì„±",
            "improvement_points": ["í‰ê°€ ì¬ì‹œë„ í•„ìš”"],
        }


def call_llm_for_people_evaluation(
    task_data: List[Dict],
    collaboration_data: Dict,
    peer_talk_data: Dict,
    basic_info: Dict,
    evaluation_criteria: Dict[str, str],
) -> Dict:
    """People (ê³µë™ì²´) LLM í‰ê°€"""

    emp_name = basic_info.get("emp_name", "")

    # í˜‘ì—… ë°ì´í„° ì •ë¦¬
    collaboration_info = ""
    if collaboration_data:
        collaboration_info = f"""
        íŒ€ ì—­í• : {collaboration_data.get('team_role', '')}
        í˜‘ì—…ë¥ : {collaboration_data.get('collaboration_rate', 0)}%
        í•µì‹¬ í˜‘ì—…ì: {', '.join(collaboration_data.get('key_collaborators', []))}
        ë™ë£Œí‰ê°€ ìš”ì•½: {collaboration_data.get('peer_talk_summary', '')}
        ì „ì²´ í‰ê°€: {collaboration_data.get('overall_evaluation', '')}
        """
    else:
        collaboration_info = "í˜‘ì—… ë°ì´í„° ì—†ìŒ"

    # Peer Talk ë°ì´í„° (JSON êµ¬ì¡° ë°˜ì˜)
    strengths = peer_talk_data.get("strengths", "")
    concerns = peer_talk_data.get("concerns", "")
    collaboration_observations = peer_talk_data.get("collaboration_observations", "")

    peer_talk_section = f"""
    [ë™ë£Œ í”¼ë“œë°± ìš”ì•½]
    - ê°•ì : {strengths if strengths else 'ì •ë³´ ì—†ìŒ'}
    - ìš°ë ¤/ê°œì„ ì : {concerns if concerns else 'ì •ë³´ ì—†ìŒ'}
    - í˜‘ì—… ê´€ì°°: {collaboration_observations if collaboration_observations else 'ì •ë³´ ì—†ìŒ'}
    """

    # Task ë°ì´í„°ì—ì„œ í˜‘ì—… ê´€ë ¨ ë‚´ìš© ì¶”ì¶œ
    collaboration_tasks = ""
    for task in task_data:
        if any(
            keyword in task.get("task_summary", "")
            for keyword in ["í˜‘ì—…", "í•¨ê»˜", "ê³µë™", "íŒ€", "ë™ë£Œ"]
        ):
            collaboration_tasks += f"- {task.get('task_summary', '')}\n"

    # í‰ê°€ê¸°ì¤€ì„ ë™ì ìœ¼ë¡œ ì‚½ì…
    bars_text = evaluation_criteria.get("people", "").strip()
    if not bars_text:
        bars_text = "í‰ê°€ ê¸°ì¤€ ì—†ìŒ. ê¸°ë³¸ ì ìˆ˜ë¡œ í‰ê°€ ì§„í–‰"

    system_prompt = f"""
    ë‹¹ì‹ ì€ SK AX 4P í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    People (ê³µë™ì²´) ê¸°ì¤€ìœ¼ë¡œ ì§ì›ì„ í‰ê°€í•˜ì„¸ìš”.

    í‰ê°€ ê¸°ì¤€:
    {bars_text}

    People ì •ì˜: "ì¡°ì§ ë‚´ì—ì„œ ì˜ë¯¸ ìˆëŠ” ê´€ê³„ì™€ íŒ€ì›Œí¬ë¥¼ í˜•ì„±í•˜ëŠ” ë° ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤. ë™ë£Œ, ì´í•´ê´€ê³„ì, ê³ ê°ê³¼ì˜ í˜‘ë ¥, ê³µê°, ì¡´ì¤‘ì„ ì¥ë ¤í•©ë‹ˆë‹¤."
    """

    human_prompt = f"""
    <ì§ì› ì •ë³´>
    ì´ë¦„: {emp_name}
    </ì§ì› ì •ë³´>

    <í˜‘ì—… ë°ì´í„°>
    {collaboration_info}
    </í˜‘ì—… ë°ì´í„°>

    {peer_talk_section}

    <í˜‘ì—… ê´€ë ¨ ì—…ë¬´>
    {collaboration_tasks if collaboration_tasks else 'í˜‘ì—… ê´€ë ¨ ì—…ë¬´ ë°ì´í„° ì—†ìŒ'}
    </í˜‘ì—… ê´€ë ¨ ì—…ë¬´>

    ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ People ê´€ì ì—ì„œ í‰ê°€í•˜ì„¸ìš”.

    ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:
    ```json
    {{
        "score": [1-5ì  ì‚¬ì´ì˜ ìˆ«ì],
        "evidence": ["êµ¬ì²´ì  ê·¼ê±°1", "êµ¬ì²´ì  ê·¼ê±°2", "êµ¬ì²´ì  ê·¼ê±°3"],
        "reasoning": "í‰ê°€ ê·¼ê±° ì„¤ëª…",
        "bars_level": "í•´ë‹¹ í™œë™ì´ ë¶€í•©í•œ í‰ê°€ ê¸°ì¤€ì˜ ë ˆì´ë¸” (ì˜ˆ: 'íƒì›”í•œ ì—´ì •', 'ì„±ì‹¤í•œ ìˆ˜í–‰' ë“±)",
        "improvement_points": ["ê°œì„ ì 1", "ê°œì„ ì 2"]
    }}
    ```
    """

    prompt = ChatPromptTemplate.from_messages(
        [SystemMessage(content=system_prompt), HumanMessage(content=human_prompt)]
    )

    chain = prompt | llm_client

    try:
        response = chain.invoke({})
        content = str(response.content)  # íƒ€ì… ì•ˆì „ì„± í™•ë³´
        json_output = _extract_json_from_llm_response(content)
        result = json.loads(json_output)

        if not isinstance(result.get("score"), (int, float)) or not (
            1 <= result["score"] <= 5
        ):
            result["score"] = 3.0
        if not isinstance(result.get("evidence"), list):
            result["evidence"] = ["í‰ê°€ ê·¼ê±° ìƒì„± ì‹¤íŒ¨"]
        if not result.get("reasoning"):
            result["reasoning"] = "ê¸°ë³¸ í‰ê°€"
        if not result.get("bars_level"):
            result["bars_level"] = "ê¸°ë³¸ì  í˜‘ë ¥"
        if not isinstance(result.get("improvement_points"), list):
            result["improvement_points"] = ["ì§€ì†ì  ê°œì„  í•„ìš”"]

        return result

    except Exception as e:
        print(f"People í‰ê°€ LLM ì˜¤ë¥˜: {e}")
        return {
            "score": 3.0,
            "evidence": ["AI í‰ê°€ ì‹¤íŒ¨"],
            "reasoning": f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)[:100]}",
            "bars_level": "ê¸°ë³¸ì  í˜‘ë ¥",
            "improvement_points": ["í‰ê°€ ì¬ì‹œë„ í•„ìš”"],
        }