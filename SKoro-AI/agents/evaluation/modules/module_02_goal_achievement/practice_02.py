# -*- coding: utf-8 -*-
"""
ëª¨ë“ˆ2: ëª©í‘œë‹¬ì„±ë„ ë¶„ì„ ëª¨ë“ˆ - ì™„ì „ êµ¬í˜„

ìƒì˜ ë‚´ìš© ë°˜ì˜ì‚¬í•­:
1. 6ë‹¨ê³„ ì„œë¸Œëª¨ë“ˆ êµ¬ì¡° (ë“±ê¸‰ ì‚°ì •ì„ ë‹¬ì„±ë¥ ê³¼ í†µí•©)
2. evaluation_type ê¸°ë°˜ ì •ëŸ‰/ì •ì„± í‰ê°€ ë¶„ë¥˜
3. í•˜ì´ë¸Œë¦¬ë“œ ê°œì¸ ì¢…í•© ê¸°ì—¬ë„ ê³„ì‚° (ì°¸ì—¬ì ìˆ˜ ë³´ì • + KPI ë¹„ì¤‘)
4. LLM ë°°ì¹˜ ì²˜ë¦¬ + ì—ëŸ¬ ì²˜ë¦¬
5. êµ¬ì¡°í™”ëœ ì½”ë©˜íŠ¸ ìƒì„± (ë¶„ê¸°ë³„/ì—°ë§ í†¤ ì°¨ë³„í™”)
6. ë¶„ê¸°ë³„ ëˆ„ì  ì²˜ë¦¬ (ìµœì¢… ì„±ê³¼ ê¸°ì¤€)
7. íŒ€ì› ë³€ê²½ì‹œ ì‹¤ì œ ì°¸ì—¬ ê¸°ê°„ë§Œ í‰ê°€
8. íŒ€ ë‹¨ìœ„ ì½”ë©˜íŠ¸ ì¼ê´€ì„± ê°€ì´ë“œ
9. grades.grade_rule ê¸°ë°˜ í‰ê°€ ê¸°ì¤€ ì¶”ì¶œ
10. DB ê¸°ë°˜ ê²½ëŸ‰ State ì „ë‹¬ ë°©ì‹
"""

import sys
import os
import json
import re
import time
import logging
from typing import Dict, List, Optional, Any, Literal, TypedDict
from dataclasses import dataclass
from functools import lru_cache
from datetime import datetime

# í™˜ê²½ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, '../../../../'))  # 4ë‹¨ê³„ë¡œ ìˆ˜ì •
sys.path.append(project_root)

from config.settings import DatabaseConfig
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage
from sqlalchemy import create_engine, text
from sqlalchemy.engine import Row
from langgraph.graph import StateGraph, START, END  # LangGraph ì¶”ê°€

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# httpx ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê·¸ ë ˆë²¨ ì¡°ì • (HTTP ìš”ì²­ ë¡œê·¸ ìˆ¨ê¹€)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("openai").setLevel(logging.WARNING)
logging.getLogger("langchain").setLevel(logging.WARNING)
logging.getLogger("langchain_openai").setLevel(logging.WARNING)

# DB ë° LLM ì„¤ì •
db_config = DatabaseConfig()
DATABASE_URL = db_config.DATABASE_URL
engine = create_engine(DATABASE_URL, pool_pre_ping=True)
llm_client = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# ===== ìƒíƒœ ì •ì˜ =====
@dataclass
class Module2State:
    """ê²½ëŸ‰ State - ìš°ë¦¬ê°€ ìƒì˜í•œ DB ê¸°ë°˜ ì „ë‹¬ ë°©ì‹"""
    # ê¸°ë³¸ ì •ë³´
    report_type: Literal["quarterly", "annual"]
    team_id: int
    period_id: int
    
    # íƒ€ê²Ÿ IDë“¤
    target_task_ids: List[int]
    target_team_kpi_ids: List[int]
    
    # ì²˜ë¦¬ ê²°ê³¼ ì¶”ì ìš© (DB IDë§Œ ì €ì¥)
    updated_task_ids: Optional[List[int]] = None
    updated_team_kpi_ids: Optional[List[int]] = None
    feedback_report_ids: Optional[List[int]] = None
    team_evaluation_id: Optional[int] = None
    final_evaluation_report_ids: Optional[List[int]] = None
    
    # íŠ¹ë³„ ì „ë‹¬ ë°ì´í„° (ì„œë¸Œëª¨ë“ˆ ê°„ í•„ìš”ì‹œë§Œ)
    team_context_guide: Optional[Dict] = None

# ===== ì—ëŸ¬ ì²˜ë¦¬ í´ë˜ìŠ¤ =====
class LLMValidationError(Exception):
    pass

class DataIntegrityError(Exception):
    pass

# ===== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ =====
def row_to_dict(row: Row) -> Dict[str, Any]:
    """SQLAlchemy Row ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
    if row is None:
        return {}
    return row._asdict()

def extract_json_from_llm_response(text: str) -> str:
    """LLM ì‘ë‹µì—ì„œ JSON ë¸”ë¡ ì¶”ì¶œ"""
    match = re.search(r"```(?:json)?\s*(.*?)\s*```", text, re.DOTALL)
    if match:
        return match.group(1).strip()
    return text.strip()

def extract_number_from_response(response: str) -> float:
    """ì‘ë‹µì—ì„œ ìˆ«ì ì¶”ì¶œ"""
    patterns = [
        r'^(\d+(?:\.\d+)?)$',           # "85", "85.5"
        r'(\d+(?:\.\d+)?)%',            # "85%"  
        r'(\d+(?:\.\d+)?)\s*ì ',         # "85ì "
        r':(\d+(?:\.\d+)?)(?:[:%]|$)',  # "1:85", "1:85%"
    ]
    
    for pattern in patterns:
        match = re.search(pattern, response.strip())
        if match:
            return float(match.group(1))
    
    raise ValueError(f"No valid number found in response: {response}")

def safe_divide(numerator: float, denominator: float, default: float = 0) -> float:
    """ì•ˆì „í•œ ë‚˜ëˆ„ê¸°"""
    if denominator == 0:
        print(f"   âš ï¸  ë‚˜ëˆ„ê¸° ì˜¤ë¥˜: {numerator}/{denominator}, ê¸°ë³¸ê°’ {default} ì‚¬ìš©")
        return default
    return numerator / denominator

def calculate_weighted_average(values: List[float], weights: List[float], default: float = 0) -> float:
    """ê°€ì¤‘í‰ê·  ê³„ì‚°: Î£(ê°’ Ã— ê°€ì¤‘ì¹˜) / Î£(ê°€ì¤‘ì¹˜)"""
    if not values or not weights or len(values) != len(weights):
        return default
    
    weighted_sum = sum(value * weight for value, weight in zip(values, weights))
    total_weight = sum(weights)
    
    return safe_divide(weighted_sum, total_weight, default)

def calculate_individual_weighted_achievement_rate(individual_tasks: List[Dict]) -> Dict[str, float]:
    """ê°œì¸ë³„ ê°€ì¤‘í‰ê·  ë‹¬ì„±ë¥  ê³„ì‚°"""
    if not individual_tasks:
        return {"achievement_rate": 0, "contribution_rate": 0, "total_weight": 0}
    
    # ë‹¬ì„±ë¥  ê°€ì¤‘í‰ê·  ê³„ì‚°
    achievement_rates = []
    weights = []
    
    for task in individual_tasks:
        achievement_rate = task.get('ai_achievement_rate', 0)
        weight = task.get('weight', 0)
        
        achievement_rates.append(achievement_rate)
        weights.append(weight)
    
    # ê°€ì¤‘í‰ê·  ë‹¬ì„±ë¥ 
    weighted_achievement = calculate_weighted_average(achievement_rates, weights, 0)
    
    # ê¸°ì—¬ë„ëŠ” ë‹¨ìˆœí‰ê·  (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
    contribution_rates = [task.get('ai_contribution_score', 0) for task in individual_tasks]
    avg_contribution = sum(contribution_rates) / len(contribution_rates) if contribution_rates else 0
    
    total_weight = sum(weights)
    
    return {
        "achievement_rate": weighted_achievement,
        "contribution_rate": avg_contribution,
        "total_weight": total_weight
    }

# ===== ë°ì´í„° ì¡°íšŒ í•¨ìˆ˜ =====
def fetch_team_evaluation_id(team_id: int, period_id: int) -> Optional[int]:
    """team_evaluationsì—ì„œ ID ì¡°íšŒ"""
    with engine.connect() as connection:
        query = text("""
            SELECT team_evaluation_id FROM team_evaluations 
            WHERE team_id = :team_id AND period_id = :period_id
        """)
        result = connection.execute(query, {"team_id": team_id, "period_id": period_id})
        return result.scalar_one_or_none()

def fetch_team_members(team_id: int) -> List[Dict]:
    """íŒ€ ë©¤ë²„ ì¡°íšŒ"""
    with engine.connect() as connection:
        query = text("""
            SELECT emp_no, emp_name, cl, position, role 
            FROM employees 
            WHERE team_id = :team_id
        """)
        results = connection.execute(query, {"team_id": team_id})
        return [row_to_dict(row) for row in results]

def fetch_cumulative_task_data(task_id: int, period_id: int) -> Dict:
    """ëˆ„ì  Task ë°ì´í„° ì¡°íšŒ - ìš°ë¦¬ê°€ ìƒì˜í•œ ë°©ì‹"""
    with engine.connect() as connection:
        query = text("""
            SELECT ts.*, t.task_name, t.target_level, t.weight, t.emp_no, t.team_kpi_id, 
                   e.emp_name, tk.kpi_name, tk.kpi_description
            FROM task_summaries ts
            JOIN tasks t ON ts.task_id = t.task_id
            JOIN employees e ON t.emp_no = e.emp_no
            JOIN team_kpis tk ON t.team_kpi_id = tk.team_kpi_id
            WHERE ts.task_id = :task_id AND ts.period_id <= :period_id
            ORDER BY ts.period_id
        """)
        results = connection.execute(query, {"task_id": task_id, "period_id": period_id})
        task_summaries = [row_to_dict(row) for row in results]
        
        if not task_summaries:
            return {}
        
        latest = task_summaries[-1]
        cumulative_summary = "\n".join([
            f"Q{ts['period_id']}: {ts['task_summary']}" 
            for ts in task_summaries if ts['task_summary']
        ])
        
        return {
            **latest,
            "cumulative_task_summary": cumulative_summary,
            "cumulative_performance": latest.get('task_performance', ''),
            "participation_periods": len(task_summaries)
        }

def fetch_team_kpi_data(team_kpi_id: int) -> Dict:
    """Team KPI ë°ì´í„° ì¡°íšŒ"""
    with engine.connect() as connection:
        query = text("""
            SELECT tk.*, g.grade_rule, g.grade_s, g.grade_a, g.grade_b, g.grade_c, g.grade_d
            FROM team_kpis tk
            LEFT JOIN grades g ON tk.team_kpi_id = g.team_kpi_id OR g.team_kpi_id IS NULL
            WHERE tk.team_kpi_id = :team_kpi_id
            LIMIT 1
        """)
        result = connection.execute(query, {"team_kpi_id": team_kpi_id})
        row = result.fetchone()
        return row_to_dict(row) if row else {}

def fetch_kpi_tasks(team_kpi_id: int, period_id: int) -> List[Dict]:
    """íŠ¹ì • KPIì˜ ìµœì‹  ë¶„ê¸° Taskë“¤ ì¡°íšŒ"""
    with engine.connect() as connection:
        query = text("""
            SELECT DISTINCT t.task_id, t.task_name, t.target_level, t.weight, t.emp_no,
                   e.emp_name, ts.task_summary, ts.task_performance, ts.period_id
            FROM tasks t
            JOIN employees e ON t.emp_no = e.emp_no
            JOIN task_summaries ts ON t.task_id = ts.task_id
            WHERE t.team_kpi_id = :team_kpi_id 
            AND ts.period_id = :period_id
        """)
        results = connection.execute(query, {"team_kpi_id": team_kpi_id, "period_id": period_id})
        return [row_to_dict(row) for row in results]

def check_evaluation_type(team_kpi_id: int) -> str:
    """evaluation_type í™•ì¸ (ì—†ìœ¼ë©´ ìë™ ë¶„ë¥˜)"""
    with engine.connect() as connection:
        query = text("SELECT evaluation_type FROM team_kpis WHERE team_kpi_id = :team_kpi_id")
        result = connection.execute(query, {"team_kpi_id": team_kpi_id})
        evaluation_type = result.scalar_one_or_none()
        
        if evaluation_type:
            return evaluation_type
            
        # ìë™ ë¶„ë¥˜ (LLM ê¸°ë°˜)
        kpi_data = fetch_team_kpi_data(team_kpi_id)
        auto_type = classify_kpi_type_by_llm(kpi_data)
        
        # DB ì—…ë°ì´íŠ¸
        update_query = text("""
            UPDATE team_kpis SET evaluation_type = :evaluation_type 
            WHERE team_kpi_id = :team_kpi_id
        """)
        connection.execute(update_query, {
            "evaluation_type": auto_type, 
            "team_kpi_id": team_kpi_id
        })
        connection.commit()
        
        return auto_type

# ===== ë°ì´í„° ì—…ë°ì´íŠ¸ í•¨ìˆ˜ =====
def update_task_summary(task_summary_id: int, data: Dict) -> bool:
    """task_summaries ì—…ë°ì´íŠ¸"""
    with engine.connect() as connection:
        set_clauses = [f"{k} = :{k}" for k in data.keys()]
        query = text(f"""
            UPDATE task_summaries 
            SET {', '.join(set_clauses)}
            WHERE task_summary_id = :task_summary_id
        """)
        result = connection.execute(query, {**data, "task_summary_id": task_summary_id})
        connection.commit()
        return result.rowcount > 0

def update_team_kpi(team_kpi_id: int, data: Dict) -> bool:
    """team_kpis ì—…ë°ì´íŠ¸"""
    with engine.connect() as connection:
        set_clauses = [f"{k} = :{k}" for k in data.keys()]
        query = text(f"""
            UPDATE team_kpis 
            SET {', '.join(set_clauses)}
            WHERE team_kpi_id = :team_kpi_id
        """)
        result = connection.execute(query, {**data, "team_kpi_id": team_kpi_id})
        connection.commit()
        return result.rowcount > 0

def save_feedback_report(emp_no: str, team_evaluation_id: int, data: Dict) -> int:
    """feedback_reports ì €ì¥/ì—…ë°ì´íŠ¸"""
    with engine.connect() as connection:
        # ê¸°ì¡´ ë ˆì½”ë“œ í™•ì¸
        check_query = text("""
            SELECT feedback_report_id FROM feedback_reports 
            WHERE emp_no = :emp_no AND team_evaluation_id = :team_evaluation_id
        """)
        existing_id = connection.execute(check_query, {
            "emp_no": emp_no, 
            "team_evaluation_id": team_evaluation_id
        }).scalar_one_or_none()
        
        if existing_id:
            # ì—…ë°ì´íŠ¸
            set_clauses = [f"{k} = :{k}" for k in data.keys()]
            update_query = text(f"""
                UPDATE feedback_reports 
                SET {', '.join(set_clauses)}
                WHERE feedback_report_id = :feedback_report_id
            """)
            connection.execute(update_query, {**data, "feedback_report_id": existing_id})
            connection.commit()
            return existing_id
        else:
            # ì‹ ê·œ ìƒì„±
            cols = ["emp_no", "team_evaluation_id"] + list(data.keys())
            placeholders = [f":{col}" for col in cols]
            insert_query = text(f"""
                INSERT INTO feedback_reports ({', '.join(cols)})
                VALUES ({', '.join(placeholders)})
            """)
            connection.execute(insert_query, {
                "emp_no": emp_no,
                "team_evaluation_id": team_evaluation_id,
                **data
            })
            connection.commit()
            
            # ìƒˆë¡œ ìƒì„±ëœ ID ì¡°íšŒ
            new_id = connection.execute(check_query, {
                "emp_no": emp_no, 
                "team_evaluation_id": team_evaluation_id
            }).scalar_one()
            return new_id

def update_team_evaluations(team_evaluation_id: int, data: Dict) -> bool:
    """team_evaluations ì—…ë°ì´íŠ¸"""
    with engine.connect() as connection:
        set_clauses = [f"{k} = :{k}" for k in data.keys()]
        query = text(f"""
            UPDATE team_evaluations 
            SET {', '.join(set_clauses)}
            WHERE team_evaluation_id = :team_evaluation_id
        """)
        result = connection.execute(query, {**data, "team_evaluation_id": team_evaluation_id})
        connection.commit()
        return result.rowcount > 0

def save_final_evaluation_report(emp_no: str, team_evaluation_id: int, data: Dict) -> int:
    """final_evaluation_reports ì €ì¥/ì—…ë°ì´íŠ¸"""
    with engine.connect() as connection:
        # ê¸°ì¡´ ë ˆì½”ë“œ í™•ì¸
        check_query = text("""
            SELECT final_evaluation_report_id FROM final_evaluation_reports 
            WHERE emp_no = :emp_no AND team_evaluation_id = :team_evaluation_id
        """)
        existing_id = connection.execute(check_query, {
            "emp_no": emp_no, 
            "team_evaluation_id": team_evaluation_id
        }).scalar_one_or_none()
        
        if existing_id:
            # ì—…ë°ì´íŠ¸
            set_clauses = [f"{k} = :{k}" for k in data.keys()]
            update_query = text(f"""
                UPDATE final_evaluation_reports 
                SET {', '.join(set_clauses)}
                WHERE final_evaluation_report_id = :final_evaluation_report_id
            """)
            connection.execute(update_query, {**data, "final_evaluation_report_id": existing_id})
            connection.commit()
            return existing_id
        else:
            # ì‹ ê·œ ìƒì„±
            cols = ["emp_no", "team_evaluation_id"] + list(data.keys())
            placeholders = [f":{col}" for col in cols]
            insert_query = text(f"""
                INSERT INTO final_evaluation_reports ({', '.join(cols)})
                VALUES ({', '.join(placeholders)})
            """)
            connection.execute(insert_query, {
                "emp_no": emp_no,
                "team_evaluation_id": team_evaluation_id,
                **data
            })
            connection.commit()
            
            # ìƒˆë¡œ ìƒì„±ëœ ID ì¡°íšŒ
            new_id = connection.execute(check_query, {
                "emp_no": emp_no, 
                "team_evaluation_id": team_evaluation_id
            }).scalar_one()
            return new_id

# ===== LLM í˜¸ì¶œ ë° ê²€ì¦ í•¨ìˆ˜ =====
def robust_llm_call(prompt: str, validation_func, max_retries: int = 3, context: str = ""):
    """ê²¬ê³ í•œ LLM í˜¸ì¶œ - ìš°ë¦¬ê°€ ìƒì˜í•œ ì—ëŸ¬ ì²˜ë¦¬"""
    last_error = None
    
    for attempt in range(max_retries):
        try:
            response = llm_client.invoke(prompt)
            content = str(response.content)
            validated_result = validation_func(content)
            return validated_result
            
        except Exception as e:
            last_error = e
            logger.warning(f"LLM call attempt {attempt + 1} failed for {context}: {e}")
            
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
    
    logger.error(f"All LLM attempts failed for {context}: {last_error}")
    raise LLMValidationError(f"Failed after {max_retries} attempts: {last_error}")

def validate_achievement_rate(response: str) -> Dict:
    """ë‹¬ì„±ë¥  ì‘ë‹µ ê²€ì¦"""
    try:
        json_output = extract_json_from_llm_response(response)
        data = json.loads(json_output)
        
        rate = data.get("achievement_rate")
        grade = data.get("grade", "")
        
        if not isinstance(rate, (int, float)) or not (0 <= rate <= 200):
            raise ValueError(f"Invalid achievement rate: {rate}")
        
        if grade and grade not in ["S", "A", "B", "C", "D"]:
            raise ValueError(f"Invalid grade: {grade}")
            
        return {
            "achievement_rate": round(float(rate), 2),
            "grade": grade if grade else None
        }
        
    except (json.JSONDecodeError, ValueError) as e:
        raise LLMValidationError(f"Achievement rate validation failed: {e}")

def validate_contribution_analysis(response: str) -> Dict:
    """ê¸°ì—¬ë„ ë¶„ì„ ì‘ë‹µ ê²€ì¦"""
    try:
        json_output = extract_json_from_llm_response(response)
        data = json.loads(json_output)
        
        kpi_rate = data.get("kpi_overall_rate")
        contributions = data.get("individual_contributions", {})
        
        if not isinstance(kpi_rate, (int, float)) or not (0 <= kpi_rate <= 200):
            raise ValueError(f"Invalid KPI rate: {kpi_rate}")
            
        # ê¸°ì—¬ë„ í•©ê³„ ê²€ì¦ (100% Â± 5% í—ˆìš©)
        total_contribution = sum(float(v) for v in contributions.values())
        if abs(total_contribution - 100.0) > 5.0:
            logger.warning(f"Contribution sum: {total_contribution}%, normalizing to 100%")
            # ì •ê·œí™”
            if total_contribution > 0:
                contributions = {k: round((float(v) / total_contribution) * 100, 2) 
                               for k, v in contributions.items()}
        
        return {
            "kpi_overall_rate": round(float(kpi_rate), 2),
            "individual_contributions": contributions,
            "kpi_analysis_comment": data.get("kpi_analysis_comment", "")
        }
        
    except (json.JSONDecodeError, ValueError) as e:
        raise LLMValidationError(f"Contribution analysis validation failed: {e}")

def classify_kpi_type_by_llm(kpi_data: Dict) -> str:
    """KPI í‰ê°€ ë°©ì‹ ìë™ ë¶„ë¥˜"""
    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content="""
        ë‹¤ìŒ KPIì˜ í‰ê°€ ë°©ì‹ì„ ë¶„ë¥˜í•´ì£¼ì„¸ìš”:
        
        - quantitative: ê°œì¸ë³„ ìˆ˜ì¹˜ ì„±ê³¼ë¥¼ í•©ì‚°í•˜ì—¬ ê¸°ì—¬ë„ ê³„ì‚° ê°€ëŠ¥
        - qualitative: ìˆ˜ì¹˜ì  ë¹„êµ ë¶ˆê°€ëŠ¥í•œ ì •ì„± í‰ê°€ í•„ìš”
        
        ë‹µë³€: quantitative ë˜ëŠ” qualitative (í•œ ë‹¨ì–´ë§Œ)
        """),
        HumanMessage(content=f"""
        KPIëª…: {kpi_data.get('kpi_name', '')}
        KPI ì„¤ëª…: {kpi_data.get('kpi_description', '')}
        """)
    ])
    
    def validate_type(response: str) -> str:
        response = response.strip().lower()
        if response in ["quantitative", "qualitative"]:
            return response
        raise ValueError(f"Invalid type: {response}")
    
    return robust_llm_call(str(prompt.format()), validate_type, context="KPI classification")

# ===== í‰ê°€ ê¸°ì¤€ ì²˜ë¦¬ =====
def get_evaluation_criteria(team_kpi_id: int) -> List[str]:
    """ìš°ë¦¬ê°€ ìƒì˜í•œ í‰ê°€ ê¸°ì¤€ ì²˜ë¦¬ ë¡œì§"""
    kpi_data = fetch_team_kpi_data(team_kpi_id)
    grade_rule = kpi_data.get('grade_rule')
    
    if grade_rule and grade_rule.strip():
        criteria = parse_criteria_from_grade_rule(grade_rule)
        if criteria:
            return criteria
    
    # ê¸°ë³¸ í‰ê°€ ê¸°ì¤€
    return ["ëª©í‘œë‹¬ì„± ê¸°ì—¬ë„", "ì„±ê³¼ ì˜í–¥ë ¥", "ì—…ë¬´ ì™„ì„±ë„"]

def parse_criteria_from_grade_rule(grade_rule: str) -> Optional[List[str]]:
    """grade_ruleì—ì„œ í‰ê°€ ê¸°ì¤€ ì¶”ì¶œ"""
    if not grade_rule or not grade_rule.strip():
        return None
    
    lines = grade_rule.strip().split('\n')
    criteria = []
    
    for line in lines:
        line = line.strip()
        # "- " ë˜ëŠ” "â€¢ " ì œê±°í•˜ê³  ë‚´ìš© ì¶”ì¶œ
        match = re.match(r'^[-â€¢]\s*(.+)$', line)
        if match:
            criteria.append(match.group(1).strip())
        elif line and not line.startswith(('-', 'â€¢')):
            criteria.append(line)
    
    # ë„ˆë¬´ ë§ì€ ê¸°ì¤€ì€ ì œí•œ
    if len(criteria) > 5:
        criteria = criteria[:5]
        
    return criteria if criteria else None

# ===== ì„œë¸Œëª¨ë“ˆ 1: ë°ì´í„° ìˆ˜ì§‘ =====
def data_collection_submodule(state: Module2State) -> Module2State:
    """ë°ì´í„° ìˆ˜ì§‘ ì„œë¸Œëª¨ë“ˆ"""
    print(f"   ğŸ“‹ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    
    # team_evaluation_id í™•ì¸/ìƒì„±
    team_evaluation_id = fetch_team_evaluation_id(state.team_id, state.period_id)
    if not team_evaluation_id:
        raise DataIntegrityError(f"team_evaluation_id not found for team {state.team_id}, period {state.period_id}")
    
    state.team_evaluation_id = team_evaluation_id
    
    # evaluation_type í™•ì¸/ì„¤ì •
    for kpi_id in state.target_team_kpi_ids:
        evaluation_type = check_evaluation_type(kpi_id)
        print(f"      â€¢ KPI {kpi_id}: {evaluation_type} í‰ê°€")
    
    print(f"   âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
    return state

# ===== ì„œë¸Œëª¨ë“ˆ 2: ë‹¬ì„±ë¥ +ë“±ê¸‰ ê³„ì‚° =====
def achievement_and_grade_calculation_submodule(state: Module2State) -> Module2State:
    """ë‹¬ì„±ë¥ +ë“±ê¸‰ ê³„ì‚° ì„œë¸Œëª¨ë“ˆ (í†µí•©) - ìš°ë¦¬ê°€ ìƒì˜í•œ ë°°ì¹˜ ì²˜ë¦¬"""
    print(f"   ğŸ¯ ë‹¬ì„±ë¥  ë° ë“±ê¸‰ ê³„ì‚° ì¤‘...")
    
    updated_task_ids = []
    batch_data = []
    
    # ë°°ì¹˜ìš© ë°ì´í„° ì¤€ë¹„
    for task_id in state.target_task_ids:
        task_data = fetch_cumulative_task_data(task_id, state.period_id)
        if not task_data:
            continue
            
        batch_data.append({
            "task_id": task_id,
            "task_summary_id": task_data.get('task_summary_id'),
            "target_level": task_data.get('target_level', ''),
            "cumulative_performance": task_data.get('cumulative_performance', ''),
            "cumulative_summary": task_data.get('cumulative_task_summary', ''),
            "kpi_data": fetch_team_kpi_data(task_data.get('team_kpi_id') or 0)
        })
    
    # ë°°ì¹˜ ì²˜ë¦¬ (15ê°œì”©)
    batch_size = 15
    for i in range(0, len(batch_data), batch_size):
        batch = batch_data[i:i+batch_size]
        results = batch_calculate_achievement_and_grades(batch, state.report_type == "annual")
        
        # ê²°ê³¼ ì €ì¥
        for task_data, result in zip(batch, results):
            task_summary_id = task_data['task_summary_id']
            if not task_summary_id:
                continue
                
            update_data = {
                "ai_achievement_rate": int(result['achievement_rate'])
            }
            
            # ì—°ë§ì¸ ê²½ìš° ë“±ê¸‰ë„ ì €ì¥
            if state.report_type == "annual" and result.get('grade'):
                update_data["ai_assessed_grade"] = result['grade']
            
            if update_task_summary(task_summary_id, update_data):
                updated_task_ids.append(task_data['task_id'])
    
    state.updated_task_ids = updated_task_ids
    print(f"   âœ… ë‹¬ì„±ë¥  ê³„ì‚° ì™„ë£Œ: {len(updated_task_ids)}ê°œ Task ì—…ë°ì´íŠ¸")
    return state

def batch_calculate_achievement_and_grades(batch_data: List[Dict], include_grades: bool) -> List[Dict]:
    """ë°°ì¹˜ ë‹¬ì„±ë¥ +ë“±ê¸‰ ê³„ì‚°"""
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    tasks_text = ""
    for i, data in enumerate(batch_data):
        tasks_text += f"\n{i+1}. Task ID: {data['task_id']}\n"
        tasks_text += f"   ëª©í‘œ: {data['target_level']}\n"
        tasks_text += f"   ì„±ê³¼: {data['cumulative_performance']}\n"
        tasks_text += f"   ìƒì„¸: {data['cumulative_summary'][:200]}...\n"
    
    if include_grades:
        prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content="""
            ë‹¤ìŒ Taskë“¤ì˜ ë‹¬ì„±ë¥ (0-200%)ê³¼ ë“±ê¸‰(S,A,B,C,D)ì„ ê³„ì‚°í•´ì£¼ì„¸ìš”.
            
            í‰ê°€ ê¸°ì¤€:
            - ë‹¬ì„±ë¥ : ëª©í‘œ ëŒ€ë¹„ ì‹¤ì œ ì„±ê³¼ (100% = ëª©í‘œ ë‹¬ì„±, 100% ì´ˆê³¼ = ëª©í‘œ ì´ˆê³¼)
            - ë“±ê¸‰: S(ì´ˆê³¼ë‹¬ì„±), A(ì™„ì „ë‹¬ì„±), B(ì–‘í˜¸), C(ë¯¸í¡), D(ë¶ˆëŸ‰)
            
            í˜„ì¬ ì„±ê³¼ë¥¼ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.
            JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
            """),
            HumanMessage(content=f"""
            {tasks_text}
            
            ë‹µë³€ í˜•ì‹:
            {{
              "results": [
                {{"task_id": 1, "achievement_rate": 85, "grade": "B"}},
                {{"task_id": 2, "achievement_rate": 120, "grade": "S"}}
              ]
            }}
            """)
        ])
    else:
        prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content="""
            ë‹¤ìŒ Taskë“¤ì˜ ë‹¬ì„±ë¥ (0-200%)ì„ ê³„ì‚°í•´ì£¼ì„¸ìš”.
            
            ë‹¬ì„±ë¥  ê¸°ì¤€: ëª©í‘œ ëŒ€ë¹„ ì‹¤ì œ ì„±ê³¼ (100% = ëª©í‘œ ë‹¬ì„±)
            í˜„ì¬ ì„±ê³¼ë¥¼ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.
            """),
            HumanMessage(content=f"""
            {tasks_text}
            
            ë‹µë³€ í˜•ì‹:
            {{
              "results": [
                {{"task_id": 1, "achievement_rate": 85}},
                {{"task_id": 2, "achievement_rate": 120}}
              ]
            }}
            """)
        ])
    
    def validate_batch_response(response: str) -> List[Dict]:
        try:
            json_output = extract_json_from_llm_response(response)
            data = json.loads(json_output)
            results = data.get("results", [])
            
            validated_results = []
            for result in results:
                rate = result.get("achievement_rate")
                if not isinstance(rate, (int, float)) or not (0 <= rate <= 200):
                    rate = 80.0  # ê¸°ë³¸ê°’
                
                validated_result: Dict[str, Any] = {"achievement_rate": round(float(rate), 2)}
                
                if include_grades:
                    grade = result.get("grade", "C")
                    if grade not in ["S", "A", "B", "C", "D"]:
                        grade = "C"
                    validated_result["grade"] = str(grade)
                
                validated_results.append(validated_result)
            
            return validated_results
            
        except Exception as e:
            logger.error(f"Batch validation failed: {e}")
            # ê¸°ë³¸ê°’ìœ¼ë¡œ í´ë°±
            fallback_results = []
            for _ in batch_data:
                result: Dict[str, Any] = {"achievement_rate": 80.0}
                if include_grades:
                    result["grade"] = "C"
                fallback_results.append(result)
            return fallback_results
    
    return robust_llm_call(str(prompt.format()), validate_batch_response, context="batch achievement calculation")

# ===== ì„œë¸Œëª¨ë“ˆ 3: ê¸°ì—¬ë„ ê³„ì‚° =====
def contribution_calculation_submodule(state: Module2State) -> Module2State:
    """ê¸°ì—¬ë„ ê³„ì‚° ì„œë¸Œëª¨ë“ˆ - ìš°ë¦¬ê°€ ìƒì˜í•œ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹"""
    print(f"   âš–ï¸ ê¸°ì—¬ë„ ê³„ì‚° ì¤‘...")
    
    updated_task_ids = []
    kpi_contributions_by_emp = {}  # {emp_no: total_score} - í•˜ì´ë¸Œë¦¬ë“œ 3ë‹¨ê³„ ê²°ê³¼
    
    # KPIë³„ë¡œ ì²˜ë¦¬
    for kpi_id in state.target_team_kpi_ids:
        evaluation_type = check_evaluation_type(kpi_id)
        kpi_data = fetch_team_kpi_data(kpi_id)
        
        if evaluation_type == "quantitative":
            # ì •ëŸ‰ í‰ê°€: ê°œì¸ì„±ê³¼/íŒ€ì „ì²´ì„±ê³¼ Ã— 100
            contributions = calculate_quantitative_contributions(kpi_id, state.period_id)
        else:
            # ì •ì„± í‰ê°€: LLM ê¸°ë°˜ ìƒëŒ€ í‰ê°€
            contributions = calculate_qualitative_contributions(kpi_id, state.period_id, kpi_data)
        
        # í•˜ì´ë¸Œë¦¬ë“œ 1ë‹¨ê³„: ì°¸ì—¬ì ìˆ˜ ë³´ì •
        kpi_tasks = fetch_kpi_tasks(kpi_id, state.period_id)
        participants_count = len(set(task['emp_no'] for task in kpi_tasks))
        
        print(f"      â€¢ KPI {kpi_id}: {evaluation_type} í‰ê°€, ì°¸ì—¬ì {participants_count}ëª…")
        
        for emp_no, contribution_rate in contributions.items():
            # 1ë‹¨ê³„: ì°¸ì—¬ì ìˆ˜ ë³´ì •
            adjusted_score = contribution_rate * participants_count
            
            # 2ë‹¨ê³„: KPI ë¹„ì¤‘ ì ìš©
            kpi_weight = kpi_data.get('weight', 0) / 100.0
            weighted_score = adjusted_score * kpi_weight
            
            if emp_no not in kpi_contributions_by_emp:
                kpi_contributions_by_emp[emp_no] = 0
            kpi_contributions_by_emp[emp_no] += weighted_score
            
            print(f"        - {emp_no}: ì›ë˜ {contribution_rate:.1f}% â†’ ë³´ì • {adjusted_score:.1f} â†’ ê°€ì¤‘ {weighted_score:.1f}")
        
        # Taskë³„ ê¸°ì—¬ë„ ì—…ë°ì´íŠ¸ (ì›ë˜ KPIë³„ ê¸°ì—¬ë„ ì €ì¥)
        for task in kpi_tasks:
            task_data = fetch_cumulative_task_data(task['task_id'], state.period_id)
            if not task_data:
                continue
                
            emp_contribution = contributions.get(task['emp_no'], 0)
            
            update_data = {
                "ai_contribution_score": int(emp_contribution)  # KPIë³„ ì›ë˜ ê¸°ì—¬ë„
            }
            
            if update_task_summary(task_data['task_summary_id'], update_data):
                updated_task_ids.append(task['task_id'])
    
    # í•˜ì´ë¸Œë¦¬ë“œ 3ë‹¨ê³„: íŒ€ ë‚´ % ê¸°ì—¬ë„ ë³€í™˜
    total_team_score = sum(kpi_contributions_by_emp.values())
    final_contributions = {}
    
    if total_team_score > 0:
        for emp_no in kpi_contributions_by_emp:
            percentage = (kpi_contributions_by_emp[emp_no] / total_team_score) * 100
            final_contributions[emp_no] = round(percentage, 2)
            print(f"      â€¢ {emp_no} ìµœì¢… ê¸°ì—¬ë„: {percentage:.1f}%")
    else:
        # íŒ€ ì ìˆ˜ê°€ 0ì¸ ê²½ìš° ë™ë“± ë¶„ë°°
        emp_count = len(kpi_contributions_by_emp)
        if emp_count > 0:
            equal_share = 100.0 / emp_count
            for emp_no in kpi_contributions_by_emp:
                final_contributions[emp_no] = round(equal_share, 2)
    
    # ìµœì¢… ê¸°ì—¬ë„ë¥¼ feedback_reports ë˜ëŠ” final_evaluation_reportsì— ì €ì¥
    save_final_contributions_to_db(state, final_contributions)
    
    # ë””ë²„ê¹…: í•˜ì´ë¸Œë¦¬ë“œ ê³„ì‚° ê³¼ì • ì‹œê°í™”
    debug_contribution_calculation(state)
    
    state.updated_task_ids = list(set((state.updated_task_ids or []) + updated_task_ids))
    print(f"   âœ… ê¸°ì—¬ë„ ê³„ì‚° ì™„ë£Œ: {len(updated_task_ids)}ê°œ Task ì—…ë°ì´íŠ¸, {len(final_contributions)}ëª… ìµœì¢… ê¸°ì—¬ë„ ì €ì¥")
    return state

def save_final_contributions_to_db(state: Module2State, final_contributions: Dict[str, float]):
    """ìµœì¢… ê¸°ì—¬ë„ë¥¼ DBì— ì €ì¥"""
    team_members = fetch_team_members(state.team_id)
    
    for member in team_members:
        if member.get('role') == 'MANAGER':
            continue
            
        emp_no = member['emp_no']
        final_contribution = final_contributions.get(emp_no, 0)
        
        if state.report_type == "quarterly":
            # ë¶„ê¸°ë³„: feedback_reportsì— ì €ì¥
            save_feedback_report(
                emp_no, 
                state.team_evaluation_id or 0,
                {"contribution_rate": int(final_contribution)}  # ê¸°ì¡´ ì»¬ëŸ¼ëª… ì‚¬ìš©
            )
        else:
            # ì—°ë§: final_evaluation_reportsì— ì €ì¥
            save_final_evaluation_report(
                emp_no,
                state.team_evaluation_id or 0,
                {"contribution_rate": int(final_contribution)}  # ê¸°ì¡´ ì»¬ëŸ¼ëª… ì‚¬ìš©
            )

def debug_contribution_calculation(state: Module2State):
    """ê¸°ì—¬ë„ ê³„ì‚° ê³¼ì • ë””ë²„ê¹… - í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ ê²€ì¦"""
    print(f"\nğŸ” ê¸°ì—¬ë„ ê³„ì‚° ê³¼ì • ë””ë²„ê¹…")
    print(f"{'='*50}")
    
    # 1ë‹¨ê³„: KPIë³„ ì›ë˜ ê¸°ì—¬ë„ ìˆ˜ì§‘
    kpi_contributions = {}
    for kpi_id in state.target_team_kpi_ids:
        evaluation_type = check_evaluation_type(kpi_id)
        kpi_data = fetch_team_kpi_data(kpi_id)
        kpi_tasks = fetch_kpi_tasks(kpi_id, state.period_id)
        participants_count = len(set(task['emp_no'] for task in kpi_tasks))
        
        if evaluation_type == "quantitative":
            contributions = calculate_quantitative_contributions(kpi_id, state.period_id)
        else:
            contributions = calculate_qualitative_contributions(kpi_id, state.period_id, kpi_data)
        
        kpi_contributions[kpi_id] = {
            'kpi_name': kpi_data.get('kpi_name', f'KPI{kpi_id}'),
            'weight': kpi_data.get('weight', 0),
            'participants_count': participants_count,
            'contributions': contributions
        }
    
    # 2ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ê³„ì‚° ê³¼ì • ì‹œê°í™”
    print(f"ğŸ“Š KPIë³„ ê¸°ì—¬ë„ ë¶„ì„:")
    for kpi_id, kpi_info in kpi_contributions.items():
        print(f"\nğŸ¯ {kpi_info['kpi_name']} (ë¹„ì¤‘: {kpi_info['weight']}%, ì°¸ì—¬ì: {kpi_info['participants_count']}ëª…)")
        print(f"   ì›ë˜ ê¸°ì—¬ë„ â†’ ì°¸ì—¬ììˆ˜ ë³´ì • â†’ KPI ë¹„ì¤‘ ì ìš©")
        print(f"   {'â”€' * 50}")
        
        for emp_no, original_rate in kpi_info['contributions'].items():
            # 1ë‹¨ê³„: ì°¸ì—¬ì ìˆ˜ ë³´ì •
            adjusted = original_rate * kpi_info['participants_count']
            # 2ë‹¨ê³„: KPI ë¹„ì¤‘ ì ìš©
            weighted = adjusted * (kpi_info['weight'] / 100.0)
            
            print(f"   {emp_no}: {original_rate:5.1f}% â†’ {adjusted:6.1f} â†’ {weighted:6.1f}")
    
    # 3ë‹¨ê³„: ê°œì¸ë³„ ì¢…í•© ì ìˆ˜ ê³„ì‚°
    print(f"\nğŸ“ˆ ê°œì¸ë³„ ì¢…í•© ì ìˆ˜ (í•˜ì´ë¸Œë¦¬ë“œ 1-2ë‹¨ê³„ ê²°ê³¼):")
    emp_total_scores = {}
    
    for kpi_id, kpi_info in kpi_contributions.items():
        for emp_no, original_rate in kpi_info['contributions'].items():
            if emp_no not in emp_total_scores:
                emp_total_scores[emp_no] = 0
            
            adjusted = original_rate * kpi_info['participants_count']
            weighted = adjusted * (kpi_info['weight'] / 100.0)
            emp_total_scores[emp_no] += weighted
    
    for emp_no, total_score in sorted(emp_total_scores.items(), key=lambda x: x[1], reverse=True):
        print(f"   {emp_no}: {total_score:.1f}ì ")
    
    # 4ë‹¨ê³„: íŒ€ ë‚´ % ê¸°ì—¬ë„ ë³€í™˜
    total_team_score = sum(emp_total_scores.values())
    print(f"\nğŸ† ìµœì¢… ê¸°ì—¬ë„ (í•˜ì´ë¸Œë¦¬ë“œ 3ë‹¨ê³„ ê²°ê³¼):")
    print(f"   íŒ€ ì „ì²´ ì ìˆ˜: {total_team_score:.1f}")
    print(f"   {'â”€' * 30}")
    
    for emp_no, total_score in sorted(emp_total_scores.items(), key=lambda x: x[1], reverse=True):
        final_percentage = (total_score / total_team_score) * 100 if total_team_score > 0 else 0
        print(f"   {emp_no}: {final_percentage:.1f}% ({total_score:.1f}ì )")
    
    print(f"{'='*50}")

def calculate_quantitative_contributions(kpi_id: int, period_id: int) -> Dict[str, float]:
    """ì •ëŸ‰ í‰ê°€ ê¸°ì—¬ë„ ê³„ì‚°"""
    tasks = fetch_kpi_tasks(kpi_id, period_id)
    
    # ê°œì¸ë³„ ì„±ê³¼ ìˆ˜ì§‘
    emp_performance = {}
    for task in tasks:
        emp_no = task['emp_no']
        performance_text = task.get('task_performance', '')
        
        # ì„±ê³¼ì—ì„œ ìˆ˜ì¹˜ ì¶”ì¶œ ì‹œë„
        try:
            performance_value = extract_number_from_response(performance_text)
            if emp_no not in emp_performance:
                emp_performance[emp_no] = 0
            emp_performance[emp_no] += performance_value
        except:
            # ìˆ˜ì¹˜ ì¶”ì¶œ ì‹¤íŒ¨ì‹œ ë™ë“± ë¶„ë°°
            emp_performance[emp_no] = 1.0
    
    # ê¸°ì—¬ë„ ê³„ì‚°
    total_performance = sum(emp_performance.values())
    contributions = {}
    
    for emp_no, performance in emp_performance.items():
        contribution_rate = safe_divide(performance, total_performance, 1/len(emp_performance)) * 100
        contributions[emp_no] = round(contribution_rate, 2)
    
    return contributions

def calculate_qualitative_contributions(kpi_id: int, period_id: int, kpi_data: Dict) -> Dict[str, float]:
    """ì •ì„± í‰ê°€ ê¸°ì—¬ë„ ê³„ì‚° - ìš°ë¦¬ê°€ ìƒì˜í•œ grade_rule ê¸°ë°˜"""
    tasks = fetch_kpi_tasks(kpi_id, period_id)
    evaluation_criteria = get_evaluation_criteria(kpi_id)
    
    # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    criteria_text = "\n".join([f"- {criterion}" for criterion in evaluation_criteria])
    
    tasks_text = ""
    emp_nos = []
    for task in tasks:
        emp_nos.append(task['emp_no'])
        tasks_text += f"\n- {task['emp_name']}({task['emp_no']}): {task['task_name']}\n"
        tasks_text += f"  ë‚´ìš©: {task['task_summary']}\n"
        tasks_text += f"  ì„±ê³¼: {task['task_performance']}\n"
    
    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=f"""
        ë‹¤ìŒ KPIì— ëŒ€í•´ íŒ€ì›ë“¤ì˜ ìƒëŒ€ì  ê¸°ì—¬ë„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.
        
        í‰ê°€ ê¸°ì¤€:
        {criteria_text}
        
        ëª¨ë“  íŒ€ì›ì˜ ê¸°ì—¬ë„ í•©ê³„ëŠ” ì •í™•íˆ 100%ê°€ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
        í˜„ì¬ ì„±ê³¼ì™€ ê¸°ì—¬ë„ë¥¼ ê°ê´€ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.
        """),
        HumanMessage(content=f"""
        KPI: {kpi_data.get('kpi_name', '')}
        ì„¤ëª…: {kpi_data.get('kpi_description', '')}
        
        íŒ€ì›ë³„ ì—…ë¬´:
        {tasks_text}
        
        JSON ë‹µë³€:
        {{
          "kpi_overall_rate": [KPI ì „ì²´ ì§„í–‰ë¥  0-200%],
          "individual_contributions": {{
            "{emp_nos[0] if emp_nos else 'EMP001'}": [ê¸°ì—¬ë„ 0-100%],
            "{emp_nos[1] if len(emp_nos) > 1 else 'EMP002'}": [ê¸°ì—¬ë„ 0-100%]
          }},
          "kpi_analysis_comment": "[í˜„ì¬ ìƒíƒœ ë¶„ì„ ì½”ë©˜íŠ¸]"
        }}
        """)
    ])
    
    result = robust_llm_call(str(prompt.format()), validate_contribution_analysis, context=f"KPI {kpi_id} qualitative analysis")
    
    # KPI ë ˆë²¨ ê²°ê³¼ ì €ì¥
    update_team_kpi(kpi_id, {
        "ai_kpi_progress_rate": int(result['kpi_overall_rate']),
        "ai_kpi_analysis_comment": result['kpi_analysis_comment']
    })
    
    return result['individual_contributions']

# ===== ì„œë¸Œëª¨ë“ˆ 4: íŒ€ ëª©í‘œ ë¶„ì„ =====
def team_analysis_submodule(state: Module2State) -> Module2State:
    """íŒ€ ëª©í‘œ ë¶„ì„ ì„œë¸Œëª¨ë“ˆ - ìš°ë¦¬ê°€ ìƒì˜í•œ LLM ê¸°ë°˜"""
    print(f"   ğŸ¢ íŒ€ ëª©í‘œ ë¶„ì„ ì¤‘...")
    
    updated_kpi_ids = []
    kpi_rates = []
    
    # ì •ëŸ‰ í‰ê°€ KPIë“¤ ì²˜ë¦¬ (LLMìœ¼ë¡œ íŒ€ KPI ë‹¬ì„±ë¥  ê³„ì‚°)
    for kpi_id in state.target_team_kpi_ids:
        evaluation_type = check_evaluation_type(kpi_id)
        
        if evaluation_type == "quantitative":
            # ì •ëŸ‰ KPIë„ LLMì´ ì¢…í•© íŒë‹¨
            kpi_data = fetch_team_kpi_data(kpi_id)
            kpi_rate = calculate_team_kpi_achievement_rate(kpi_id, state.period_id, kpi_data)
            
            update_data = {
                "ai_kpi_progress_rate": int(kpi_rate['rate']),
                "ai_kpi_analysis_comment": kpi_rate['comment']
            }
            
            if update_team_kpi(kpi_id, update_data):
                updated_kpi_ids.append(kpi_id)
                kpi_rates.append(kpi_rate['rate'])
        else:
            # ì •ì„± KPIëŠ” ì´ë¯¸ ì„œë¸Œëª¨ë“ˆ 3ì—ì„œ ì²˜ë¦¬ë¨
            kpi_data = fetch_team_kpi_data(kpi_id)
            if kpi_data.get('ai_kpi_progress_rate') is not None:
                kpi_rates.append(kpi_data['ai_kpi_progress_rate'])
    
    # íŒ€ ì „ì²´ í‰ê·  ë‹¬ì„±ë¥  ê³„ì‚° (KPI ë¹„ì¤‘ ê³ ë ¤)
    team_average_rate = calculate_team_average_achievement_rate(state.target_team_kpi_ids)
    
    # team_evaluations ì—…ë°ì´íŠ¸
    team_eval_data = {
        "average_achievement_rate": int(team_average_rate)
    }
    
    # ì—°ë§ì¸ ê²½ìš° ì „ë…„ ëŒ€ë¹„ ì„±ì¥ë¥  ê³„ì‚° ì‹œë„
    if state.report_type == "annual":
        yoy_growth = calculate_year_over_year_growth(state.team_id, state.period_id, team_average_rate)
        if yoy_growth is not None:
            team_eval_data["year_over_year_growth"] = int(yoy_growth)
    
    if update_team_evaluations(state.team_evaluation_id or 0, team_eval_data):
        print(f"      â€¢ íŒ€ í‰ê·  ë‹¬ì„±ë¥ : {team_average_rate:.1f}%")
    
    state.updated_team_kpi_ids = updated_kpi_ids
    print(f"   âœ… íŒ€ ë¶„ì„ ì™„ë£Œ: {len(updated_kpi_ids)}ê°œ KPI ì—…ë°ì´íŠ¸")
    return state

def calculate_team_kpi_achievement_rate(kpi_id: int, period_id: int, kpi_data: Dict) -> Dict:
    """íŒ€ KPI ë‹¬ì„±ë¥  LLM ê³„ì‚°"""
    tasks = fetch_kpi_tasks(kpi_id, period_id)
    
    tasks_text = ""
    for task in tasks:
        tasks_text += f"\n- {task['emp_name']}: {task['task_name']}\n"
        tasks_text += f"  ëª©í‘œ: {task['target_level']}\n"
        tasks_text += f"  ì„±ê³¼: {task['task_performance']}\n"
    
    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content="""
        íŒ€ KPIì˜ ì „ì²´ ë‹¬ì„±ë¥ ì„ 0-200% ë²”ìœ„ë¡œ ê³„ì‚°í•´ì£¼ì„¸ìš”.
        ê°œë³„ íŒ€ì›ë“¤ì˜ ì„±ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ íŒ€ ì „ì²´ì˜ í˜„ì¬ ëª©í‘œ ë‹¬ì„± ìˆ˜ì¤€ì„ ê°ê´€ì ìœ¼ë¡œ íŒë‹¨í•´ì£¼ì„¸ìš”.
        """),
        HumanMessage(content=f"""
        KPI: {kpi_data.get('kpi_name', '')}
        KPI ëª©í‘œ: {kpi_data.get('kpi_description', '')}
        
        íŒ€ì›ë³„ ê°œë³„ ì„±ê³¼:
        {tasks_text}
        
        JSON ë‹µë³€:
        {{
            "kpi_overall_rate": [íŒ€ KPI ë‹¬ì„±ë¥  0-200%],
            "kpi_analysis_comment": "[í˜„ì¬ KPI ë‹¬ì„± ìƒíƒœ ë¶„ì„]"
        }}
        """)
    ])
    
    def validate_kpi_rate(response: str) -> Dict:
        try:
            json_output = extract_json_from_llm_response(response)
            data = json.loads(json_output)
            
            rate = data.get("kpi_overall_rate")
            if not isinstance(rate, (int, float)) or not (0 <= rate <= 200):
                rate = 80.0
                
            return {
                "rate": round(float(rate), 2),
                "comment": data.get("kpi_analysis_comment", "í˜„ì¬ ìƒíƒœ ë¶„ì„ ì‹¤íŒ¨")
            }
        except Exception as e:
            return {"rate": 80.0, "comment": f"KPI ë¶„ì„ ì‹¤íŒ¨: {str(e)[:100]}"}
    
    return robust_llm_call(str(prompt.format()), validate_kpi_rate, context=f"Team KPI {kpi_id}")

def calculate_team_average_achievement_rate(team_kpi_ids: List[int]) -> float:
    """íŒ€ ì „ì²´ í‰ê·  ë‹¬ì„±ë¥  ê³„ì‚° (KPI ë¹„ì¤‘ ê³ ë ¤)"""
    total_weight = 0
    weighted_sum = 0
    
    for kpi_id in team_kpi_ids:
        kpi_data = fetch_team_kpi_data(kpi_id)
        weight = kpi_data.get('weight', 0)
        rate = kpi_data.get('ai_kpi_progress_rate', 0)
        
        total_weight += weight
        weighted_sum += rate * weight
    
    return safe_divide(weighted_sum, total_weight, 80.0)

def calculate_year_over_year_growth(team_id: int, current_period_id: int, current_rate: float) -> Optional[float]:
    """ì „ë…„ ëŒ€ë¹„ ì„±ì¥ë¥  ê³„ì‚° (periods í…Œì´ë¸” í™œìš©)"""
    try:
        with engine.connect() as connection:
            # í˜„ì¬ periodì˜ ì—°ë„ ì¡°íšŒ
            cur_period_year = connection.execute(
                text("SELECT year FROM periods WHERE period_id = :pid"),
                {"pid": current_period_id}
            ).scalar_one_or_none()
            if not cur_period_year:
                return None

            # ì „ë…„ë„ ì—°ë§ period_id ì¡°íšŒ
            last_year = cur_period_year - 1
            last_period_id = connection.execute(
                text("SELECT period_id FROM periods WHERE year = :y AND is_final = 1"),
                {"y": last_year}
            ).scalar_one_or_none()
            if not last_period_id:
                return None

            # ì „ë…„ë„ ì—°ë§ íŒ€ ì„±ê³¼ ì¡°íšŒ
            last_year_rate = connection.execute(
                text("""
                    SELECT average_achievement_rate
                    FROM team_evaluations
                    WHERE team_id = :team_id AND period_id = :period_id
                """),
                {"team_id": team_id, "period_id": last_period_id}
            ).scalar_one_or_none()

            if last_year_rate and last_year_rate > 0:
                growth = ((current_rate - last_year_rate) / last_year_rate) * 100
                return round(growth, 2)
    except Exception as e:
        logger.warning(f"Year-over-year calculation failed: {e}")
    return None

# ===== íŒ€ ì¼ê´€ì„± ê°€ì´ë“œ ìƒì„± í•¨ìˆ˜ =====
def generate_team_consistency_guide(team_id: int, period_id: int) -> Dict:
    """íŒ€ ë‹¨ìœ„ ì¼ê´€ì„± ê°€ì´ë“œ ìƒì„± - ìš°ë¦¬ê°€ ìƒì˜í•œ ë°©ì‹"""
    team_members = fetch_team_members(team_id)
    team_avg_rate = calculate_team_average_achievement_rate(
        [kpi_id for kpi_id in range(1, 10)]  # ì„ì‹œë¡œ KPI ID ë²”ìœ„
    )
    
    # íŒ€ ì„±ê³¼ ìˆ˜ì¤€ì— ë”°ë¥¸ ê°€ì´ë“œë¼ì¸ ê²°ì •
    if team_avg_rate >= 90:
        performance_level = "high"
        tone_guide = "ì„±ê³¼ ì¤‘ì‹¬, êµ¬ì²´ì  ìˆ˜ì¹˜ ê°•ì¡°"
        style_guide = "ì „ë¬¸ì ì´ê³  ê°ê´€ì "
    elif team_avg_rate >= 70:
        performance_level = "average"
        tone_guide = "ê· í˜•ì , í˜„ì¬ ì„±ê³¼ ë¶„ì„"
        style_guide = "ê°ê´€ì ì´ê³  ë¶„ì„ì "
    else:
        performance_level = "improvement_needed"
        tone_guide = "í˜„ì¬ ìƒíƒœ ë¶„ì„, ì„±ê³¼ ìš”ì•½"
        style_guide = "ê°ê´€ì ì´ê³  êµ¬ì²´ì "
    
    return {
        "performance_level": performance_level,
        "tone_guide": tone_guide,
        "style_guide": style_guide,
        "length_target": 250,
        "length_tolerance": 30,
        "team_context": f"íŒ€ í‰ê·  ë‹¬ì„±ë¥  {team_avg_rate:.1f}%, {len(team_members)}ëª… êµ¬ì„±"
    }

# ===== í†µí•© ì½”ë©˜íŠ¸ ìƒì„± ì‹œìŠ¤í…œ =====
class CommentGenerator:
    """í†µí•© ì½”ë©˜íŠ¸ ìƒì„± ì‹œìŠ¤í…œ - ì¼ê´€ì„± ìˆëŠ” ì½”ë©˜íŠ¸ ìƒì„±"""
    
    def __init__(self, comment_type: str, period_type: str, team_guide: Optional[Dict] = None):
        self.comment_type = comment_type  # "task", "individual", "team", "kpi"
        self.period_type = period_type    # "quarterly", "annual"
        self.team_guide = team_guide or {}
        self.config = self._load_config()
    
    def _load_config(self) -> Dict:
        """ì½”ë©˜íŠ¸ íƒ€ì…ë³„ ì„¤ì • ë¡œë“œ"""
        base_configs = {
            "task": {
                "quarterly": {
                    "elements": ["ì„±ê³¼ìš”ì•½", "ì£¼ìš”í¬ì¸íŠ¸", "íŒ€ê¸°ì—¬ë„", "í˜„ì¬ìƒíƒœë¶„ì„"],
                    "tone": "ê°ê´€ì ì´ê³  ë¶„ì„ì ",
                    "focus": "í˜„ì¬ ì„±ê³¼ì™€ ê¸°ì—¬ë„ ë¶„ì„",
                    "length": {"target": 250, "tolerance": 30}
                },
                "annual": {
                    "elements": ["ì—°ê°„ìš”ì•½", "ì„±ì¥ì¶”ì´", "íŒ€ê¸°ì—¬ë„", "ì¢…í•©í‰ê°€"],
                    "tone": "ì¢…í•©ì ì´ê³  ê°ê´€ì ",
                    "focus": "ì—°ê°„ ì„±ê³¼ì™€ ì„±ì¥ ë¶„ì„",
                    "length": {"target": 300, "tolerance": 40}
                }
            },
            "individual": {
                "quarterly": {
                    "elements": ["ì „ì²´ì„±ê³¼ìš”ì•½", "ì£¼ìš”ì„±ê³¼í•˜ì´ë¼ì´íŠ¸", "ì„±ì¥í¬ì¸íŠ¸", "í˜„ì¬ì—­ëŸ‰í‰ê°€"],
                    "tone": "ê°ê´€ì ì´ê³  ë¶„ì„ì ",
                    "length": {"target": 350, "tolerance": 50}
                },
                "annual": {
                    "elements": ["ì—°ê°„ì„±ê³¼ì¢…í•©", "ë¶„ê¸°ë³„ì„±ì¥ì¶”ì´", "í•µì‹¬ê¸°ì—¬ì˜ì—­", "ì¢…í•©ì—­ëŸ‰í‰ê°€"],
                    "tone": "ì¢…í•©í‰ê°€ì ì´ê³  ê°ê´€ì ",
                    "length": {"target": 450, "tolerance": 50}
                }
            },
            "team": {
                "quarterly": {
                    "elements": ["íŒ€ì„±ê³¼ì¢…í•©", "íŒ€ì›ê¸°ì—¬ë¶„ì„", "ì£¼ìš”ì„±ê³¼ì˜ì—­", "íŒ€í˜„ì¬ìƒíƒœ"],
                    "tone": "ë¶„ì„ì ì´ê³  ê°ê´€ì ",
                    "length": {"target": 450, "tolerance": 50}
                },
                "annual": {
                    "elements": ["ì—°ê°„íŒ€ì„±ê³¼ìš”ì•½", "íŒ€ì¡°ì§ë ¥í‰ê°€", "í•µì‹¬ì„±ê³¼ê¸°ì—¬", "íŒ€ì¢…í•©í‰ê°€"],
                    "tone": "ì¢…í•©ì ì´ê³  ê°ê´€ì ",
                    "length": {"target": 550, "tolerance": 50}
                }
            },
            "kpi": {
                "quarterly": {
                    "elements": ["KPIë‹¬ì„±í˜„í™©", "ì£¼ìš”ì„±ê³¼ë¶„ì„", "íŒ€ê¸°ì—¬ë„í‰ê°€", "í˜„ì¬ë‹¬ì„±ìˆ˜ì¤€"],
                    "tone": "ê°ê´€ì ì´ê³  ë¶„ì„ì ",
                    "length": {"target": 200, "tolerance": 30}
                },
                "annual": {
                    "elements": ["ì—°ê°„KPIì¢…í•©", "ì„±ê³¼ì¶”ì´ë¶„ì„", "íŒ€ê¸°ì—¬ë„í‰ê°€", "ì¢…í•©ë‹¬ì„±í‰ê°€"],
                    "tone": "ì¢…í•©ì ì´ê³  ê°ê´€ì ",
                    "length": {"target": 250, "tolerance": 30}
                }
            }
        }
        
        return base_configs.get(self.comment_type, {}).get(self.period_type, {})
    
    def generate(self, data: Dict, context: Optional[Dict] = None) -> str:
        """í†µí•© ì½”ë©˜íŠ¸ ìƒì„± ë©”ì¸ í•¨ìˆ˜"""
        if not self.config:
            raise ValueError(f"Invalid comment type: {self.comment_type} or period type: {self.period_type}")
        
        context = context or {}
        
        # ì½”ë©˜íŠ¸ íƒ€ì…ë³„ ë°ì´í„° ì „ì²˜ë¦¬
        processed_data = self._preprocess_data(data)
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._create_prompt(processed_data, context)
        
        # LLM í˜¸ì¶œ ë° ê²€ì¦
        comment = self._call_llm_with_validation(prompt)
        
        return comment
    
    def _preprocess_data(self, data: Dict) -> Dict:
        """ì½”ë©˜íŠ¸ íƒ€ì…ë³„ ë°ì´í„° ì „ì²˜ë¦¬"""
        if self.comment_type == "task":
            return {
                "task_name": data.get('task_name', ''),
                "emp_name": data.get('emp_name', ''),
                "emp_no": data.get('emp_no', ''),
                "target_level": data.get('target_level', ''),
                "performance": data.get('cumulative_performance', ''),
                "achievement_rate": data.get('ai_achievement_rate', 0),
                "contribution_score": data.get('ai_contribution_score', 0)
            }
        
        elif self.comment_type == "individual":
            tasks = data.get('tasks', [])
            tasks_summary = ""
            total_achievement = 0
            total_contribution = 0
            
            for task in tasks:
                tasks_summary += f"- {task.get('task_name', '')}: ë‹¬ì„±ë¥  {task.get('ai_achievement_rate', 0)}%, ê¸°ì—¬ë„ {task.get('ai_contribution_score', 0)}ì \n"
                total_achievement += task.get('ai_achievement_rate', 0)
                total_contribution += task.get('ai_contribution_score', 0)
            
            avg_achievement = total_achievement / len(tasks) if tasks else 0
            avg_contribution = total_contribution / len(tasks) if tasks else 0
            
            return {
                "emp_name": data.get('emp_name', ''),
                "emp_no": data.get('emp_no', ''),
                "position": data.get('position', ''),
                "cl": data.get('cl', ''),
                "tasks_summary": tasks_summary,
                "avg_achievement": avg_achievement,
                "avg_contribution": avg_contribution,
                "task_count": len(tasks)
            }
        
        elif self.comment_type == "team":
            kpis = data.get('kpis', [])
            kpis_summary = ""
            total_rate = 0
            
            for kpi in kpis:
                rate = kpi.get('ai_kpi_progress_rate', 0)
                weight = kpi.get('weight', 0)
                kpis_summary += f"- {kpi.get('kpi_name', '')}: {rate}% (ë¹„ì¤‘ {weight}%)\n"
                total_rate += rate * (weight / 100)
            
            return {
                "kpis_summary": kpis_summary,
                "total_rate": total_rate,
                "team_context": data.get('team_context', ''),
                "performance_level": data.get('performance_level', '')
            }
        
        elif self.comment_type == "kpi":
            tasks = data.get('tasks', [])
            tasks_text = ""
            for task in tasks:
                tasks_text += f"- {task.get('emp_name', '')}: {task.get('task_name', '')}\n"
                tasks_text += f"  ëª©í‘œ: {task.get('target_level', '')}\n"
                tasks_text += f"  ì„±ê³¼: {task.get('task_performance', '')}\n"
            
            return {
                "kpi_name": data.get('kpi_name', ''),
                "kpi_description": data.get('kpi_description', ''),
                "tasks_text": tasks_text
            }
        
        return data
    
    def _create_prompt(self, data: Dict, context: Dict) -> str:
        """í†µí•© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        elements = self.config.get('elements', [])
        tone = self.config.get('tone', '')
        focus = self.config.get('focus', '')
        length = self.config.get('length', {})
        
        # íŒ€ ê°€ì´ë“œë¼ì¸ ì ìš©
        team_tone = self.team_guide.get('tone_guide', '')
        team_style = self.team_guide.get('style_guide', '')
        team_context = self.team_guide.get('team_context', '')
        
        system_content = f"""
        ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ {self.comment_type} ë¶„ì„ ì½”ë©˜íŠ¸ë¥¼ í•˜ë‚˜ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
        
        í¬í•¨í•  ë‚´ìš©: {', '.join(elements)}
        í†¤: {tone}
        ì´ˆì : {focus}
        ê¸¸ì´: {length.get('target', 250)}Â±{length.get('tolerance', 30)}ì
        
        íŒ€ ê°€ì´ë“œë¼ì¸:
        - {team_tone}
        - {team_style}
        - {team_context}
        
        ì‘ì„± ì›ì¹™:
        1. í˜„ì¬ ìƒíƒœì™€ ê³¼ê±° ì„±ì¥ ì¶”ì´ì— ì§‘ì¤‘
        2. êµ¬ì²´ì  ìˆ˜ì¹˜ì™€ ì„±ê³¼ë¥¼ í¬í•¨
        3. ë¯¸ë˜ ê³„íšì´ë‚˜ ì œì•ˆì€ í¬í•¨í•˜ì§€ ì•ŠìŒ
        4. ê°ê´€ì ì´ê³  íŒ©íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±
        5. ì§ì› ì´ë¦„ ì–¸ê¸‰ì‹œ "ì´ë¦„(ì‚¬ë²ˆ)ë‹˜" í˜•íƒœë¡œ ì‘ì„±
        6. "ì—°ê°„ìš”ì•½:", "íŒ€ê¸°ì—¬ë„:" ë“±ì˜ ì œëª© ì—†ì´ í•˜ë‚˜ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ìœ¼ë¡œ ì‘ì„±
        7. ë¬¸ë‹¨ ê°„ ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°ë¡œ ì „ì²´ì ì¸ íë¦„ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”
        """
        
        # ì½”ë©˜íŠ¸ íƒ€ì…ë³„ human content ìƒì„±
        human_content = self._create_human_content(data)
        
        prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content=system_content),
            HumanMessage(content=human_content)
        ])
        
        return str(prompt.format())
    
    def _create_human_content(self, data: Dict) -> str:
        """ì½”ë©˜íŠ¸ íƒ€ì…ë³„ human content ìƒì„±"""
        if self.comment_type == "task":
            return f"""
            Task: {data.get('task_name', '')}
            ë‹´ë‹¹ì: {data.get('emp_name', '')}({data.get('emp_no', '')})
            ëª©í‘œ: {data.get('target_level', '')}
            ëˆ„ì  ì„±ê³¼: {data.get('performance', '')}
            ë‹¬ì„±ë¥ : {data.get('achievement_rate', 0)}%
            ê¸°ì—¬ë„: {data.get('contribution_score', 0)}ì 
            """
        
        elif self.comment_type == "individual":
            return f"""
            ì§ì›: {data.get('emp_name', '')}({data.get('emp_no', '')})
            ì§ìœ„: {data.get('position', '')} (CL{data.get('cl', '')})
            
            Task ìˆ˜í–‰ í˜„í™©:
            {data.get('tasks_summary', '')}
            
            ì¢…í•© ì„±ê³¼:
            - í‰ê·  ë‹¬ì„±ë¥ : {data.get('avg_achievement', 0):.1f}%
            - í‰ê·  ê¸°ì—¬ë„: {data.get('avg_contribution', 0):.1f}ì 
            - ì°¸ì—¬ Task ìˆ˜: {data.get('task_count', 0)}ê°œ
            """
        
        elif self.comment_type == "team":
            return f"""
            íŒ€ KPI ì„±ê³¼ í˜„í™©:
            {data.get('kpis_summary', '')}
            
            íŒ€ ì „ì²´ í‰ê·  ë‹¬ì„±ë¥ : {data.get('total_rate', 0):.1f}%
            íŒ€ êµ¬ì„±: {data.get('team_context', '')}
            ì„±ê³¼ ìˆ˜ì¤€: {data.get('performance_level', '')}
            """
        
        elif self.comment_type == "kpi":
            return f"""
            KPI: {data.get('kpi_name', '')}
            KPI ëª©í‘œ: {data.get('kpi_description', '')}
            
            íŒ€ì›ë³„ ê°œë³„ ì„±ê³¼:
            {data.get('tasks_text', '')}
            """
        
        return str(data)
    
    def _call_llm_with_validation(self, prompt: str) -> str:
        """LLM í˜¸ì¶œ ë° ê²€ì¦"""
        def validate_comment(response: str) -> str:
            response = response.strip()
            
            # ê¸¸ì´ ê²€ì¦ (ê²½ê³  ë¡œê·¸ ì œê±°)
            target_length = self.config.get('length', {}).get('target', 250)
            tolerance = self.config.get('length', {}).get('tolerance', 30)
            
            # ê¸¸ì´ ê²€ì¦ì€ í•˜ë˜ ê²½ê³  ë¡œê·¸ëŠ” ì¶œë ¥í•˜ì§€ ì•ŠìŒ
            # if not (target_length - tolerance <= len(response) <= target_length + tolerance):
            #     logger.warning(f"Comment length {len(response)} outside target {target_length}Â±{tolerance}")
            
            return response
        
        return robust_llm_call(prompt, validate_comment, context=f"{self.comment_type} comment")

# ===== ì„œë¸Œëª¨ë“ˆ 5: ì½”ë©˜íŠ¸ ìƒì„± (ê°œì„ ëœ ë²„ì „) =====
def comment_generation_submodule(state: Module2State) -> Module2State:
    """ì½”ë©˜íŠ¸ ìƒì„± ì„œë¸Œëª¨ë“ˆ - í†µí•© ì‹œìŠ¤í…œ ì‚¬ìš©"""
    print(f"   ğŸ“ ì½”ë©˜íŠ¸ ìƒì„± ì¤‘...")
    
    # íŒ€ ì¼ê´€ì„± ê°€ì´ë“œ ìƒì„±
    team_context_guide = generate_team_consistency_guide(state.team_id, state.period_id)
    state.team_context_guide = team_context_guide
    
    # í†µí•© ì½”ë©˜íŠ¸ ìƒì„±ê¸° ì‚¬ìš©
    generate_task_comments_unified(state)
    generate_individual_summary_comments_unified(state)
    generate_team_overall_comment_unified(state)
    
    print(f"   âœ… ì½”ë©˜íŠ¸ ìƒì„± ì™„ë£Œ")
    return state

def generate_task_comments_unified(state: Module2State):
    """Taskë³„ ì½”ë©˜íŠ¸ ìƒì„± (í†µí•© ì‹œìŠ¤í…œ)"""
    period_type = "annual" if state.report_type == "annual" else "quarterly"
    
    for task_id in state.target_task_ids:
        task_data = fetch_cumulative_task_data(task_id, state.period_id)
        if not task_data:
            continue
        
        # í†µí•© ì½”ë©˜íŠ¸ ìƒì„±ê¸° ì‚¬ìš©
        generator = CommentGenerator("task", period_type, state.team_context_guide)
        comment = generator.generate(task_data)
        
        if task_data.get('task_summary_id'):
            update_task_summary(task_data['task_summary_id'], {
                "ai_analysis_comment_task": comment
            })

def generate_individual_summary_comments_unified(state: Module2State):
    """ê°œì¸ ì¢…í•© ì½”ë©˜íŠ¸ ìƒì„± (í†µí•© ì‹œìŠ¤í…œ)"""
    team_members = fetch_team_members(state.team_id)
    period_type = "annual" if state.report_type == "annual" else "quarterly"
    
    for member in team_members:
        if member.get('role') == 'MANAGER':
            continue
        
        # ê°œì¸ Task ë°ì´í„° ìˆ˜ì§‘
        individual_tasks = []
        for task_id in state.target_task_ids:
            task_data = fetch_cumulative_task_data(task_id, state.period_id)
            if task_data and task_data.get('emp_no') == member['emp_no']:
                individual_tasks.append(task_data)
        
        if not individual_tasks:
            continue
        
        # í†µí•© ì½”ë©˜íŠ¸ ìƒì„±ê¸° ì‚¬ìš©
        generator = CommentGenerator("individual", period_type, state.team_context_guide)
        comment = generator.generate({
            **member,
            "tasks": individual_tasks
        })
        
        # ë¶„ê¸°ë³„/ì—°ë§ë³„ ì €ì¥
        if state.report_type == "quarterly":
            feedback_report_id = save_feedback_report(
                member['emp_no'], 
                state.team_evaluation_id or 0,
                {"ai_overall_contribution_summary_comment": comment}
            )
            if state.feedback_report_ids is None:
                state.feedback_report_ids = []
            state.feedback_report_ids.append(feedback_report_id)
        else:  # annual
            final_report_id = save_final_evaluation_report(
                member['emp_no'],
                state.team_evaluation_id or 0, 
                {"ai_annual_performance_summary_comment": comment}
            )
            if state.final_evaluation_report_ids is None:
                state.final_evaluation_report_ids = []
            state.final_evaluation_report_ids.append(final_report_id)

def generate_team_overall_comment_unified(state: Module2State):
    """íŒ€ ì „ì²´ ë¶„ì„ ì½”ë©˜íŠ¸ ìƒì„± (í†µí•© ì‹œìŠ¤í…œ)"""
    # íŒ€ KPI ë°ì´í„° ìˆ˜ì§‘
    team_kpis_data = []
    for kpi_id in state.target_team_kpi_ids:
        kpi_data = fetch_team_kpi_data(kpi_id)
        if kpi_data:
            team_kpis_data.append(kpi_data)
    
    period_type = "annual" if state.report_type == "annual" else "quarterly"
    
    # í†µí•© ì½”ë©˜íŠ¸ ìƒì„±ê¸° ì‚¬ìš©
    generator = CommentGenerator("team", period_type, state.team_context_guide)
    comment = generator.generate({
        "kpis": team_kpis_data,
        "team_context": state.team_context_guide.get('team_context', '') if state.team_context_guide else '',
        "performance_level": state.team_context_guide.get('performance_level', '') if state.team_context_guide else ''
    })
    
    # team_evaluations ì—…ë°ì´íŠ¸
    update_team_evaluations(state.team_evaluation_id or 0, {
        "ai_team_overall_analysis_comment": comment
    })

# ===== ì„œë¸Œëª¨ë“ˆ 6: DB ì—…ë°ì´íŠ¸ =====
def db_update_submodule(state: Module2State) -> Module2State:
    """ìµœì¢… DB ì—…ë°ì´íŠ¸ ì„œë¸Œëª¨ë“ˆ - íŠ¸ëœì­ì…˜ ì²˜ë¦¬"""
    print(f"   ğŸ’¾ ìµœì¢… DB ì—…ë°ì´íŠ¸ ì¤‘...")
    
    try:
        with engine.begin() as transaction:
            # ì´ë¯¸ ê° ì„œë¸Œëª¨ë“ˆì—ì„œ ì—…ë°ì´íŠ¸í–ˆìœ¼ë¯€ë¡œ ìµœì¢… ê²€ì¦ë§Œ ìˆ˜í–‰
            
            # 1. ë¶„ê¸°ë³„ ì¶”ê°€ ì—…ë°ì´íŠ¸ (ranking, cumulative ë°ì´í„°)
            if state.report_type == "quarterly":
                update_quarterly_specific_data(state)
            
            # 2. ì—°ë§ ì¶”ê°€ ì—…ë°ì´íŠ¸ (final_evaluation_reports ì¶”ê°€ í•„ë“œ)
            elif state.report_type == "annual":
                update_annual_specific_data(state)
            
            # 3. ì—…ë°ì´íŠ¸ ê²°ê³¼ ê²€ì¦
            validation_result = validate_final_update_results(state)
            
            if not validation_result['success']:
                raise DataIntegrityError(f"Final validation failed: {validation_result['errors']}")
            
            # 4. ì—…ë°ì´íŠ¸ í†µê³„ ë¡œê¹…
            updated_tasks = len(state.updated_task_ids or [])
            updated_kpis = len(state.updated_team_kpi_ids or [])
            updated_feedback_reports = len(state.feedback_report_ids or [])
            updated_final_reports = len(state.final_evaluation_report_ids or [])
            
            print(f"      â€¢ Task ì—…ë°ì´íŠ¸: {updated_tasks}ê°œ")
            print(f"      â€¢ KPI ì—…ë°ì´íŠ¸: {updated_kpis}ê°œ")
            print(f"      â€¢ í”¼ë“œë°± ë¦¬í¬íŠ¸: {updated_feedback_reports}ê°œ")
            print(f"      â€¢ ìµœì¢… ë¦¬í¬íŠ¸: {updated_final_reports}ê°œ")
            
            # 5. ìµœì¢… ìƒíƒœ ë¡œê¹…
            if state.report_type == "quarterly":
                print(f"      â€¢ ë¶„ê¸° í‰ê°€ ì™„ë£Œ")
            else:
                print(f"      â€¢ ì—°ë§ í‰ê°€ ì™„ë£Œ")
                
            return state
                
    except Exception as e:
        print(f"   âŒ ìµœì¢… DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        raise

def update_quarterly_specific_data(state: Module2State):
    """ë¶„ê¸°ë³„ ì „ìš© ë°ì´í„° ì—…ë°ì´íŠ¸ - ê°œì¸ ë‹¬ì„±ë¥  ê¸°ë°˜ ìˆœìœ„ ë§¤ê¸°ê¸°"""
    print(f"      ğŸ“Š ë¶„ê¸°ë³„ ì „ìš© ë°ì´í„° ì—…ë°ì´íŠ¸ ì¤‘...")
    
    # 1. íŒ€ ë‚´ ê°œì¸ë³„ ë‹¬ì„±ë¥  ê¸°ë°˜ ìˆœìœ„ ê³„ì‚° ë° ì—…ë°ì´íŠ¸
    team_ranking_result = calculate_team_ranking(state)
    
    # 2. ìˆœìœ„ ê²°ê³¼ë¥¼ feedback_reportsì— ì €ì¥
    update_team_ranking_to_feedback_reports(state, team_ranking_result)
    
    print(f"      âœ… ë¶„ê¸°ë³„ ìˆœìœ„ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(team_ranking_result)}ëª…")
    print(f"      ğŸ“Š íŒ€ ë‚´ ë‹¬ì„±ë¥  ìˆœìœ„:")
    for i, member in enumerate(team_ranking_result):
        print(f"        {i+1}ìœ„: {member['emp_name']}({member['emp_no']}) - {member['avg_achievement_rate']:.1f}%")

def calculate_team_ranking(state: Module2State) -> List[Dict]:
    """íŒ€ ë‚´ ê°œì¸ë³„ ë‹¬ì„±ë¥  ê¸°ë°˜ ìˆœìœ„ ê³„ì‚°"""
    print(f"        ğŸ† íŒ€ ë‚´ ìˆœìœ„ ê³„ì‚° ì¤‘...")
    
    team_members = fetch_team_members(state.team_id)
    member_achievements = []
    
    for member in team_members:
        if member.get('role') == 'MANAGER':
            continue
            
        # ê°œì¸ë³„ Task ìˆ˜ì§‘
        individual_tasks = []
        for task_id in state.target_task_ids:
            task_data = fetch_cumulative_task_data(task_id, state.period_id)
            if task_data and task_data.get('emp_no') == member['emp_no']:
                individual_tasks.append(task_data)
        
        if individual_tasks:
            # ê°€ì¤‘í‰ê·  ë‹¬ì„±ë¥  ê³„ì‚°
            result = calculate_individual_weighted_achievement_rate(individual_tasks)
            
            # ê³„ì‚° ê³¼ì • ìƒì„¸ ë¡œê¹…
            print(f"          ğŸ“ˆ {member['emp_name']}({member['emp_no']}) ë‹¬ì„±ë¥  ê³„ì‚°:")
            total_weighted_score = 0
            for task in individual_tasks:
                task_name = task.get('task_name', f'Task{task.get("task_id")}')
                task_weight = task.get('weight', 0)
                task_achievement = task.get('ai_achievement_rate', 0)
                weighted_score = task_achievement * task_weight
                total_weighted_score += weighted_score
                print(f"            â€¢ {task_name}: {task_achievement}% Ã— {task_weight} = {weighted_score}")
            
            print(f"            = {result['achievement_rate']:.1f}% (ì´ ê°€ì¤‘ì ìˆ˜: {total_weighted_score}, ì´ ê°€ì¤‘ì¹˜: {result['total_weight']})")
            
            member_achievements.append({
                'emp_no': member['emp_no'],
                'emp_name': member['emp_name'],
                'position': member.get('position', ''),
                'cl': member.get('cl', ''),
                'avg_achievement_rate': result['achievement_rate'],
                'avg_contribution_rate': result['contribution_rate'],
                'task_count': len(individual_tasks),
                'total_weight': result['total_weight'],
                'total_weighted_score': total_weighted_score
            })
        else:
            print(f"          âš ï¸  {member['emp_name']}({member['emp_no']}): ì°¸ì—¬ Task ì—†ìŒ")
    
    # ë‹¬ì„±ë¥  ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ë†’ì€ ë‹¬ì„±ë¥ ì´ 1ìœ„)
    member_achievements.sort(key=lambda x: x['avg_achievement_rate'], reverse=True)
    
    # ë™ì ì ì²˜ë¦¬ (ê°™ì€ ë‹¬ì„±ë¥ ì¸ ê²½ìš° ê°€ì¤‘ì ìˆ˜ë¡œ ì¬ì •ë ¬)
    for i in range(len(member_achievements) - 1):
        if member_achievements[i]['avg_achievement_rate'] == member_achievements[i + 1]['avg_achievement_rate']:
            # ë™ì ìì¸ ê²½ìš° ê°€ì¤‘ì ìˆ˜ë¡œ ì¬ì •ë ¬
            if member_achievements[i]['total_weighted_score'] < member_achievements[i + 1]['total_weighted_score']:
                member_achievements[i], member_achievements[i + 1] = member_achievements[i + 1], member_achievements[i]
    
    return member_achievements

def update_team_ranking_to_feedback_reports(state: Module2State, team_ranking: List[Dict]):
    """íŒ€ ìˆœìœ„ ê²°ê³¼ë¥¼ feedback_reportsì— ì €ì¥"""
    print(f"        ğŸ’¾ ìˆœìœ„ ê²°ê³¼ë¥¼ feedback_reportsì— ì €ì¥ ì¤‘...")
    
    updated_count = 0
    
    for i, member_data in enumerate(team_ranking):
        ranking = i + 1
        
        # feedback_reports ì—…ë°ì´íŠ¸ ë°ì´í„°
        feedback_data = {
            'ranking': ranking,  # íŒ€ ë‚´ ìˆœìœ„ (1, 2, 3, ...)
            'ai_achievement_rate': int(member_data['avg_achievement_rate']),  # ê°€ì¤‘í‰ê·  ë‹¬ì„±ë¥ 
            'contribution_rate': int(member_data['avg_contribution_rate'])    # í‰ê·  ê¸°ì—¬ë„
        }
        
        # ê¸°ì¡´ feedback_report ì—…ë°ì´íŠ¸ ë˜ëŠ” ìƒˆë¡œ ìƒì„±
        feedback_report_id = save_feedback_report(
            member_data['emp_no'],
            state.team_evaluation_id or 0,
            feedback_data
        )
        
        updated_count += 1
        
        # ìˆœìœ„ ì €ì¥ ê²°ê³¼ ë¡œê¹…
        print(f"          {ranking}ìœ„: {member_data['emp_name']}({member_data['emp_no']}) - {member_data['avg_achievement_rate']:.1f}% â†’ feedback_report_id: {feedback_report_id}")
    
    print(f"        âœ… {updated_count}ëª…ì˜ ìˆœìœ„ ì •ë³´ ì €ì¥ ì™„ë£Œ")

def validate_team_ranking_data(state: Module2State) -> Dict[str, Any]:
    """íŒ€ ìˆœìœ„ ë°ì´í„° ê²€ì¦"""
    print(f"        ğŸ” íŒ€ ìˆœìœ„ ë°ì´í„° ê²€ì¦ ì¤‘...")
    
    errors = []
    warnings = []
    
    try:
        with engine.connect() as connection:
            # feedback_reportsì—ì„œ ìˆœìœ„ ë°ì´í„° ì¡°íšŒ
            query = text("""
                SELECT emp_no, ranking, ai_achievement_rate, contribution_rate
                FROM feedback_reports 
                WHERE team_evaluation_id = :team_evaluation_id
                ORDER BY ranking
            """)
            results = connection.execute(query, {"team_evaluation_id": state.team_evaluation_id})
            ranking_data = [row_to_dict(row) for row in results]
            
            if not ranking_data:
                errors.append("íŒ€ ìˆœìœ„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return {'success': False, 'errors': errors, 'warnings': warnings}
            
            # 1. ìˆœìœ„ ì—°ì†ì„± ê²€ì¦
            expected_rankings = list(range(1, len(ranking_data) + 1))
            actual_rankings = [r['ranking'] for r in ranking_data]
            
            if actual_rankings != expected_rankings:
                errors.append(f"ìˆœìœ„ê°€ ì—°ì†ì ì´ì§€ ì•ŠìŒ: ì˜ˆìƒ {expected_rankings}, ì‹¤ì œ {actual_rankings}")
            
            # 2. ë‹¬ì„±ë¥  ë²”ìœ„ ê²€ì¦
            for rank_data in ranking_data:
                achievement_rate = rank_data.get('ai_achievement_rate', 0)
                if not (0 <= achievement_rate <= 200):
                    errors.append(f"ì‚¬ë²ˆ {rank_data['emp_no']}: ë‹¬ì„±ë¥  {achievement_rate}%ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨")
                
                contribution_rate = rank_data.get('contribution_rate', 0)
                if not (0 <= contribution_rate <= 100):
                    warnings.append(f"ì‚¬ë²ˆ {rank_data['emp_no']}: ê¸°ì—¬ë„ {contribution_rate}%ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨")
            
            # 3. ìˆœìœ„ì™€ ë‹¬ì„±ë¥  ì¼ê´€ì„± ê²€ì¦
            for i in range(len(ranking_data) - 1):
                current_rate = ranking_data[i]['ai_achievement_rate']
                next_rate = ranking_data[i + 1]['ai_achievement_rate']
                
                if current_rate < next_rate:
                    errors.append(f"ìˆœìœ„ {i+1}ìœ„({ranking_data[i]['emp_no']})ì˜ ë‹¬ì„±ë¥  {current_rate}%ê°€ {i+2}ìœ„({ranking_data[i+1]['emp_no']})ì˜ ë‹¬ì„±ë¥  {next_rate}%ë³´ë‹¤ ë‚®ìŒ")
            
            # 4. íŒ€ì› ìˆ˜ì™€ ìˆœìœ„ ìˆ˜ ì¼ì¹˜ ê²€ì¦
            team_members = fetch_team_members(state.team_id)
            non_manager_count = len([m for m in team_members if m.get('role') != 'MANAGER'])
            
            if len(ranking_data) != non_manager_count:
                warnings.append(f"íŒ€ì› ìˆ˜({non_manager_count}ëª…)ì™€ ìˆœìœ„ ìˆ˜({len(ranking_data)}ëª…)ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŒ")
            
            success = len(errors) == 0
            
            if warnings:
                print(f"          âš ï¸  ê²€ì¦ ê²½ê³ : {len(warnings)}ê±´")
            
            return {
                'success': success,
                'errors': errors,
                'warnings': warnings,
                'ranking_count': len(ranking_data),
                'team_member_count': non_manager_count
            }
            
    except Exception as e:
        print(f"          âŒ ìˆœìœ„ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
        return {
            'success': False,
            'errors': [f"ìˆœìœ„ ë°ì´í„° ê²€ì¦ ì˜¤ë¥˜: {str(e)}"],
            'warnings': [],
            'ranking_count': 0,
            'team_member_count': 0
        }

def update_annual_specific_data(state: Module2State):
    """ì—°ë§ ì „ìš© ë°ì´í„° ì—…ë°ì´íŠ¸ - Task Weight ê¸°ë°˜ ê°€ì¤‘í‰ê· """
    print(f"      ğŸ“Š ì—°ë§ ì „ìš© ë°ì´í„° ì—…ë°ì´íŠ¸ ì¤‘...")
    
    team_members = fetch_team_members(state.team_id)
    
    for member in team_members:
        if member.get('role') == 'MANAGER':
            continue
            
        # ê°œì¸ë³„ Task ìˆ˜ì§‘
        individual_tasks = []
        for task_id in state.target_task_ids:
            task_data = fetch_cumulative_task_data(task_id, state.period_id)
            if task_data and task_data.get('emp_no') == member['emp_no']:
                individual_tasks.append(task_data)
        
        if individual_tasks:
            # ê°€ì¤‘í‰ê·  ê³„ì‚°
            result = calculate_individual_weighted_achievement_rate(individual_tasks)
            
            # ê³„ì‚° ê³¼ì • ë¡œê¹…
            print(f"        ğŸ“ˆ {member['emp_name']}({member['emp_no']}) ì—°ê°„ ê°€ì¤‘í‰ê·  ê³„ì‚°:")
            for task in individual_tasks:
                task_name = task.get('task_name', f'Task{task.get("task_id")}')
                task_weight = task.get('weight', 0)
                task_achievement = task.get('ai_achievement_rate', 0)
                print(f"          â€¢ {task_name}: {task_achievement}% Ã— {task_weight} = {task_achievement * task_weight}")
            print(f"          = {result['achievement_rate']:.1f}% (ì´ ê°€ì¤‘ì¹˜: {result['total_weight']})")
            
            # final_evaluation_reports ì—…ë°ì´íŠ¸
            final_data = {
                'ai_annual_achievement_rate': int(result['achievement_rate'])
            }
            
            save_final_evaluation_report(
                member['emp_no'],
                state.team_evaluation_id or 0,
                final_data
            )
    
    print(f"      âœ… ì—°ë§ ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len([m for m in team_members if m.get('role') != 'MANAGER'])}ëª…")

def validate_final_update_results(state: Module2State) -> Dict[str, Any]:
    """ìµœì¢… ì—…ë°ì´íŠ¸ ê²°ê³¼ ê²€ì¦"""
    errors = []
    warnings = []
    
    try:
        # 1. Task ì—…ë°ì´íŠ¸ ê²€ì¦
        for task_id in (state.updated_task_ids or []):
            task_data = fetch_cumulative_task_data(task_id, state.period_id)
            
            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            if task_data.get('ai_achievement_rate') is None:
                errors.append(f"Task {task_id}: ai_achievement_rate not updated")
            
            if task_data.get('ai_contribution_score') is None:
                errors.append(f"Task {task_id}: ai_contribution_score not updated")
            
            if not task_data.get('ai_analysis_comment_task'):
                warnings.append(f"Task {task_id}: ai_analysis_comment_task empty")
            
            # ì—°ë§ ì „ìš© ê²€ì¦
            if state.report_type == "annual" and not task_data.get('ai_assessed_grade'):
                warnings.append(f"Task {task_id}: ai_assessed_grade not set for annual evaluation")
        
        # 2. Team KPI ì—…ë°ì´íŠ¸ ê²€ì¦
        for kpi_id in (state.updated_team_kpi_ids or []):
            kpi_data = fetch_team_kpi_data(kpi_id)
            
            if kpi_data.get('ai_kpi_progress_rate') is None:
                errors.append(f"KPI {kpi_id}: ai_kpi_progress_rate not updated")
            
            if not kpi_data.get('ai_kpi_analysis_comment'):
                warnings.append(f"KPI {kpi_id}: ai_kpi_analysis_comment empty")
        
        # 3. Team evaluation ê²€ì¦
        if state.team_evaluation_id:
            with engine.connect() as connection:
                query = text("""
                    SELECT average_achievement_rate, ai_team_overall_analysis_comment,
                           year_over_year_growth
                    FROM team_evaluations 
                    WHERE team_evaluation_id = :team_evaluation_id
                """)
                result = connection.execute(query, {"team_evaluation_id": state.team_evaluation_id})
                row = result.fetchone()
                team_eval = row_to_dict(row) if row else {}
                
                if team_eval.get('average_achievement_rate') is None:
                    errors.append("Team evaluation: average_achievement_rate not updated")
                
                if not team_eval.get('ai_team_overall_analysis_comment'):
                    warnings.append("Team evaluation: ai_team_overall_analysis_comment empty")
        
        # 4. ë¶„ê¸°ë³„ íŒ€ ìˆœìœ„ ê²€ì¦ (ìƒˆë¡œ ì¶”ê°€)
        if state.report_type == "quarterly":
            ranking_validation = validate_team_ranking_data(state)
            if not ranking_validation['success']:
                errors.extend(ranking_validation['errors'])
            warnings.extend(ranking_validation['warnings'])
            
            print(f"      ğŸ“Š íŒ€ ìˆœìœ„ ê²€ì¦ ê²°ê³¼:")
            print(f"        â€¢ ìˆœìœ„ ë°ì´í„°: {ranking_validation['ranking_count']}ëª…")
            print(f"        â€¢ íŒ€ì› ìˆ˜: {ranking_validation['team_member_count']}ëª…")
        
        # 5. ë ˆí¬íŠ¸ ê²€ì¦
        if state.report_type == "quarterly" and state.feedback_report_ids:
            for report_id in state.feedback_report_ids:
                # feedback_reports ê²€ì¦ ë¡œì§
                pass
        
        elif state.report_type == "annual" and state.final_evaluation_report_ids:
            for report_id in state.final_evaluation_report_ids:
                # final_evaluation_reports ê²€ì¦ ë¡œì§
                pass
        
        # 6. ë°ì´í„° ì¼ê´€ì„± ê²€ì¦
        consistency_errors = validate_data_consistency(state)
        errors.extend(consistency_errors)
        
        success = len(errors) == 0
        
        if warnings:
            print(f"      âš ï¸  ê²€ì¦ ê²½ê³ : {len(warnings)}ê±´")
        
        return {
            'success': success,
            'errors': errors,
            'warnings': warnings,
            'stats': {
                'tasks_validated': len(state.updated_task_ids or []),
                'kpis_validated': len(state.updated_team_kpi_ids or []),
                'reports_validated': len(state.feedback_report_ids or []) + len(state.final_evaluation_report_ids or []),
                'ranking_validated': state.report_type == "quarterly"
            }
        }
        
    except Exception as e:
        print(f"      âŒ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
        return {
            'success': False,
            'errors': [f"Validation process error: {str(e)}"],
            'warnings': [],
            'stats': {}
        }

def validate_data_consistency(state: Module2State) -> List[str]:
    """ë°ì´í„° ì¼ê´€ì„± ê²€ì¦"""
    errors = []
    
    try:
        # 1. ê¸°ì—¬ë„ í•©ê³„ ê²€ì¦ (KPIë³„)
        for kpi_id in state.target_team_kpi_ids:
            kpi_tasks = fetch_kpi_tasks(kpi_id, state.period_id)
            total_contribution = 0
            
            for task in kpi_tasks:
                task_data = fetch_cumulative_task_data(task['task_id'], state.period_id)
                contribution = task_data.get('ai_contribution_score', 0)
                total_contribution += contribution
            
            # KPIë³„ ê¸°ì—¬ë„ í•©ê³„ê°€ 100ì— ê°€ê¹Œìš´ì§€ í™•ì¸ (ì •ëŸ‰í‰ê°€ì¸ ê²½ìš°)
            evaluation_type = check_evaluation_type(kpi_id)
            if evaluation_type == "quantitative" and abs(total_contribution - 100) > 10:
                errors.append(f"KPI {kpi_id}: contribution sum {total_contribution} far from 100")
        
        # 2. ë‹¬ì„±ë¥  ë²”ìœ„ ê²€ì¦
        for task_id in state.updated_task_ids or []:
            task_data = fetch_cumulative_task_data(task_id, state.period_id)
            achievement_rate = task_data.get('ai_achievement_rate', 0)
            
            if not (0 <= achievement_rate <= 200):
                errors.append(f"Task {task_id}: achievement_rate {achievement_rate} out of range")
        
        # 3. íŒ€ í‰ê·  ë‹¬ì„±ë¥ ê³¼ ê°œë³„ ë‹¬ì„±ë¥  ì¼ê´€ì„± ê²€ì¦
        if state.team_evaluation_id:
            with engine.connect() as connection:
                query = text("""
                    SELECT average_achievement_rate 
                    FROM team_evaluations 
                    WHERE team_evaluation_id = :team_evaluation_id
                """)
                result = connection.execute(query, {"team_evaluation_id": state.team_evaluation_id})
                team_avg = result.scalar_one_or_none()
                
                # ê°œë³„ Taskë“¤ì˜ ê°€ì¤‘í‰ê· ê³¼ íŒ€ í‰ê· ì´ í¬ê²Œ ë‹¤ë¥´ì§€ ì•Šì€ì§€ í™•ì¸
                calculated_avg = calculate_team_average_achievement_rate(state.target_team_kpi_ids)
                
                if team_avg and abs(team_avg - calculated_avg) > 15:
                    errors.append(f"Team average inconsistency: stored {team_avg} vs calculated {calculated_avg}")
        
    except Exception as e:
        errors.append(f"Consistency validation error: {str(e)}")
    
    return errors

# ===== LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„± =====

# StateGraphì— ì‚¬ìš©í•  íƒ€ì…: dataclass ê·¸ëŒ€ë¡œ ì‚¬ìš© (dictë„ ê°€ëŠ¥)
module2_workflow = StateGraph(Module2State)

# ê° ì„œë¸Œëª¨ë“ˆì„ ë…¸ë“œë¡œ ë“±ë¡
module2_workflow.add_node("data_collection", data_collection_submodule)
module2_workflow.add_node("achievement_and_grade", achievement_and_grade_calculation_submodule)
module2_workflow.add_node("contribution", contribution_calculation_submodule)
module2_workflow.add_node("team_analysis", team_analysis_submodule)
module2_workflow.add_node("comment_generation", comment_generation_submodule)
module2_workflow.add_node("db_update", db_update_submodule)

# ì—£ì§€(ì‹¤í–‰ ìˆœì„œ) ì •ì˜
module2_workflow.add_edge(START, "data_collection")
module2_workflow.add_edge("data_collection", "achievement_and_grade")
module2_workflow.add_edge("achievement_and_grade", "contribution")
module2_workflow.add_edge("contribution", "team_analysis")
module2_workflow.add_edge("team_analysis", "comment_generation")
module2_workflow.add_edge("comment_generation", "db_update")
module2_workflow.add_edge("db_update", END)

# ê·¸ë˜í”„ ì»´íŒŒì¼
module2_graph = module2_workflow.compile()

# ===== ì‹¤í–‰ í•¨ìˆ˜ ë° main ì§„ì…ì  ì¶”ê°€ =====
def run_module2_for_team_period(team_id: int, period_id: int, report_type: str, target_task_ids: list, target_team_kpi_ids: list):
    """
    ëª¨ë“ˆ2 ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ í•¨ìˆ˜
    Args:
        team_id: íŒ€ ID
        period_id: í‰ê°€ ê¸°ê°„ ID (ë¶„ê¸°)
        report_type: 'quarterly' ë˜ëŠ” 'annual'
        target_task_ids: í‰ê°€ ëŒ€ìƒ Task ID ë¦¬ìŠ¤íŠ¸
        target_team_kpi_ids: í‰ê°€ ëŒ€ìƒ KPI ID ë¦¬ìŠ¤íŠ¸
    """
    print(f"\n============================")
    print(f"[ëª¨ë“ˆ2] íŒ€ {team_id}, ê¸°ê°„ {period_id} ({'ì—°ë§' if report_type == 'annual' else 'ë¶„ê¸°'}) í‰ê°€ ì‹¤í–‰")
    print(f"============================\n")
    
    # State ìƒì„±
    state = Module2State(
        report_type=report_type if report_type == "annual" else "quarterly",  # Literal íƒ€ì… ë³´ì¥
        team_id=team_id,
        period_id=period_id,
        target_task_ids=target_task_ids,
        target_team_kpi_ids=target_team_kpi_ids
    )
    
    # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    result = module2_graph.invoke(state)
    print(f"\n[ì™„ë£Œ] íŒ€ {team_id}, ê¸°ê°„ {period_id} í‰ê°€ ì¢…ë£Œ\n")
    return result

if __name__ == "__main__":
    # ë¶„ê¸°ë³„ period_idì™€ report_type ë§¤í•‘ (ì˜ˆì‹œ)
    period_map = {
        1: {"period_id": 1, "report_type": "quarterly"},
        2: {"period_id": 2, "report_type": "quarterly"},
        3: {"period_id": 3, "report_type": "quarterly"},
        4: {"period_id": 4, "report_type": "annual"},
    }
    
    # ì‹¤ì œ DBì—ì„œ team_id=1ì˜ task/kpi idë¥¼ ì¡°íšŒí•˜ëŠ” ì½”ë“œ í•„ìš”
    # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ ì„ì˜ì˜ ID ì‚¬ìš© (ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
    # ì•„ë˜ ì¿¼ë¦¬ë¡œ ìë™ ì¡°íšŒ ê°€ëŠ¥
    def fetch_team_tasks_and_kpis(team_id: int, period_id: int):
        with engine.connect() as connection:
            # í•´ë‹¹ íŒ€ì˜ í•´ë‹¹ ê¸°ê°„ Task ID
            task_query = text("""
                SELECT t.task_id FROM tasks t
                JOIN employees e ON t.emp_no = e.emp_no
                WHERE e.team_id = :team_id
            """)
            task_ids = [row[0] for row in connection.execute(task_query, {"team_id": team_id})]
            
            # í•´ë‹¹ íŒ€ì˜ KPI ID
            kpi_query = text("""
                SELECT team_kpi_id FROM team_kpis WHERE team_id = :team_id
            """)
            kpi_ids = [row[0] for row in connection.execute(kpi_query, {"team_id": team_id})]
            return task_ids, kpi_ids
    
    import argparse
    parser = argparse.ArgumentParser(description="Module2 Goal Achievement Runner")
    parser.add_argument("--quarter", type=int, choices=[1,2,3,4], required=False, default=4, help="ì‹¤í–‰í•  ë¶„ê¸° (1,2,3,4). ê¸°ë³¸ê°’: 1")
    args = parser.parse_args()
    
    team_id = 1
    period_info = period_map[args.quarter]
    period_id = period_info["period_id"]
    report_type = period_info["report_type"]
    
    # Task/KPI ID ìë™ ì¡°íšŒ
    task_ids, kpi_ids = fetch_team_tasks_and_kpis(team_id, period_id)
    
    run_module2_for_team_period(
        team_id=team_id,
        period_id=period_id,
        report_type=report_type,
        target_task_ids=task_ids,
        target_team_kpi_ids=kpi_ids
    )

