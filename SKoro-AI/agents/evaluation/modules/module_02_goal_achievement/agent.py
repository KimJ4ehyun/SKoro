# ================================================================
# agent_module2.py - ëª¨ë“ˆ 2 LangGraph ì—ì´ì „íŠ¸ ë° ìƒíƒœ ê´€ë¦¬
# ================================================================

import logging
from typing import Dict, List, Optional, Any, Literal, TypedDict
from langgraph.graph import StateGraph, START, END

from agents.evaluation.modules.module_02_goal_achievement.db_utils import *
from agents.evaluation.modules.module_02_goal_achievement.calculation_utils import *
from agents.evaluation.modules.module_02_goal_achievement.llm_utils import *
from agents.evaluation.modules.module_02_goal_achievement.comment_generator import *

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ================================================================
# ìƒíƒœ ì •ì˜
# ================================================================

class Module2State(TypedDict):
    """ê²½ëŸ‰ State - ìš°ë¦¬ê°€ ìƒì˜í•œ DB ê¸°ë°˜ ì „ë‹¬ ë°©ì‹"""
    # ê¸°ë³¸ ì •ë³´
    report_type: Literal["quarterly", "annual"]
    team_id: int
    period_id: int
    
    # íƒ€ê²Ÿ IDë“¤
    target_task_ids: List[int]
    target_team_kpi_ids: List[int]
    
    # ì²˜ë¦¬ ê²°ê³¼ ì¶”ì ìš© (DB IDë§Œ ì €ì¥)
    updated_task_ids: Optional[List[int]]
    updated_team_kpi_ids: Optional[List[int]]
    feedback_report_ids: Optional[List[int]]
    team_evaluation_id: Optional[int]
    final_evaluation_report_ids: Optional[List[int]]
    
    # íŠ¹ë³„ ì „ë‹¬ ë°ì´í„° (ì„œë¸Œëª¨ë“ˆ ê°„ í•„ìš”ì‹œë§Œ)
    team_context_guide: Optional[Dict]

# ================================================================
# ì—ëŸ¬ ì²˜ë¦¬ í´ë˜ìŠ¤
# ================================================================

class DataIntegrityError(Exception):
    pass

# ================================================================
# ì„œë¸Œëª¨ë“ˆ í•¨ìˆ˜ë“¤
# ================================================================

def data_collection_submodule(state: Module2State) -> Module2State:
    """ë°ì´í„° ìˆ˜ì§‘ ì„œë¸Œëª¨ë“ˆ"""
    print(f"   ğŸ“‹ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    
    # team_evaluation_id í™•ì¸/ìƒì„±
    team_evaluation_id = fetch_team_evaluation_id(state['team_id'], state['period_id'])
    if not team_evaluation_id:
        raise DataIntegrityError(f"team_evaluation_id not found for team {state['team_id']}, period {state['period_id']}")
    
    state['team_evaluation_id'] = team_evaluation_id
    
    # evaluation_type í™•ì¸/ì„¤ì •
    for kpi_id in state['target_team_kpi_ids']:
        evaluation_type = check_evaluation_type(kpi_id)
        print(f"      â€¢ KPI {kpi_id}: {evaluation_type} í‰ê°€")
    
    print(f"   âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
    return state

def achievement_and_grade_calculation_submodule(state: Module2State) -> Module2State:
    """ë‹¬ì„±ë¥ +ë“±ê¸‰ ê³„ì‚° ì„œë¸Œëª¨ë“ˆ (í†µí•©) - ìš°ë¦¬ê°€ ìƒì˜í•œ ë°°ì¹˜ ì²˜ë¦¬"""
    print(f"   ğŸ¯ ë‹¬ì„±ë¥  ë° ë“±ê¸‰ ê³„ì‚° ì¤‘...")
    
    updated_task_ids = []
    batch_data = []
    
    # ë°°ì¹˜ìš© ë°ì´í„° ì¤€ë¹„
    for task_id in state['target_task_ids']:
        task_data = fetch_cumulative_task_data(task_id, state['period_id'])
        if not task_data:
            continue
            
        batch_data.append({
            "task_id": task_id,
            "task_summary_id": task_data.get('task_summary_id'),
            "target_level": task_data.get('target_level', ''),
            "cumulative_performance": task_data.get('cumulative_performance', ''),
            "cumulative_summary": task_data.get('cumulative_task_summary', ''),
            "kpi_data": fetch_team_kpi_data(task_data.get('team_kpi_id') or 0)
        })
    
    # ë°°ì¹˜ ì²˜ë¦¬ (15ê°œì”©)
    batch_size = 15
    for i in range(0, len(batch_data), batch_size):
        batch = batch_data[i:i+batch_size]
        results = batch_calculate_achievement_and_grades(batch, state['report_type'] == "annual")
        
        # ê²°ê³¼ ì €ì¥
        for task_data, result in zip(batch, results):
            task_summary_id = task_data['task_summary_id']
            if not task_summary_id:
                continue
                
            update_data = {
                "ai_achievement_rate": int(result['achievement_rate'])
            }
            
            # ì—°ë§ì¸ ê²½ìš° ë“±ê¸‰ë„ ì €ì¥
            if state['report_type'] == "annual" and result.get('grade'):
                update_data["ai_assessed_grade"] = result['grade']
            
            if update_task_summary(task_summary_id, update_data):
                updated_task_ids.append(task_data['task_id'])
    
    state['updated_task_ids'] = updated_task_ids
    print(f"   âœ… ë‹¬ì„±ë¥  ê³„ì‚° ì™„ë£Œ: {len(updated_task_ids)}ê°œ Task ì—…ë°ì´íŠ¸")
    return state

def contribution_calculation_submodule(state: Module2State) -> Module2State:
    """ê¸°ì—¬ë„ ê³„ì‚° ì„œë¸Œëª¨ë“ˆ - ìš°ë¦¬ê°€ ìƒì˜í•œ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹"""
    print(f"   âš–ï¸ ê¸°ì—¬ë„ ê³„ì‚° ì¤‘...")
    
    updated_task_ids = []
    kpi_contributions_by_emp = {}  # {emp_no: total_score} - í•˜ì´ë¸Œë¦¬ë“œ 3ë‹¨ê³„ ê²°ê³¼
    
    # KPIë³„ë¡œ ì²˜ë¦¬
    for kpi_id in state['target_team_kpi_ids']:
        evaluation_type = check_evaluation_type(kpi_id)
        kpi_data = fetch_team_kpi_data(kpi_id)
        
        if evaluation_type == "quantitative":
            # ì •ëŸ‰ í‰ê°€: ê°œì¸ì„±ê³¼/íŒ€ì „ì²´ì„±ê³¼ Ã— 100
            contributions = calculate_quantitative_contributions(kpi_id, state['period_id'])
        else:
            # ì •ì„± í‰ê°€: LLM ê¸°ë°˜ ìƒëŒ€ í‰ê°€
            contributions = calculate_qualitative_contributions(kpi_id, state['period_id'], kpi_data)
        
        # ğŸ”§ ì¶”ê°€: KPIë³„ ê¸°ì—¬ë„ í•©ê³„ ê²€ì¦
        total_contribution = sum(contributions.values())
        if abs(total_contribution - 100) > 10:
            raise DataIntegrityError(f"KPI {kpi_id}: ê¸°ì—¬ë„ í•©ê³„ {total_contribution:.1f}%ê°€ 100%ì—ì„œ ë²—ì–´ë‚¨")
        print(f"      âœ… KPI {kpi_id} ê¸°ì—¬ë„ í•©ê³„: {total_contribution:.1f}%")
        
        # í•˜ì´ë¸Œë¦¬ë“œ 1ë‹¨ê³„: ì°¸ì—¬ì ìˆ˜ ë³´ì •
        kpi_tasks = fetch_kpi_tasks(kpi_id, state['period_id'])
        participants_count = len(set(task['emp_no'] for task in kpi_tasks))
        
        print(f"      â€¢ KPI {kpi_id}: {evaluation_type} í‰ê°€, ì°¸ì—¬ì {participants_count}ëª…")
        
        for emp_no, contribution_rate in contributions.items():
            # 1ë‹¨ê³„: ì°¸ì—¬ì ìˆ˜ ë³´ì •
            adjusted_score = contribution_rate * participants_count
            
            # 2ë‹¨ê³„: KPI ë¹„ì¤‘ ì ìš©
            kpi_weight = kpi_data.get('weight', 0) / 100.0
            weighted_score = adjusted_score * kpi_weight
            
            if emp_no not in kpi_contributions_by_emp:
                kpi_contributions_by_emp[emp_no] = 0
            kpi_contributions_by_emp[emp_no] += weighted_score
            
            print(f"        - {emp_no}: ì›ë˜ {contribution_rate:.1f}% â†’ ë³´ì • {adjusted_score:.1f} â†’ ê°€ì¤‘ {weighted_score:.1f}")
        
        # Taskë³„ ê¸°ì—¬ë„ ì—…ë°ì´íŠ¸ (ì›ë˜ KPIë³„ ê¸°ì—¬ë„ ì €ì¥)
        for task in kpi_tasks:
            task_data = fetch_cumulative_task_data(task['task_id'], state['period_id'])
            if not task_data:
                continue
                
            emp_contribution = contributions.get(task['emp_no'], 0)
            
            update_data = {
                "ai_contribution_score": int(emp_contribution)  # KPIë³„ ì›ë˜ ê¸°ì—¬ë„
            }
            
            if update_task_summary(task_data['task_summary_id'], update_data):
                updated_task_ids.append(task['task_id'])
    
    # í•˜ì´ë¸Œë¦¬ë“œ 3ë‹¨ê³„: íŒ€ ë‚´ % ê¸°ì—¬ë„ ë³€í™˜
    total_team_score = sum(kpi_contributions_by_emp.values())
    final_contributions = {}
    
    if total_team_score > 0:
        for emp_no in kpi_contributions_by_emp:
            percentage = (kpi_contributions_by_emp[emp_no] / total_team_score) * 100
            final_contributions[emp_no] = round(percentage, 2)
            print(f"      â€¢ {emp_no} ìµœì¢… ê¸°ì—¬ë„: {percentage:.1f}%")
    else:
        # íŒ€ ì ìˆ˜ê°€ 0ì¸ ê²½ìš° ë™ë“± ë¶„ë°°
        emp_count = len(kpi_contributions_by_emp)
        if emp_count > 0:
            equal_share = 100.0 / emp_count
            for emp_no in kpi_contributions_by_emp:
                final_contributions[emp_no] = round(equal_share, 2)
    
    # ìµœì¢… ê¸°ì—¬ë„ë¥¼ feedback_reports ë˜ëŠ” final_evaluation_reportsì— ì €ì¥
    save_final_contributions_to_db(state, final_contributions)
    
    # ë””ë²„ê¹…: í•˜ì´ë¸Œë¦¬ë“œ ê³„ì‚° ê³¼ì • ì‹œê°í™”
    debug_contribution_calculation(state)
    
    state['updated_task_ids'] = list(set((state['updated_task_ids'] or []) + updated_task_ids))
    print(f"   âœ… ê¸°ì—¬ë„ ê³„ì‚° ì™„ë£Œ: {len(updated_task_ids)}ê°œ Task ì—…ë°ì´íŠ¸, {len(final_contributions)}ëª… ìµœì¢… ê¸°ì—¬ë„ ì €ì¥")
    return state

def save_final_contributions_to_db(state: Module2State, final_contributions: Dict[str, float]):
    """ìµœì¢… ê¸°ì—¬ë„ë¥¼ DBì— ì €ì¥"""
    team_members = fetch_team_members(state['team_id'])
    
    for member in team_members:
        if member.get('role') == 'MANAGER':
            continue
            
        emp_no = member['emp_no']
        final_contribution = final_contributions.get(emp_no, 0)
        
        if state['report_type'] == "quarterly":
            # ë¶„ê¸°ë³„: feedback_reportsì— ì €ì¥
            save_feedback_report(
                emp_no, 
                state['team_evaluation_id'] or 0,
                {"contribution_rate": int(final_contribution)}  # ê¸°ì¡´ ì»¬ëŸ¼ëª… ì‚¬ìš©
            )
        else:
            # ì—°ë§: final_evaluation_reportsì— ì €ì¥
            save_final_evaluation_report(
                emp_no,
                state['team_evaluation_id'] or 0,
                {"contribution_rate": int(final_contribution)}  # ê¸°ì¡´ ì»¬ëŸ¼ëª… ì‚¬ìš©
            )

def debug_contribution_calculation(state: Module2State):
    """ê¸°ì—¬ë„ ê³„ì‚° ê³¼ì • ë””ë²„ê¹… - í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ ê²€ì¦"""
    print(f"\nğŸ” ê¸°ì—¬ë„ ê³„ì‚° ê³¼ì • ë””ë²„ê¹…")
    print(f"{'='*50}")
    
    # 1ë‹¨ê³„: KPIë³„ ì›ë˜ ê¸°ì—¬ë„ ìˆ˜ì§‘
    kpi_contributions = {}
    for kpi_id in state['target_team_kpi_ids']:
        evaluation_type = check_evaluation_type(kpi_id)
        kpi_data = fetch_team_kpi_data(kpi_id)
        kpi_tasks = fetch_kpi_tasks(kpi_id, state['period_id'])
        participants_count = len(set(task['emp_no'] for task in kpi_tasks))
        
        if evaluation_type == "quantitative":
            contributions = calculate_quantitative_contributions(kpi_id, state['period_id'])
        else:
            contributions = calculate_qualitative_contributions(kpi_id, state['period_id'], kpi_data)
        
        kpi_contributions[kpi_id] = {
            'kpi_name': kpi_data.get('kpi_name', f'KPI{kpi_id}'),
            'weight': kpi_data.get('weight', 0),
            'participants_count': participants_count,
            'contributions': contributions
        }
    
    # 2ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ê³„ì‚° ê³¼ì • ì‹œê°í™”
    print(f"ğŸ“Š KPIë³„ ê¸°ì—¬ë„ ë¶„ì„:")
    for kpi_id, kpi_info in kpi_contributions.items():
        print(f"\nğŸ¯ {kpi_info['kpi_name']} (ë¹„ì¤‘: {kpi_info['weight']}%, ì°¸ì—¬ì: {kpi_info['participants_count']}ëª…)")
        print(f"   ì›ë˜ ê¸°ì—¬ë„ â†’ ì°¸ì—¬ììˆ˜ ë³´ì • â†’ KPI ë¹„ì¤‘ ì ìš©")
        print(f"   {'â”€' * 50}")
        
        for emp_no, original_rate in kpi_info['contributions'].items():
            # 1ë‹¨ê³„: ì°¸ì—¬ì ìˆ˜ ë³´ì •
            adjusted = original_rate * kpi_info['participants_count']
            # 2ë‹¨ê³„: KPI ë¹„ì¤‘ ì ìš©
            weighted = adjusted * (kpi_info['weight'] / 100.0)
            
            print(f"   {emp_no}: {original_rate:5.1f}% â†’ {adjusted:6.1f} â†’ {weighted:6.1f}")
    
    # 3ë‹¨ê³„: ê°œì¸ë³„ ì¢…í•© ì ìˆ˜ ê³„ì‚°
    print(f"\nğŸ“ˆ ê°œì¸ë³„ ì¢…í•© ì ìˆ˜ (í•˜ì´ë¸Œë¦¬ë“œ 1-2ë‹¨ê³„ ê²°ê³¼):")
    emp_total_scores = {}
    
    for kpi_id, kpi_info in kpi_contributions.items():
        for emp_no, original_rate in kpi_info['contributions'].items():
            if emp_no not in emp_total_scores:
                emp_total_scores[emp_no] = 0
            
            adjusted = original_rate * kpi_info['participants_count']
            weighted = adjusted * (kpi_info['weight'] / 100.0)
            emp_total_scores[emp_no] += weighted
    
    for emp_no, total_score in sorted(emp_total_scores.items(), key=lambda x: x[1], reverse=True):
        print(f"   {emp_no}: {total_score:.1f}ì ")
    
    # 4ë‹¨ê³„: íŒ€ ë‚´ % ê¸°ì—¬ë„ ë³€í™˜
    total_team_score = sum(emp_total_scores.values())
    print(f"\nğŸ† ìµœì¢… ê¸°ì—¬ë„ (í•˜ì´ë¸Œë¦¬ë“œ 3ë‹¨ê³„ ê²°ê³¼):")
    print(f"   íŒ€ ì „ì²´ ì ìˆ˜: {total_team_score:.1f}")
    print(f"   {'â”€' * 30}")
    
    for emp_no, total_score in sorted(emp_total_scores.items(), key=lambda x: x[1], reverse=True):
        final_percentage = (total_score / total_team_score) * 100 if total_team_score > 0 else 0
        print(f"   {emp_no}: {final_percentage:.1f}% ({total_score:.1f}ì )")
    
    print(f"{'='*50}")

def team_analysis_submodule(state: Module2State) -> Module2State:
    """íŒ€ ëª©í‘œ ë¶„ì„ ì„œë¸Œëª¨ë“ˆ - ìš°ë¦¬ê°€ ìƒì˜í•œ LLM ê¸°ë°˜"""
    print(f"   ğŸ¢ íŒ€ ëª©í‘œ ë¶„ì„ ì¤‘...")
    
    updated_kpi_ids = []
    kpi_rates = []
    
    # ì •ëŸ‰ í‰ê°€ KPIë“¤ ì²˜ë¦¬ (LLMìœ¼ë¡œ íŒ€ KPI ë‹¬ì„±ë¥  ê³„ì‚°)
    for kpi_id in state['target_team_kpi_ids']:
        evaluation_type = check_evaluation_type(kpi_id)
        
        if evaluation_type == "quantitative":
            # ì •ëŸ‰ KPIë„ LLMì´ ì¢…í•© íŒë‹¨
            kpi_data = fetch_team_kpi_data(kpi_id)
            kpi_rate = calculate_team_kpi_achievement_rate(kpi_id, state['period_id'], kpi_data)
            
            update_data = {
                "ai_kpi_progress_rate": int(kpi_rate['rate']),
                "ai_kpi_analysis_comment": kpi_rate['comment']
            }
            
            if update_team_kpi(kpi_id, update_data):
                updated_kpi_ids.append(kpi_id)
                kpi_rates.append(kpi_rate['rate'])
        else:
            # ì •ì„± KPIëŠ” ì´ë¯¸ ì„œë¸Œëª¨ë“ˆ 3ì—ì„œ ì²˜ë¦¬ë¨
            kpi_data = fetch_team_kpi_data(kpi_id)
            if kpi_data.get('ai_kpi_progress_rate') is not None:
                kpi_rates.append(kpi_data['ai_kpi_progress_rate'])
    
    # íŒ€ ì „ì²´ í‰ê·  ë‹¬ì„±ë¥  ê³„ì‚° (KPI ë¹„ì¤‘ ê³ ë ¤)
    team_average_rate = calculate_team_average_achievement_rate(state['target_team_kpi_ids'])
    
    # team_evaluations ì—…ë°ì´íŠ¸
    team_eval_data = {
        "average_achievement_rate": int(team_average_rate)
    }
    
    # ì—°ë§ì¸ ê²½ìš° ì „ë…„ ëŒ€ë¹„ ì„±ì¥ë¥  ê³„ì‚° ì‹œë„
    if state['report_type'] == "annual":
        yoy_growth = calculate_year_over_year_growth(state['team_id'], state['period_id'], team_average_rate)
        if yoy_growth is not None:
            team_eval_data["year_over_year_growth"] = int(yoy_growth)
    
    if update_team_evaluations(state['team_evaluation_id'] or 0, team_eval_data):
        print(f"      â€¢ íŒ€ í‰ê·  ë‹¬ì„±ë¥ : {team_average_rate:.1f}%")
    
    state['updated_team_kpi_ids'] = updated_kpi_ids
    print(f"   âœ… íŒ€ ë¶„ì„ ì™„ë£Œ: {len(updated_kpi_ids)}ê°œ KPI ì—…ë°ì´íŠ¸")
    return state

def comment_generation_submodule(state: Module2State) -> Module2State:
    """ì½”ë©˜íŠ¸ ìƒì„± ì„œë¸Œëª¨ë“ˆ - í†µí•© ì‹œìŠ¤í…œ ì‚¬ìš©"""
    print(f"   ğŸ“ ì½”ë©˜íŠ¸ ìƒì„± ì¤‘...")
    
    # íŒ€ ì¼ê´€ì„± ê°€ì´ë“œ ìƒì„±
    team_context_guide = generate_team_consistency_guide(state['team_id'], state['period_id'])
    state['team_context_guide'] = team_context_guide
    
    # í†µí•© ì½”ë©˜íŠ¸ ìƒì„±ê¸° ì‚¬ìš©
    generate_task_comments_unified(state)
    generate_individual_summary_comments_unified(state)
    generate_team_overall_comment_unified(state)
    
    print(f"   âœ… ì½”ë©˜íŠ¸ ìƒì„± ì™„ë£Œ")
    return state

def generate_task_comments_unified(state: Module2State):
    """Taskë³„ ì½”ë©˜íŠ¸ ìƒì„± (í†µí•© ì‹œìŠ¤í…œ)"""
    period_type = "annual" if state['report_type'] == "annual" else "quarterly"
    
    for task_id in state['target_task_ids']:
        task_data = fetch_cumulative_task_data(task_id, state['period_id'])
        if not task_data:
            continue
        
        # í†µí•© ì½”ë©˜íŠ¸ ìƒì„±ê¸° ì‚¬ìš©
        generator = CommentGenerator("task", period_type, state['team_context_guide'])
        comment = generator.generate(task_data)
        
        if task_data.get('task_summary_id'):
            update_task_summary(task_data['task_summary_id'], {
                "ai_analysis_comment_task": comment
            })

def generate_individual_summary_comments_unified(state: Module2State):
    """ê°œì¸ ì¢…í•© ì½”ë©˜íŠ¸ ìƒì„± (í†µí•© ì‹œìŠ¤í…œ)"""
    if 'feedback_report_ids' not in state or state['feedback_report_ids'] is None:
        state['feedback_report_ids'] = []
    team_members = fetch_team_members(state['team_id'])
    period_type = "annual" if state['report_type'] == "annual" else "quarterly"
    
    for member in team_members:
        if member.get('role') == 'MANAGER':
            continue
        
        # ê°œì¸ Task ë°ì´í„° ìˆ˜ì§‘
        individual_tasks = []
        for task_id in state['target_task_ids']:
            task_data = fetch_cumulative_task_data(task_id, state['period_id'])
            if task_data and task_data.get('emp_no') == member['emp_no']:
                individual_tasks.append(task_data)
        
        if not individual_tasks:
            continue
        
        # í†µí•© ì½”ë©˜íŠ¸ ìƒì„±ê¸° ì‚¬ìš©
        generator = CommentGenerator("individual", period_type, state['team_context_guide'])
        comment = generator.generate({
            **member,
            "tasks": individual_tasks
        })
        
        # ë¶„ê¸°ë³„/ì—°ë§ë³„ ì €ì¥
        if state['report_type'] == "quarterly":
            feedback_report_id = save_feedback_report(
                member['emp_no'], 
                state['team_evaluation_id'] or 0,
                {"ai_overall_contribution_summary_comment": comment}
            )
            if state['feedback_report_ids'] is None:
                state['feedback_report_ids'] = []
            state['feedback_report_ids'].append(feedback_report_id)
        else:  # annual
            final_report_id = save_final_evaluation_report(
                member['emp_no'],
                state['team_evaluation_id'] or 0, 
                {"ai_annual_performance_summary_comment": comment}
            )
            if state['final_evaluation_report_ids'] is None:
                state['final_evaluation_report_ids'] = []
            state['final_evaluation_report_ids'].append(final_report_id)

def generate_team_overall_comment_unified(state: Module2State):
    """íŒ€ ì „ì²´ ë¶„ì„ ì½”ë©˜íŠ¸ ìƒì„± (í†µí•© ì‹œìŠ¤í…œ)"""
    if 'final_evaluation_report_ids' not in state or state['final_evaluation_report_ids'] is None:
        state['final_evaluation_report_ids'] = []
    # íŒ€ KPI ë°ì´í„° ìˆ˜ì§‘
    team_kpis_data = []
    for kpi_id in state['target_team_kpi_ids']:
        kpi_data = fetch_team_kpi_data(kpi_id)
        if kpi_data:
            team_kpis_data.append(kpi_data)
    
    period_type = "annual" if state['report_type'] == "annual" else "quarterly"
    
    # í†µí•© ì½”ë©˜íŠ¸ ìƒì„±ê¸° ì‚¬ìš©
    generator = CommentGenerator("team", period_type, state['team_context_guide'])
    comment = generator.generate({
        "kpis": team_kpis_data,
        "team_context": state['team_context_guide'].get('team_context', '') if state['team_context_guide'] else '',
        "performance_level": state['team_context_guide'].get('performance_level', '') if state['team_context_guide'] else ''
    })
    
    # team_evaluations ì—…ë°ì´íŠ¸
    update_team_evaluations(state['team_evaluation_id'] or 0, {
        "ai_team_overall_analysis_comment": comment
    })

def db_update_submodule(state: Module2State) -> Module2State:
    """ìµœì¢… DB ì—…ë°ì´íŠ¸ ì„œë¸Œëª¨ë“ˆ - íŠ¸ëœì­ì…˜ ì²˜ë¦¬"""
    print(f"   ğŸ’¾ ìµœì¢… DB ì—…ë°ì´íŠ¸ ì¤‘...")
    
    try:
        with engine.begin() as transaction:
            # ì´ë¯¸ ê° ì„œë¸Œëª¨ë“ˆì—ì„œ ì—…ë°ì´íŠ¸í–ˆìœ¼ë¯€ë¡œ ìµœì¢… ê²€ì¦ë§Œ ìˆ˜í–‰
            
            # 1. ë¶„ê¸°ë³„ ì¶”ê°€ ì—…ë°ì´íŠ¸ (ranking, cumulative ë°ì´í„°)
            if state['report_type'] == "quarterly":
                update_quarterly_specific_data(state)
            
            # 2. ì—°ë§ ì¶”ê°€ ì—…ë°ì´íŠ¸ (final_evaluation_reports ì¶”ê°€ í•„ë“œ)
            elif state['report_type'] == "annual":
                update_annual_specific_data(state)
            
            # 3. ì—…ë°ì´íŠ¸ ê²°ê³¼ ê²€ì¦
            validation_result = validate_final_update_results(state)
            
            if not validation_result['success']:
                raise DataIntegrityError(f"Final validation failed: {validation_result['errors']}")
            
            # 4. ì—…ë°ì´íŠ¸ í†µê³„ ë¡œê¹…
            updated_tasks = len(state['updated_task_ids'] or [])
            updated_kpis = len(state['updated_team_kpi_ids'] or [])
            updated_feedback_reports = len(state['feedback_report_ids'] or [])
            updated_final_reports = len(state['final_evaluation_report_ids'] or [])
            
            print(f"      â€¢ Task ì—…ë°ì´íŠ¸: {updated_tasks}ê°œ")
            print(f"      â€¢ KPI ì—…ë°ì´íŠ¸: {updated_kpis}ê°œ")
            print(f"      â€¢ í”¼ë“œë°± ë¦¬í¬íŠ¸: {updated_feedback_reports}ê°œ")
            print(f"      â€¢ ìµœì¢… ë¦¬í¬íŠ¸: {updated_final_reports}ê°œ")
            
            # 5. ìµœì¢… ìƒíƒœ ë¡œê¹…
            if state['report_type'] == "quarterly":
                print(f"      â€¢ ë¶„ê¸° í‰ê°€ ì™„ë£Œ")
            else:
                print(f"      â€¢ ì—°ë§ í‰ê°€ ì™„ë£Œ")
                
            return state
                
    except Exception as e:
        print(f"   âŒ ìµœì¢… DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        raise

def update_quarterly_specific_data(state: Module2State):
    """ë¶„ê¸°ë³„ ì „ìš© ë°ì´í„° ì—…ë°ì´íŠ¸ - ê°œì¸ ë‹¬ì„±ë¥  ê¸°ë°˜ ìˆœìœ„ ë§¤ê¸°ê¸°"""
    print(f"      ğŸ“Š ë¶„ê¸°ë³„ ì „ìš© ë°ì´í„° ì—…ë°ì´íŠ¸ ì¤‘...")
    
    # 1. íŒ€ ë‚´ ê°œì¸ë³„ ë‹¬ì„±ë¥  ê¸°ë°˜ ìˆœìœ„ ê³„ì‚° ë° ì—…ë°ì´íŠ¸
    team_ranking_result = calculate_team_ranking(state)
    
    # 2. ìˆœìœ„ ê²°ê³¼ë¥¼ feedback_reportsì— ì €ì¥ (ê¸°ì—¬ë„ëŠ” ì´ë¯¸ í•˜ì´ë¸Œë¦¬ë“œ 3ë‹¨ê³„ì—ì„œ ì €ì¥ë¨)
    update_team_ranking_to_feedback_reports(state, team_ranking_result)
    
    print(f"      âœ… ë¶„ê¸°ë³„ ìˆœìœ„ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(team_ranking_result)}ëª…")
    print(f"      ğŸ“Š íŒ€ ë‚´ ë‹¬ì„±ë¥  ìˆœìœ„:")
    for i, member in enumerate(team_ranking_result):
        print(f"        {i+1}ìœ„: {member['emp_name']}({member['emp_no']}) - {member['avg_achievement_rate']:.1f}%")

def calculate_team_ranking(state: Module2State) -> List[Dict]:
    """íŒ€ ë‚´ ê°œì¸ë³„ ë‹¬ì„±ë¥  ê¸°ë°˜ ìˆœìœ„ ê³„ì‚°"""
    print(f"        ğŸ† íŒ€ ë‚´ ìˆœìœ„ ê³„ì‚° ì¤‘...")
    
    team_members = fetch_team_members(state['team_id'])
    member_achievements = []
    
    for member in team_members:
        if member.get('role') == 'MANAGER':
            continue
            
        # ê°œì¸ë³„ Task ìˆ˜ì§‘
        individual_tasks = []
        for task_id in state['target_task_ids']:
            task_data = fetch_cumulative_task_data(task_id, state['period_id'])
            if task_data and task_data.get('emp_no') == member['emp_no']:
                individual_tasks.append(task_data)
        
        if individual_tasks:
            # ê°€ì¤‘í‰ê·  ë‹¬ì„±ë¥  ê³„ì‚° (ê¸°ì—¬ë„ëŠ” ì´ë¯¸ í•˜ì´ë¸Œë¦¬ë“œ 3ë‹¨ê³„ì—ì„œ ê³„ì‚°ë˜ì–´ ì €ì¥ë¨)
            result = calculate_individual_weighted_achievement_rate(individual_tasks)
            
            # ê³„ì‚° ê³¼ì • ìƒì„¸ ë¡œê¹…
            print(f"          ğŸ“ˆ {member['emp_name']}({member['emp_no']}) ë‹¬ì„±ë¥  ê³„ì‚°:")
            total_weighted_score = 0
            for task in individual_tasks:
                task_name = task.get('task_name', f'Task{task.get("task_id")}')
                task_weight = task.get('weight', 0)
                task_achievement = task.get('ai_achievement_rate', 0)
                weighted_score = task_achievement * task_weight
                total_weighted_score += weighted_score
                print(f"            â€¢ {task_name}: {task_achievement}% Ã— {task_weight} = {weighted_score}")
            
            print(f"            = {result['achievement_rate']:.1f}% (ì´ ê°€ì¤‘ì ìˆ˜: {total_weighted_score}, ì´ ê°€ì¤‘ì¹˜: {result['total_weight']})")
            
            member_achievements.append({
                'emp_no': member['emp_no'],
                'emp_name': member['emp_name'],
                'position': member.get('position', ''),
                'cl': member.get('cl', ''),
                'avg_achievement_rate': result['achievement_rate'],
                # 'avg_contribution_rate': result['contribution_rate'],  # ì‚­ì œ - í•˜ì´ë¸Œë¦¬ë“œ 3ë‹¨ê³„ì—ì„œ ì´ë¯¸ ê³„ì‚°ë¨
                'task_count': len(individual_tasks),
                'total_weight': result['total_weight'],
                'total_weighted_score': total_weighted_score
            })
        else:
            print(f"          âš ï¸  {member['emp_name']}({member['emp_no']}): ì°¸ì—¬ Task ì—†ìŒ")
    
    # ë‹¬ì„±ë¥  ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ë†’ì€ ë‹¬ì„±ë¥ ì´ 1ìœ„)
    member_achievements.sort(key=lambda x: x['avg_achievement_rate'], reverse=True)
    
    # ë™ì ì ì²˜ë¦¬ (ê°™ì€ ë‹¬ì„±ë¥ ì¸ ê²½ìš° ê°€ì¤‘ì ìˆ˜ë¡œ ì¬ì •ë ¬)
    for i in range(len(member_achievements) - 1):
        if member_achievements[i]['avg_achievement_rate'] == member_achievements[i + 1]['avg_achievement_rate']:
            # ë™ì ìì¸ ê²½ìš° ê°€ì¤‘ì ìˆ˜ë¡œ ì¬ì •ë ¬
            if member_achievements[i]['total_weighted_score'] < member_achievements[i + 1]['total_weighted_score']:
                member_achievements[i], member_achievements[i + 1] = member_achievements[i + 1], member_achievements[i]
    
    return member_achievements

def update_team_ranking_to_feedback_reports(state: Module2State, team_ranking: List[Dict]):
    """íŒ€ ìˆœìœ„ ê²°ê³¼ë¥¼ feedback_reportsì— ì €ì¥"""
    print(f"        ğŸ’¾ ìˆœìœ„ ê²°ê³¼ë¥¼ feedback_reportsì— ì €ì¥ ì¤‘...")
    
    updated_count = 0
    
    for i, member_data in enumerate(team_ranking):
        ranking = i + 1
        
        # feedback_reports ì—…ë°ì´íŠ¸ ë°ì´í„° - ê¸°ì—¬ë„ëŠ” ì´ë¯¸ ì˜¬ë°”ë¥´ê²Œ ì €ì¥ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì œì™¸
        feedback_data = {
            'ranking': ranking,  # íŒ€ ë‚´ ìˆœìœ„ (1, 2, 3, ...)
            'ai_achievement_rate': int(member_data['avg_achievement_rate']),  # ê°€ì¤‘í‰ê·  ë‹¬ì„±ë¥ 
            # 'contribution_rate': int(member_data['avg_contribution_rate'])  # ì‚­ì œ - í•˜ì´ë¸Œë¦¬ë“œ 3ë‹¨ê³„ ê¸°ì—¬ë„ê°€ ì´ë¯¸ ì €ì¥ë¨
        }
        
        # ê¸°ì¡´ feedback_report ì—…ë°ì´íŠ¸ ë˜ëŠ” ìƒˆë¡œ ìƒì„±
        feedback_report_id = save_feedback_report(
            member_data['emp_no'],
            state['team_evaluation_id'] or 0,
            feedback_data
        )
        
        updated_count += 1
        
        # ìˆœìœ„ ì €ì¥ ê²°ê³¼ ë¡œê¹…
        print(f"          {ranking}ìœ„: {member_data['emp_name']}({member_data['emp_no']}) - {member_data['avg_achievement_rate']:.1f}% â†’ feedback_report_id: {feedback_report_id}")
    
    print(f"        âœ… {updated_count}ëª…ì˜ ìˆœìœ„ ì •ë³´ ì €ì¥ ì™„ë£Œ")

def update_annual_specific_data(state: Module2State):
    """ì—°ë§ ì „ìš© ë°ì´í„° ì—…ë°ì´íŠ¸ - Task Weight ê¸°ë°˜ ê°€ì¤‘í‰ê· """
    print(f"      ğŸ“Š ì—°ë§ ì „ìš© ë°ì´í„° ì—…ë°ì´íŠ¸ ì¤‘...")
    
    team_members = fetch_team_members(state['team_id'])
    
    for member in team_members:
        if member.get('role') == 'MANAGER':
            continue
            
        # ê°œì¸ë³„ Task ìˆ˜ì§‘
        individual_tasks = []
        for task_id in state['target_task_ids']:
            task_data = fetch_cumulative_task_data(task_id, state['period_id'])
            if task_data and task_data.get('emp_no') == member['emp_no']:
                individual_tasks.append(task_data)
        
        if individual_tasks:
            # ê°€ì¤‘í‰ê·  ê³„ì‚°
            result = calculate_individual_weighted_achievement_rate(individual_tasks)
            
            # ê³„ì‚° ê³¼ì • ë¡œê¹…
            print(f"        ğŸ“ˆ {member['emp_name']}({member['emp_no']}) ì—°ê°„ ê°€ì¤‘í‰ê·  ê³„ì‚°:")
            for task in individual_tasks:
                task_name = task.get('task_name', f'Task{task.get("task_id")}')
                task_weight = task.get('weight', 0)
                task_achievement = task.get('ai_achievement_rate', 0)
                print(f"          â€¢ {task_name}: {task_achievement}% Ã— {task_weight} = {task_achievement * task_weight}")
            print(f"          = {result['achievement_rate']:.1f}% (ì´ ê°€ì¤‘ì¹˜: {result['total_weight']})")
            
            # final_evaluation_reports ì—…ë°ì´íŠ¸
            final_data = {
                'ai_annual_achievement_rate': int(result['achievement_rate'])
            }
            
            save_final_evaluation_report(
                member['emp_no'],
                state['team_evaluation_id'] or 0,
                final_data
            )
    
    print(f"      âœ… ì—°ë§ ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len([m for m in team_members if m.get('role') != 'MANAGER'])}ëª…")

def validate_final_update_results(state: Module2State) -> Dict[str, Any]:
    """ìµœì¢… ì—…ë°ì´íŠ¸ ê²°ê³¼ ê²€ì¦"""
    errors = []
    warnings = []
    
    try:
        # 1. Task ì—…ë°ì´íŠ¸ ê²€ì¦
        for task_id in (state['updated_task_ids'] or []):
            task_data = fetch_cumulative_task_data(task_id, state['period_id'])
            
            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            if task_data.get('ai_achievement_rate') is None:
                errors.append(f"Task {task_id}: ai_achievement_rate not updated")
            
            if task_data.get('ai_contribution_score') is None:
                errors.append(f"Task {task_id}: ai_contribution_score not updated")
            
            if not task_data.get('ai_analysis_comment_task'):
                warnings.append(f"Task {task_id}: ai_analysis_comment_task empty")
            
            # ì—°ë§ ì „ìš© ê²€ì¦
            if state['report_type'] == "annual" and not task_data.get('ai_assessed_grade'):
                warnings.append(f"Task {task_id}: ai_assessed_grade not set for annual evaluation")
        
        # 2. Team KPI ì—…ë°ì´íŠ¸ ê²€ì¦
        for kpi_id in (state['updated_team_kpi_ids'] or []):
            kpi_data = fetch_team_kpi_data(kpi_id)
            
            if kpi_data.get('ai_kpi_progress_rate') is None:
                errors.append(f"KPI {kpi_id}: ai_kpi_progress_rate not updated")
            
            if not kpi_data.get('ai_kpi_analysis_comment'):
                warnings.append(f"KPI {kpi_id}: ai_kpi_analysis_comment empty")
        
        # 3. Team evaluation ê²€ì¦
        if state['team_evaluation_id']:
            with engine.connect() as connection:
                query_text = """
                    SELECT average_achievement_rate, ai_team_overall_analysis_comment,
                           year_over_year_growth
                    FROM team_evaluations 
                    WHERE team_evaluation_id = :team_evaluation_id
                """
                from sqlalchemy import text
                query = text(query_text)
                result = connection.execute(query, {"team_evaluation_id": state['team_evaluation_id']})
                row = result.fetchone()
                team_eval = row_to_dict(row) if row else {}
                
                if team_eval.get('average_achievement_rate') is None:
                    errors.append("Team evaluation: average_achievement_rate not updated")
                
                if not team_eval.get('ai_team_overall_analysis_comment'):
                    warnings.append("Team evaluation: ai_team_overall_analysis_comment empty")
        
        # 4. ë¶„ê¸°ë³„ íŒ€ ìˆœìœ„ ê²€ì¦ (ìƒˆë¡œ ì¶”ê°€)
        if state['report_type'] == "quarterly":
            ranking_validation = validate_team_ranking_data(state)
            if not ranking_validation['success']:
                errors.extend(ranking_validation['errors'])
            warnings.extend(ranking_validation['warnings'])
            
            print(f"      ğŸ“Š íŒ€ ìˆœìœ„ ê²€ì¦ ê²°ê³¼:")
            print(f"        â€¢ ìˆœìœ„ ë°ì´í„°: {ranking_validation['ranking_count']}ëª…")
            print(f"        â€¢ íŒ€ì› ìˆ˜: {ranking_validation['team_member_count']}ëª…")
        
        # 5. ë ˆí¬íŠ¸ ê²€ì¦
        feedback_report_ids = state.get('feedback_report_ids') or []
        final_evaluation_report_ids = state.get('final_evaluation_report_ids') or []
        
        if state['report_type'] == "quarterly" and feedback_report_ids:
            for report_id in feedback_report_ids:
                # feedback_reports ê²€ì¦ ë¡œì§
                pass
        
        elif state['report_type'] == "annual" and final_evaluation_report_ids:
            for report_id in final_evaluation_report_ids:
                # final_evaluation_reports ê²€ì¦ ë¡œì§
                pass
        
        # 6. ë°ì´í„° ì¼ê´€ì„± ê²€ì¦
        consistency_errors = validate_data_consistency(state)
        errors.extend(consistency_errors)
        
        success = len(errors) == 0
        
        if warnings:
            print(f"      âš ï¸  ê²€ì¦ ê²½ê³ : {len(warnings)}ê±´")
        
        return {
            'success': success,
            'errors': errors,
            'warnings': warnings,
            'stats': {
                'tasks_validated': len(state['updated_task_ids'] or []),
                'kpis_validated': len(state['updated_team_kpi_ids'] or []),
                'reports_validated': len(state['feedback_report_ids'] or []) + len(state['final_evaluation_report_ids'] or []),
                'ranking_validated': state['report_type'] == "quarterly"
            }
        }
        
    except Exception as e:
        print(f"      âŒ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
        return {
            'success': False,
            'errors': [f"Validation process error: {str(e)}"],
            'warnings': [],
            'stats': {}
        }

def validate_team_ranking_data(state: Module2State) -> Dict[str, Any]:
    """íŒ€ ìˆœìœ„ ë°ì´í„° ê²€ì¦"""
    print(f"        ğŸ” íŒ€ ìˆœìœ„ ë°ì´í„° ê²€ì¦ ì¤‘...")
    
    errors = []
    warnings = []
    
    try:
        from sqlalchemy import text
        with engine.connect() as connection:
            # feedback_reportsì—ì„œ ìˆœìœ„ ë°ì´í„° ì¡°íšŒ
            query = text("""
                SELECT emp_no, ranking, ai_achievement_rate, contribution_rate
                FROM feedback_reports 
                WHERE team_evaluation_id = :team_evaluation_id
                ORDER BY ranking
            """)
            results = connection.execute(query, {"team_evaluation_id": state['team_evaluation_id']})
            ranking_data = [row_to_dict(row) for row in results]
            
            if not ranking_data:
                errors.append("íŒ€ ìˆœìœ„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return {'success': False, 'errors': errors, 'warnings': warnings}
            
            # 1. ìˆœìœ„ ì—°ì†ì„± ê²€ì¦
            expected_rankings = list(range(1, len(ranking_data) + 1))
            actual_rankings = [r['ranking'] for r in ranking_data]
            
            if actual_rankings != expected_rankings:
                errors.append(f"ìˆœìœ„ê°€ ì—°ì†ì ì´ì§€ ì•ŠìŒ: ì˜ˆìƒ {expected_rankings}, ì‹¤ì œ {actual_rankings}")
            
            # 2. ë‹¬ì„±ë¥  ë²”ìœ„ ê²€ì¦
            for rank_data in ranking_data:
                achievement_rate = rank_data.get('ai_achievement_rate', 0)
                if not (0 <= achievement_rate <= 200):
                    errors.append(f"ì‚¬ë²ˆ {rank_data['emp_no']}: ë‹¬ì„±ë¥  {achievement_rate}%ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨")
                
                contribution_rate = rank_data.get('contribution_rate', 0)
                if not (0 <= contribution_rate <= 100):
                    warnings.append(f"ì‚¬ë²ˆ {rank_data['emp_no']}: ê¸°ì—¬ë„ {contribution_rate}%ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨")
            
            # 3. ìˆœìœ„ì™€ ë‹¬ì„±ë¥  ì¼ê´€ì„± ê²€ì¦
            for i in range(len(ranking_data) - 1):
                current_rate = ranking_data[i]['ai_achievement_rate']
                next_rate = ranking_data[i + 1]['ai_achievement_rate']
                
                if current_rate < next_rate:
                    errors.append(f"ìˆœìœ„ {i+1}ìœ„({ranking_data[i]['emp_no']})ì˜ ë‹¬ì„±ë¥  {current_rate}%ê°€ {i+2}ìœ„({ranking_data[i+1]['emp_no']})ì˜ ë‹¬ì„±ë¥  {next_rate}%ë³´ë‹¤ ë‚®ìŒ")
            
            # 4. íŒ€ì› ìˆ˜ì™€ ìˆœìœ„ ìˆ˜ ì¼ì¹˜ ê²€ì¦
            team_members = fetch_team_members(state['team_id'])
            non_manager_count = len([m for m in team_members if m.get('role') != 'MANAGER'])
            
            if len(ranking_data) != non_manager_count:
                warnings.append(f"íŒ€ì› ìˆ˜({non_manager_count}ëª…)ì™€ ìˆœìœ„ ìˆ˜({len(ranking_data)}ëª…)ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŒ")
            
            success = len(errors) == 0
            
            if warnings:
                print(f"          âš ï¸  ê²€ì¦ ê²½ê³ : {len(warnings)}ê±´")
            
            return {
                'success': success,
                'errors': errors,
                'warnings': warnings,
                'ranking_count': len(ranking_data),
                'team_member_count': non_manager_count
            }
            
    except Exception as e:
        print(f"          âŒ ìˆœìœ„ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
        return {
            'success': False,
            'errors': [f"ìˆœìœ„ ë°ì´í„° ê²€ì¦ ì˜¤ë¥˜: {str(e)}"],
            'warnings': [],
            'ranking_count': 0,
            'team_member_count': 0
        }

def validate_data_consistency(state: Module2State) -> List[str]:
    """ë°ì´í„° ì¼ê´€ì„± ê²€ì¦"""
    errors = []
    
    try:
        # 2. ë‹¬ì„±ë¥  ë²”ìœ„ ê²€ì¦
        for task_id in state['updated_task_ids'] or []:
            task_data = fetch_cumulative_task_data(task_id, state['period_id'])
            achievement_rate = task_data.get('ai_achievement_rate', 0)
            
            if not (0 <= achievement_rate <= 200):
                errors.append(f"Task {task_id}: achievement_rate {achievement_rate} out of range")
        
        # 3. íŒ€ í‰ê·  ë‹¬ì„±ë¥ ê³¼ ê°œë³„ ë‹¬ì„±ë¥  ì¼ê´€ì„± ê²€ì¦
        if state['team_evaluation_id']:
            from sqlalchemy import text
            with engine.connect() as connection:
                query = text("""
                    SELECT average_achievement_rate 
                    FROM team_evaluations 
                    WHERE team_evaluation_id = :team_evaluation_id
                """)
                result = connection.execute(query, {"team_evaluation_id": state['team_evaluation_id']})
                team_avg = result.scalar_one_or_none()
                
                # ê°œë³„ Taskë“¤ì˜ ê°€ì¤‘í‰ê· ê³¼ íŒ€ í‰ê· ì´ í¬ê²Œ ë‹¤ë¥´ì§€ ì•Šì€ì§€ í™•ì¸
                calculated_avg = calculate_team_average_achievement_rate(state['target_team_kpi_ids'])
                
                if team_avg and abs(team_avg - calculated_avg) > 15:
                    errors.append(f"Team average inconsistency: stored {team_avg} vs calculated {calculated_avg}")
        
    except Exception as e:
        errors.append(f"Consistency validation error: {str(e)}")
    
    return errors

# ================================================================
# LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±
# ================================================================

def create_module2_graph():
    """ëª¨ë“ˆ 2 ê·¸ë˜í”„ ìƒì„± ë° ë°˜í™˜"""
    # StateGraphì— ì‚¬ìš©í•  íƒ€ì…: TypedDict ì‚¬ìš© (LangGraph ê¶Œì¥)
    module2_workflow = StateGraph(Module2State)

    # ê° ì„œë¸Œëª¨ë“ˆì„ ë…¸ë“œë¡œ ë“±ë¡
    module2_workflow.add_node("data_collection", data_collection_submodule)
    module2_workflow.add_node("achievement_and_grade", achievement_and_grade_calculation_submodule)
    module2_workflow.add_node("contribution", contribution_calculation_submodule)
    module2_workflow.add_node("team_analysis", team_analysis_submodule)
    module2_workflow.add_node("comment_generation", comment_generation_submodule)
    module2_workflow.add_node("db_update", db_update_submodule)

    # ì—£ì§€(ì‹¤í–‰ ìˆœì„œ) ì •ì˜
    module2_workflow.add_edge(START, "data_collection")
    module2_workflow.add_edge("data_collection", "achievement_and_grade")
    module2_workflow.add_edge("achievement_and_grade", "contribution")
    module2_workflow.add_edge("contribution", "team_analysis")
    module2_workflow.add_edge("team_analysis", "comment_generation")
    module2_workflow.add_edge("comment_generation", "db_update")
    module2_workflow.add_edge("db_update", END)

    # ê·¸ë˜í”„ ì»´íŒŒì¼
    return module2_workflow.compile()