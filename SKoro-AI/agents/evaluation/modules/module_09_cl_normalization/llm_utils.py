# ================================================================
# llm_utils_1.py - ëª¨ë“ˆ 9 LLM ì²˜ë¦¬ ë° JSON íŒŒì‹± ìœ í‹¸ë¦¬í‹°
# ================================================================

import re
import json
import time
import logging
from typing import Dict, List
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

# LLM í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
llm_client = ChatOpenAI(model="gpt-4o-mini", temperature=0)
logger = logging.getLogger(__name__)

def _extract_json_from_llm_response(text: str) -> str:
    """LLM ì‘ë‹µì—ì„œ JSON ë¸”ë¡ ì¶”ì¶œ"""
    # ```json ... ``` í˜•íƒœ ì¶”ì¶œ
    match = re.search(r"```(?:json)?\s*(.*?)\s*```", text, re.DOTALL)
    if match:
        return match.group(1).strip()
    
    # { ... } í˜•íƒœ ì¶”ì¶œ (ê°€ì¥ í° JSON ê°ì²´)
    brace_count = 0
    start_idx = -1
    for i, char in enumerate(text):
        if char == '{':
            if brace_count == 0:
                start_idx = i
            brace_count += 1
        elif char == '}':
            brace_count -= 1
            if brace_count == 0 and start_idx != -1:
                return text[start_idx:i+1]
    
    return text.strip()

# ================================================================
# ì—ëŸ¬ ì²˜ë¦¬ í´ë˜ìŠ¤
# ================================================================

class Module9ValidationError(Exception):
    pass

# ================================================================
# ì—…ë¬´ ì¦ê±° ì¼ì¹˜ì„± ë¶„ì„ í•¨ìˆ˜ë“¤
# ================================================================

def _fallback_task_evidence_analysis(captain_reason: str, task_data: List[Dict]) -> float:
    """LLM ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ê°„ë‹¨í•œ ì—…ë¬´ ì¦ê±° ë¶„ì„"""
    
    if not captain_reason or not task_data:
        return 0.3
    
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­
    evidence_score = 0.0
    total_checks = 0
    
    performance_indicators = ['ì™„ë£Œ', 'ë‹¬ì„±', 'ê°œì„ ', 'ì„±ê³µ', 'ìš°ìˆ˜', 'ê¸°ì—¬']
    
    for indicator in performance_indicators:
        total_checks += 1
        if indicator in captain_reason:
            # ì‹¤ì œ ì—…ë¬´ì—ì„œ ë†’ì€ ì„±ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
            high_performance = any(
                task.get('ai_achievement_rate', 0) >= 80 or 
                task.get('ai_contribution_score', 0) >= 70
                for task in task_data
            )
            if high_performance:
                evidence_score += 1.0
            else:
                evidence_score += 0.3
    
    return min(1.0, evidence_score / total_checks if total_checks > 0 else 0.3)

def _fallback_peer_evaluation_analysis(captain_reason: str, peer_data: Dict) -> float:
    """LLM ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ê°„ë‹¨í•œ ë™ë£Œí‰ê°€ ë¶„ì„"""
    
    if not captain_reason or not peer_data:
        return 0.5
    
    strengths = peer_data.get('strengths', '').lower()
    concerns = peer_data.get('concerns', '').lower()
    
    # ê¸ì •ì  í‚¤ì›Œë“œ ë§¤ì¹­
    positive_keywords = ['ìš°ìˆ˜', 'ë›°ì–´ë‚œ', 'ì˜', 'ì„±ê³µ', 'ê¸°ì—¬', 'ë¦¬ë”ì‹­', 'í˜‘ì—…']
    negative_keywords = ['ë¶€ì¡±', 'ë¬¸ì œ', 'ì§€ì—°', 'ë¯¸í¡', 'ê°œì„ ']
    
    captain_positive = any(word in captain_reason for word in positive_keywords)
    peer_positive = len(strengths) > len(concerns)
    
    if captain_positive and peer_positive:
        return 0.8
    elif not captain_positive and not peer_positive:
        return 0.7
    elif captain_positive and not peer_positive:
        return 0.2  # íŒ€ì¥ì€ ê¸ì •ì ì¸ë° ë™ë£ŒëŠ” ë¶€ì •ì 
    else:
        return 0.6  # íŒ€ì¥ì€ ë¶€ì •ì ì¸ë° ë™ë£ŒëŠ” ê¸ì •ì 

def analyze_task_evidence_consistency(member: Dict) -> float:
    """LLM ê¸°ë°˜ ì—…ë¬´ ì‹¤ì ê³¼ íŒ€ì¥ ì‚¬ìœ ì˜ ì¼ì¹˜ì„± ë¶„ì„"""
    
    captain_reason = member.get('captain_reason', '') or ''
    task_data = member.get('task_data', [])
    
    if not captain_reason.strip() or not task_data:
        return 0.3  # ë°ì´í„° ë¶€ì¡± ì‹œ ë‚®ì€ ì ìˆ˜
    
    # ì—…ë¬´ ë°ì´í„° ìš”ì•½
    task_summary = []
    for task in task_data:
        task_info = {
            "task_name": task.get('task_name', ''),
            "task_detail": task.get('task_detail', ''),
            "achievement_rate": task.get('ai_achievement_rate', 0),
            "contribution_score": task.get('ai_contribution_score', 0),
            "ai_comment": task.get('ai_analysis_comment_task', '')
        }
        task_summary.append(task_info)
    
    # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = """ë‹¹ì‹ ì€ ì¸ì‚¬í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. íŒ€ì¥ì´ ì œì‹œí•œ ìˆ˜ì • ì‚¬ìœ ê°€ ì§ì›ì˜ ì‹¤ì œ ì—…ë¬´ ì„±ê³¼ì™€ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë¶„ì„ ê¸°ì¤€:
1. íŒ€ì¥ ì‚¬ìœ ì—ì„œ ì–¸ê¸‰í•œ ì„±ê³¼ê°€ ì‹¤ì œ ì—…ë¬´ ë°ì´í„°ì—ì„œ í™•ì¸ë˜ëŠ”ê°€?
2. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ê²°ê³¼ê°€ ì—…ë¬´ ì„±ê³¼ì™€ ë¶€í•©í•˜ëŠ”ê°€?
3. íŒ€ì¥ì´ ê°•ì¡°í•œ ê°•ì ë“¤ì´ ì—…ë¬´ ë‹¬ì„±ë¥ ì´ë‚˜ ê¸°ì—¬ë„ì— ë°˜ì˜ë˜ì–´ ìˆëŠ”ê°€?

ì ìˆ˜ ê¸°ì¤€:
- 0.9-1.0: ì™„ì „íˆ ì¼ì¹˜ (êµ¬ì²´ì  ì¦ê±°ì™€ ìˆ˜ì¹˜ê°€ ì •í™•íˆ ë¶€í•©)
- 0.7-0.8: ëŒ€ì²´ë¡œ ì¼ì¹˜ (ì£¼ìš” ë‚´ìš©ì€ ë¶€í•©í•˜ë‚˜ ì¼ë¶€ ê³¼ì¥ ë˜ëŠ” ëˆ„ë½)
- 0.5-0.6: ë¶€ë¶„ì  ì¼ì¹˜ (ì¼ë¶€ ë‚´ìš©ë§Œ í™•ì¸ë˜ê³  ë‚˜ë¨¸ì§€ëŠ” ëª¨í˜¸)
- 0.3-0.4: ë¶ˆì¼ì¹˜ (ì‚¬ìœ ì™€ ì‹¤ì œ ì„±ê³¼ ê°„ ìƒë‹¹í•œ ì°¨ì´)
- 0.1-0.2: ì™„ì „íˆ ë¶ˆì¼ì¹˜ (ì‚¬ìœ ê°€ ì‹¤ì œ ì„±ê³¼ì™€ ë°˜ëŒ€ë˜ê±°ë‚˜ ê·¼ê±° ì—†ìŒ)

ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì œê³µí•˜ì„¸ìš”:
{
  "consistency_score": 0.0-1.0 ì‚¬ì´ ì ìˆ˜,
  "evidence_matches": ["ì¼ì¹˜í•˜ëŠ” ì¦ê±° í•­ëª©ë“¤"],
  "evidence_conflicts": ["ë¶ˆì¼ì¹˜í•˜ëŠ” í•­ëª©ë“¤"],
  "analysis_summary": "ë¶„ì„ ìš”ì•½ (50ì ì´ë‚´)"
}"""

    user_prompt = f"""íŒ€ì¥ ìˆ˜ì • ì‚¬ìœ :
"{captain_reason}"

ì‹¤ì œ ì—…ë¬´ ì„±ê³¼:
{json.dumps(task_summary, ensure_ascii=False, indent=2)}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ íŒ€ì¥ ì‚¬ìœ ì™€ ì—…ë¬´ ì‹¤ì ì˜ ì¼ì¹˜ì„±ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."""

    try:
        # LLM í˜¸ì¶œ
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        response = llm_client.invoke(messages)
        response_text = str(response.content)  # ì•ˆì „í•œ ë¬¸ìì—´ ë³€í™˜
        
        # JSON ì¶”ì¶œ ë° íŒŒì‹±
        json_text = _extract_json_from_llm_response(response_text)
        result = json.loads(json_text)
        
        consistency_score = result.get("consistency_score", 0.5)
        return max(0.0, min(1.0, consistency_score))
        
    except Exception as e:
        print(f"âš ï¸ LLM ì—…ë¬´ ì¦ê±° ì¼ì¹˜ì„± ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        # Fallback: ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„
        return _fallback_task_evidence_analysis(captain_reason, task_data)

def analyze_peer_evaluation_consistency(member: Dict) -> float:
    """LLM ê¸°ë°˜ ë™ë£Œí‰ê°€ì™€ íŒ€ì¥ ì‚¬ìœ ì˜ ì¼ì¹˜ì„± ë¶„ì„"""
    
    captain_reason = member.get('captain_reason', '')
    peer_data = member.get('peer_evaluation_data', {})
    
    if not captain_reason or not peer_data:
        return 0.5  # ì¤‘ë¦½
    
    # ë™ë£Œí‰ê°€ ë°ì´í„° ì¶”ì¶œ
    strengths = peer_data.get('strengths', '')
    concerns = peer_data.get('concerns', '')
    collaboration_obs = peer_data.get('collaboration_observations', '')
    
    if not (strengths or concerns or collaboration_obs):
        return 0.5  # AI ìš”ì•½ ë°ì´í„° ì—†ìŒ
    
    peer_summary = {
        "strengths": strengths,
        "concerns": concerns,
        "collaboration_observations": collaboration_obs
    }
    
    # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = """ë‹¹ì‹ ì€ ì¸ì‚¬í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. íŒ€ì¥ì´ ì œì‹œí•œ ìˆ˜ì • ì‚¬ìœ ê°€ ë™ë£Œí‰ê°€ ê²°ê³¼ì™€ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë¶„ì„ ê¸°ì¤€:
1. íŒ€ì¥ì´ ì–¸ê¸‰í•œ ê°•ì ë“¤ì´ ë™ë£Œí‰ê°€ì˜ strengthsì—ì„œ í™•ì¸ë˜ëŠ”ê°€?
2. íŒ€ì¥ ì‚¬ìœ ì™€ ë™ë£Œë“¤ì˜ concerns ì‚¬ì´ì— ëª¨ìˆœì´ ìˆëŠ”ê°€?
3. í˜‘ì—… ê´€ë ¨ ì–¸ê¸‰ì´ collaboration_observationsì™€ ë¶€í•©í•˜ëŠ”ê°€?
4. ì „ë°˜ì ì¸ í‰ê°€ í†¤(ê¸ì •/ë¶€ì •)ì´ ì¼ì¹˜í•˜ëŠ”ê°€?

ì ìˆ˜ ê¸°ì¤€:
- 0.9-1.0: ì™„ì „íˆ ì¼ì¹˜ (íŒ€ì¥ ì‚¬ìœ ê°€ ë™ë£Œí‰ê°€ì™€ ì™„ë²½í•˜ê²Œ ë¶€í•©)
- 0.7-0.8: ëŒ€ì²´ë¡œ ì¼ì¹˜ (ì£¼ìš” ë‚´ìš©ì€ ë¶€í•©í•˜ë‚˜ ì¼ë¶€ ì°¨ì´)
- 0.5-0.6: ì¤‘ë¦½ì  (íŠ¹ë³„í•œ ì¼ì¹˜ë‚˜ ë¶ˆì¼ì¹˜ ì—†ìŒ)
- 0.3-0.4: ë¶ˆì¼ì¹˜ (íŒ€ì¥ ì‚¬ìœ ì™€ ë™ë£Œí‰ê°€ ê°„ ìƒë‹¹í•œ ì°¨ì´)
- 0.1-0.2: ì‹¬ê°í•œ ë¶ˆì¼ì¹˜ (ì •ë°˜ëŒ€ í‰ê°€ë‚˜ ëª…ë°±í•œ ëª¨ìˆœ)

ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì œê³µí•˜ì„¸ìš”:
{
  "consistency_score": 0.0-1.0 ì‚¬ì´ ì ìˆ˜,
  "alignment_points": ["ì¼ì¹˜í•˜ëŠ” í‰ê°€ í¬ì¸íŠ¸ë“¤"],
  "contradiction_points": ["ëª¨ìˆœë˜ëŠ” í‰ê°€ í¬ì¸íŠ¸ë“¤"],
  "analysis_summary": "ë¶„ì„ ìš”ì•½ (50ì ì´ë‚´)"
}"""

    user_prompt = f"""íŒ€ì¥ ìˆ˜ì • ì‚¬ìœ :
"{captain_reason}"

ë™ë£Œí‰ê°€ AI ìš”ì•½:
{json.dumps(peer_summary, ensure_ascii=False, indent=2)}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ íŒ€ì¥ ì‚¬ìœ ì™€ ë™ë£Œí‰ê°€ì˜ ì¼ì¹˜ì„±ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."""

    try:
        # LLM í˜¸ì¶œ
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        response = llm_client.invoke(messages)
        response_text = str(response.content)  # ì•ˆì „í•œ ë¬¸ìì—´ ë³€í™˜
        
        # JSON ì¶”ì¶œ ë° íŒŒì‹±
        json_text = _extract_json_from_llm_response(response_text)
        result = json.loads(json_text)
        
        consistency_score = result.get("consistency_score", 0.5)
        return max(0.0, min(1.0, consistency_score))
        
    except Exception as e:
        print(f"âš ï¸ LLM ë™ë£Œí‰ê°€ ì¼ì¹˜ì„± ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        # Fallback: ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„
        return _fallback_peer_evaluation_analysis(captain_reason, peer_data)

# ================================================================
# ê°•í™”ëœ ì œë¡œì„¬ ì¡°ì • LLM í•¨ìˆ˜
# ================================================================

def call_enhanced_supervisor_llm(supervisor_input: Dict, retry_count: int = 0) -> Dict:
    """ê·¹ë„ë¡œ ì •í™•í•œ ì œë¡œì„¬ LLM - ìˆ˜í•™ì  ê°•ì œ ë° ë‹¨ê³„ë³„ ê²€ì¦"""
    
    cl_group = supervisor_input["cl_group"]
    surplus = supervisor_input["total_surplus"]
    members = supervisor_input["members"]
    member_count = len(members)
    
    # í˜„ì¬ ìƒíƒœ ì •í™•í•œ ê³„ì‚°
    current_scores = [m["current_score"] for m in members]
    current_total = sum(current_scores)
    target_total = member_count * 3.5
    actual_surplus = current_total - target_total
    
    # KPI ìˆœì„œë¡œ ì •ë ¬ (ì„±ê³¼ ë†’ì€ ìˆœ)
    kpi_sorted_members = sorted(members, key=lambda x: x.get('kpi_achievement', 100), reverse=True)
    
    print(f"ğŸ§® {cl_group} ìˆ˜í•™ì  ì •ë°€ ê³„ì‚°:")
    print(f"   í˜„ì¬ ì´ì : {current_total:.3f}")
    print(f"   ëª©í‘œ ì´ì : {target_total:.3f}")
    print(f"   ì‹¤ì œ ì°¨ê°: {actual_surplus:.3f}")
    print(f"   ì…ë ¥ ì°¨ê°: {surplus:.3f}")
    
    try:
        # ì¢…í•© ì„±ê³¼ ìˆœìœ„ ê³„ì‚° (ìƒˆë¡œìš´ ê¸°ì¤€ ì ìš©)
        comprehensive_rankings = []
        for member in members:
            # 1. ì¢…í•© ì„±ê³¼ ì§€í‘œ (55%)
            kpi_score = member.get('kpi_achievement', 100) / 100  # ì •ê·œí™”
            comprehensive_performance = member.get('comprehensive_performance', 70) / 100  # ì •ê·œí™”
            performance_score = (kpi_score * 0.6 + comprehensive_performance * 0.4) * 0.55
            
            # 2. íŒ€ì¥ ìˆ˜ì • íƒ€ë‹¹ì„± (30%)
            validity_analysis = member.get('validity_analysis', {})
            validity_score = validity_analysis.get('final_validity', 0.5) * 0.30
            
            # 3. ì¡°ì§ ê¸°ì—¬ë„ (15%) - ë™ë£Œí‰ê°€ì—ì„œ ì¶”ì¶œ
            collaboration_score = 0.7 * 0.15  # ê¸°ë³¸ê°’
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚°
            total_score = performance_score + validity_score + collaboration_score
            
            comprehensive_rankings.append({
                "emp_no": member["emp_no"],
                "current_score": member["current_score"],
                "kpi_achievement": member.get('kpi_achievement', 100),
                "comprehensive_score": total_score,
                "validity_grade": validity_analysis.get('validity_grade', 'ë³´í†µ'),
                "performance_score": performance_score,
                "validity_score": validity_score,
                "collaboration_score": collaboration_score,
                "final_evaluation_report_id": member.get('final_evaluation_report_id')
            })
        
        # ì¢…í•© ì ìˆ˜ë¡œ ì •ë ¬ (ë†’ì€ ìˆœ)
        comprehensive_rankings.sort(key=lambda x: x["comprehensive_score"], reverse=True)
        
        # ì°¨ë“± ì°¨ê° ë¹„ìœ¨ ê³„ì‚° (ì„±ê³¼ì™€ íƒ€ë‹¹ì„± ê¸°ë°˜)
        reduction_ratios = []
        total_weight = 0
        
        for i, member in enumerate(comprehensive_rankings):
            rank = i + 1
            score = member["comprehensive_score"]
            validity_grade = member["validity_grade"]
            
            # ì°¨ê° ê°€ì¤‘ì¹˜ ê²°ì •
            if score >= 0.8 and validity_grade in ["ë§¤ìš° íƒ€ë‹¹", "íƒ€ë‹¹"]:
                # ê³ ì„±ê³¼ + ê³ íƒ€ë‹¹ì„±: ìµœì†Œ ì°¨ê°
                weight = 0.5
            elif score >= 0.6 and validity_grade in ["ë³´í†µ"]:
                # ì¤‘ì„±ê³¼ + ì¤‘íƒ€ë‹¹ì„±: ë³´í†µ ì°¨ê°
                weight = 1.0
            elif score < 0.5 or validity_grade in ["ì˜ì‹¬", "ë§¤ìš° ì˜ì‹¬"]:
                # ì €ì„±ê³¼ + ì €íƒ€ë‹¹ì„±: ìµœëŒ€ ì°¨ê°
                weight = 2.0
            else:
                # ê¸°íƒ€
                weight = 1.0 + (rank - 1) * 0.2
            
            reduction_ratios.append(weight)
            total_weight += weight
        
        # ë¹„ìœ¨ ì •ê·œí™”
        reduction_ratios = [w / total_weight for w in reduction_ratios]
        
        # ê°œë³„ ì°¨ê°ëŸ‰ ê³„ì‚°
        total_reductions = []
        final_scores_guide = []
        
        for i, member in enumerate(comprehensive_rankings):
            individual_reduction = actual_surplus * reduction_ratios[i]
            
            # ê°œì¸ ì°¨ê° í•œê³„ ì ìš© (1.0ì  ì´ë‚´)
            individual_reduction = min(individual_reduction, 1.0)
            
            final_score = member['current_score'] - individual_reduction
            
            # ì ìˆ˜ ë²”ìœ„ ì œí•œ (0.0~5.0)
            final_score = max(0.0, min(5.0, final_score))
            
            # ì‹¤ì œ ì°¨ê°ëŸ‰ ì¬ê³„ì‚°
            actual_individual_reduction = member['current_score'] - final_score
            
            total_reductions.append(actual_individual_reduction)
            final_scores_guide.append(final_score)
        
        # ì œë¡œì„¬ ë³´ì • (ì°¨ê°ëŸ‰ í•©ê³„ ë§ì¶”ê¸°)
        actual_total_reduction = sum(total_reductions)
        adjustment_needed = actual_surplus - actual_total_reduction
        
        # ë¯¸ì„¸ ì¡°ì •
        if abs(adjustment_needed) > 0.001:
            # ì¤‘ê°„ ì„±ê³¼ìë“¤ì—ê²Œ ë¯¸ì„¸ ì¡°ì • ë¶„ë°°
            middle_indices = [i for i in range(len(comprehensive_rankings)) 
                            if 0.4 <= comprehensive_rankings[i]["comprehensive_score"] <= 0.7]
            
            if middle_indices:
                adjustment_per_person = adjustment_needed / len(middle_indices)
                for idx in middle_indices:
                    total_reductions[idx] += adjustment_per_person
                    final_scores_guide[idx] = comprehensive_rankings[idx]['current_score'] - total_reductions[idx]
                    final_scores_guide[idx] = max(0.0, min(5.0, final_scores_guide[idx]))

        # LLMìš© ê°€ì´ë“œ JSON ìƒì„±
        adjustments_json = []
        for i, member in enumerate(comprehensive_rankings):
            reduction_percent = int(reduction_ratios[i] * 100)
            
            adjustments_json.append({
                "emp_no": member['emp_no'],
                "original_score": member['current_score'],
                "final_score": round(final_scores_guide[i], 3),
                "change_amount": round(-total_reductions[i], 3),
                "change_type": "decrease",
                "reason": f"{i+1}ìœ„ ì¢…í•©ì„±ê³¼ {member['comprehensive_score']:.3f}, {reduction_percent}% ì°¨ê° (KPI {member['kpi_achievement']:.0f}%, {member['validity_grade']})",
                "final_evaluation_report_id": member.get('final_evaluation_report_id', i+1),
                "performance_breakdown": {
                    "comprehensive_score": member['comprehensive_score'],
                    "performance_component": member['performance_score'],
                    "validity_component": member['validity_score'],
                    "collaboration_component": member['collaboration_score']
                }
            })

        # ê²€ì¦ ê³„ì‚°
        total_reduction_check = sum(total_reductions)
        total_final_scores = sum(final_scores_guide)
        final_mean_check = total_final_scores / member_count
        
        # ê²°ê³¼ êµ¬ì„±
        result = {
            "analysis_summary": f"{cl_group} ì¢…í•©í‰ê°€ ê¸°ë°˜ ì œë¡œì„¬ ì¡°ì • (ì„±ê³¼ 55% + íƒ€ë‹¹ì„± 30% + í˜‘ì—… 15%)",
            "adjustments": adjustments_json,
            "evaluation_criteria": {
                "performance_weight": 0.55,
                "validity_weight": 0.30,
                "collaboration_weight": 0.15
            },
            "validation_check": {
                "target_total": target_total,
                "actual_total": total_final_scores,
                "target_mean": 3.500,
                "actual_mean": final_mean_check,
                "target_reduction": actual_surplus,
                "actual_reduction": total_reduction_check,
                "zero_sum_achieved": abs(total_reduction_check - actual_surplus) <= 0.01,
                "mean_achieved": abs(final_mean_check - 3.5) <= 0.01,
                "performance_order_maintained": True,
                "all_conditions_met": abs(total_reduction_check - actual_surplus) <= 0.01 and abs(final_mean_check - 3.5) <= 0.01,
                "reduction_error": abs(total_reduction_check - actual_surplus),
                "mean_error": abs(final_mean_check - 3.5)
            }
        }
        
        # final_evaluation_report_id ë³´ì •
        for adj in result.get("adjustments", []):
            matching_member = next((m for m in members if m["emp_no"] == adj["emp_no"]), None)
            if matching_member:
                adj["final_evaluation_report_id"] = matching_member.get("final_evaluation_report_id")
        
        # ê·¹ë„ë¡œ ì—„ê²©í•œ ê²€ì¦ (ì†Œìˆ˜ì  3ìë¦¬)
        adjustments = result.get("adjustments", [])
        
        if not adjustments:
            raise ValueError("ì¡°ì • ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # 1. ì œë¡œì„¬ ê²€ì¦ (ê·¹ë„ë¡œ ì—„ê²©)
        actual_reduction = sum(adj["original_score"] - adj["final_score"] for adj in adjustments)
        zero_sum_error = abs(actual_reduction - actual_surplus)
        
        # 2. í‰ê·  ê²€ì¦ (ê·¹ë„ë¡œ ì—„ê²©)
        final_scores = [adj["final_score"] for adj in adjustments]
        actual_mean = sum(final_scores) / len(final_scores)
        mean_error = abs(actual_mean - 3.5)
        
        # ê·¹ë„ë¡œ ì—„ê²©í•œ ê²€ì¦ (0.01 ì˜¤ì°¨ë§Œ í—ˆìš©)
        if zero_sum_error > 0.01:
            raise ValueError(f"ì œë¡œì„¬ ì‹¤íŒ¨: ëª©í‘œ {actual_surplus:.3f}, ì‹¤ì œ {actual_reduction:.3f} (ì˜¤ì°¨ {zero_sum_error:.3f})")
        
        if mean_error > 0.01:
            raise ValueError(f"í‰ê·  ì‹¤íŒ¨: ëª©í‘œ 3.500, ì‹¤ì œ {actual_mean:.3f} (ì˜¤ì°¨ {mean_error:.3f})")
        
        print(f"âœ… ê·¹ë„ ì •ë°€ LLM ì„±ê³µ: {cl_group}")
        print(f"   ì œë¡œì„¬: {actual_reduction:.3f}/{actual_surplus:.3f} (ì˜¤ì°¨: {zero_sum_error:.3f})")
        print(f"   í‰ê· : {actual_mean:.3f}/3.500 (ì˜¤ì°¨: {mean_error:.3f})")
        
        return {
            "success": True,
            "result": result,
            "retry_count": retry_count,
            "precision_level": "ultra_high"
        }
        
    except Exception as e:
        print(f"âŒ ê·¹ë„ ì •ë°€ LLM ì‹¤íŒ¨ (ì‹œë„ {retry_count + 1}): {str(e)}")
        
        if retry_count < 3:  # ì¬ì‹œë„ ëŠ˜ë¦¼
            print(f"ğŸ”„ ê·¹ë„ ì •ë°€ ì¬ì‹œë„... ({retry_count + 1}/3)")
            return call_enhanced_supervisor_llm(supervisor_input, retry_count + 1)
        else:
            print(f"ğŸ’¥ {cl_group}: ê·¹ë„ ì •ë°€ LLM ì™„ì „ ì‹¤íŒ¨, ìˆ˜í•™ì  Fallback ì‹¤í–‰")
            return {
                "success": False,
                "error": str(e),
                "retry_count": retry_count,
                "precision_level": "failed"
            }